{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Yz80rPMZKObK"
   },
   "source": [
    "# Aquire dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mfUX-Y7sQQV9"
   },
   "source": [
    "## Download dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kxo34j6Qdscq"
   },
   "outputs": [],
   "source": [
    "if not os.path.exists('./logs'):\n",
    "    os.makedirs('./logs')\n",
    "if not os.path.exists('./datasets'):\n",
    "    os.makedirs('./datasets')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "아래의 코드를 실행하면 데이터셋 다운로드가 진행됩니다.\n",
    "\n",
    "오류가 날 경우에는 assignment.ipynb 파일이 있는 위치에서 bash ./download_cyclegan_dataset.sh horse2zebra 명령어를 입력해주세요."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zb8ujxWx2oka"
   },
   "source": [
    "!bash ./download_cyclegan_dataset.sh horse2zebra"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZTHemRjFQQWC"
   },
   "source": [
    "# Model definition & Hyperparameter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![cyclegan.png](./cyclegan.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aAu_qett4H4v"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional\n",
    "import torch\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision.utils import make_grid, save_image\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from torchsummary import summary\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ufcX0A2Y4J_t"
   },
   "outputs": [],
   "source": [
    "img_size = 256 # 이미지 사이즈 \n",
    "channels = 3\n",
    "ngf = 32 # G channels after first layer\n",
    "ndf = 64 # D channels after first layer\n",
    "\n",
    "epochs = 200 # 200번이 충분하지만, 시간단축을 위해 15번으로 조정\n",
    "batch_size = 4 # batch size\n",
    "lambda_X = 10\n",
    "lambda_Y = 10\n",
    "lambda_identity_X = 0.5\n",
    "lambda_identity_Y = 0.5\n",
    "lr = 0.0002 # learning rate\n",
    "betas = (0.5, 0.999)\n",
    "\n",
    "mean_init = 0.0\n",
    "std_init = 0.02"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "roIYTK_rvNJP"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device is cuda.\n"
     ]
    }
   ],
   "source": [
    "# Cuda stuff\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "print(\"Device is \" + str(device) + \".\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8M086leYqCAV"
   },
   "source": [
    "# CycleGAN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hPy-hIOP43Ux"
   },
   "outputs": [],
   "source": [
    "# ResidualBlock 설계\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, c):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        \n",
    "        block = [nn.ReflectionPad2d(1),\n",
    "                 nn.Conv2d(c, c, 3, 1, 0),\n",
    "                 nn.InstanceNorm2d(c),\n",
    "                 nn.ReLU(),\n",
    "                 nn.ReflectionPad2d(1),\n",
    "                 nn.Conv2d(c, c, 3, 1, 0),\n",
    "                 nn.InstanceNorm2d(c)]\n",
    "        \n",
    "        self.block = nn.Sequential(*block)\n",
    "        \n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Hint : 미리 정의해놓은 Residual Block을 Forward하는 코드를 추가해주세요.\n",
    "        x = self.block(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7SijtRQW8aGv"
   },
   "outputs": [],
   "source": [
    "# Generator 설계\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "        \n",
    "        # Encoding\n",
    "        model = []\n",
    "        model += [nn.ReflectionPad2d(4),\n",
    "                  nn.Conv2d(3, ngf, 9, 1, 0),\n",
    "                  nn.InstanceNorm2d(ngf),\n",
    "                  nn.ReLU()]\n",
    "        model += [nn.Conv2d(ngf, ngf*2, 4, 2, 1),\n",
    "                  nn.InstanceNorm2d(ngf*2),\n",
    "                  nn.ReLU()]\n",
    "        model += [nn.Conv2d(ngf*2, ngf*4, 4, 2, 1),\n",
    "                  nn.InstanceNorm2d(ngf*4),\n",
    "                  nn.ReLU()]\n",
    "        \n",
    "        # Transformation\n",
    "        for i in range(6):\n",
    "            model += [ResidualBlock(ngf*4)]   # Hint : 채널 수를 그대로 유지하면서 반복시켜주는 residual block\n",
    "        \n",
    "        # Decoding\n",
    "        model += [nn.ConvTranspose2d(ngf*4, ngf*2, 4, 2, 1), # Hint : 줄여준 H * W 를 다시 반대로 늘려주는 과정\n",
    "                  nn.InstanceNorm2d(ngf*2),\n",
    "                  nn.ReLU()]\n",
    "        model += [nn.ConvTranspose2d(ngf*2, ngf, 4, 2, 1), \n",
    "                  nn.InstanceNorm2d(ngf),\n",
    "                  nn.ReLU()]\n",
    "        model += [nn.ReflectionPad2d(4),\n",
    "                  nn.Conv2d(ngf, 3, 9, 1, 0),\n",
    "                  nn.Tanh()]\n",
    "        \n",
    "        self.model = nn.Sequential(*model)\n",
    "\n",
    "    def weight_init(self, mean, std):\n",
    "        for m in self._modules:\n",
    "            normal_init(self._modules[m], mean, std)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ScqGWgOGTCnx"
   },
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "        \n",
    "        model = []\n",
    "        model += [nn.Conv2d(3, ndf, 4, 2, 1),   # outputchannel : ndf, kernel: 4, stride:2 , padding : 1\n",
    "                  nn.LeakyReLU(0.2)]\n",
    "        \n",
    "        in_channels = ndf\n",
    "        out_channels = ndf*2\n",
    "        for i in range(2):\n",
    "            # Hint : 어떤 변수가 input channel이 되고, 어떤 변수가 output channel이 되나요?\n",
    "            model += [nn.Conv2d(in_channels, out_channels, 4, 2, 1),     \n",
    "                      nn.InstanceNorm2d(out_channels),\n",
    "                      nn.LeakyReLU(0.2)]\n",
    "            # Hint : 매 반복마다 channel 수가 두배가 되도록 하려면?\n",
    "            in_channels = out_channels           \n",
    "            out_channels = out_channels*2\n",
    "\n",
    "        model += [nn.Conv2d(in_channels, out_channels, 4, 1, 1),\n",
    "                  nn.InstanceNorm2d(out_channels),\n",
    "                  nn.LeakyReLU(0.2)]\n",
    "        \n",
    "        model += [nn.Conv2d(out_channels, 1, 4, 1, 1)]\n",
    "        \n",
    "        self.model = nn.Sequential(*model)\n",
    "\n",
    "    def weight_init(self, mean, std):\n",
    "        for m in self._modules:\n",
    "            normal_init(self._modules[m], mean, std)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "    \n",
    "def normal_init(m, mean, std):\n",
    "    if isinstance(m, nn.ConvTranspose2d) or isinstance(m, nn.Conv2d):\n",
    "        m.weight.data.normal_(mean, std)\n",
    "        m.bias.data.zero_()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oUHQaDB95Kip"
   },
   "source": [
    "# Data Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AQZCce-PjX3Z"
   },
   "outputs": [],
   "source": [
    "# Dataset Code\n",
    "\n",
    "import os\n",
    "from PIL import Image\n",
    "import random\n",
    "\n",
    "class UnallignedDataset(Dataset):\n",
    "    def __init__(self, root, transform, phase='train'):\n",
    "        dir_A = os.path.join(root, phase + 'A')\n",
    "        dir_B = os.path.join(root, phase + 'B')\n",
    "        \n",
    "        self.A_paths = [os.path.join(dir_A, f) for f in os.listdir(dir_A)]\n",
    "        self.B_paths = [os.path.join(dir_B, f) for f in os.listdir(dir_B)]\n",
    "        self.A_size = len(self.A_paths)\n",
    "        self.B_size = len(self.B_paths)\n",
    "        \n",
    "        self.transform = transform\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        A_path = self.A_paths[index % self.A_size]\n",
    "        B_path = self.B_paths[random.randint(0, self.B_size - 1)]\n",
    "        \n",
    "        A_img = Image.open(A_path).convert('RGB')\n",
    "        B_img = Image.open(B_path).convert('RGB')\n",
    "\n",
    "        A = self.transform(A_img)\n",
    "        B = self.transform(B_img)\n",
    "        return A, B\n",
    "    \n",
    "    def __len__(self):\n",
    "        return max(self.A_size, self.B_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pHht4nVj8n7F"
   },
   "outputs": [],
   "source": [
    "# 학습을 돕기 위한 추가 테크닉 (과제를 위해 알아야할 필요는 없음) (참고: https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/issues/75)\n",
    "\n",
    "class ImagePool():\n",
    "    def __init__(self, pool_size):\n",
    "        self.pool_size = pool_size\n",
    "        self.images = []\n",
    "        \n",
    "    def get(self, img):\n",
    "        if len(self.images) < self.pool_size:\n",
    "            self.images.append(img)\n",
    "            return img\n",
    "        else:\n",
    "            p = random.random()\n",
    "            if p > 0.5:\n",
    "                idx = random.randint(0, self.pool_size-1)\n",
    "                tmp = self.images[idx]\n",
    "                self.images[idx] = img\n",
    "                return tmp\n",
    "            else:\n",
    "                return img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1_nWsNBQKhGz"
   },
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\ironm\\\\OneDrive\\\\GitHub\\\\Samsung-AI-KAIST\\\\Assignment_0731'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "il5CP_fDXnay"
   },
   "outputs": [],
   "source": [
    "G = Generator().to(device)\n",
    "F = Generator().to(device)\n",
    "D_X = Discriminator().to(device)\n",
    "D_Y = Discriminator().to(device)\n",
    "G.weight_init(mean_init, std_init)\n",
    "F.weight_init(mean_init, std_init)\n",
    "D_X.weight_init(mean_init, std_init)\n",
    "D_Y.weight_init(mean_init, std_init)\n",
    "G.train()\n",
    "F.train()\n",
    "D_X.train()\n",
    "D_Y.train()\n",
    "\n",
    "root_dir = os.getcwd() # this line is added becasue I use Windows.\n",
    "\n",
    "transform = transforms.Compose([transforms.Resize(img_size), transforms.CenterCrop(img_size), transforms.ToTensor(), transforms.Normalize((0.5,0.5,0.5), (0.5,0.5,0.5))])\n",
    "train_loader = torch.utils.data.DataLoader(dataset=UnallignedDataset(root_dir+os.sep+'datasets'+os.sep+'horse2zebra', transform), \n",
    "                                           batch_size=batch_size, \n",
    "                                           shuffle=True, # Hint : dataloader 내의 data 들이 뒤섞여 있기를 바란다면 어떤 옵션을 추가하나요?\n",
    "                                           pin_memory=True, \n",
    "                                          # num_workers=1  # This cause error on Windows\n",
    "                                          )\n",
    "test_loader = torch.utils.data.DataLoader(dataset=UnallignedDataset(root_dir+os.sep+'datasets'+os.sep+'horse2zebra', transform, phase='test'), \n",
    "                                           batch_size=batch_size, \n",
    "                                           shuffle=False, # Hint : dataloader 내의 data 들이 뒤섞여 있기를 바란다면 어떤 옵션을 추가하나요?\n",
    "                                           pin_memory=True, \n",
    "                                          # num_workers=1   # This cause error on Windows\n",
    "                                         )\n",
    "\n",
    "X_pool = ImagePool(50)\n",
    "Y_pool = ImagePool(50)\n",
    "\n",
    "mse_criterion = nn.MSELoss()\n",
    "l1_criterion = nn.L1Loss()\n",
    "\n",
    "GF_optimizer = torch.optim.Adam(list(G.parameters()) + list(F.parameters()), lr=lr, betas=betas)\n",
    "D_X_optimizer = torch.optim.Adam(D_X.parameters(), lr=lr, betas=betas)\n",
    "D_Y_optimizer = torch.optim.Adam(D_Y.parameters(), lr=lr, betas=betas)\n",
    "\n",
    "GF_scheduler = StepLR(GF_optimizer, 1, lr/100.0)\n",
    "D_X_scheduler = StepLR(D_X_optimizer, 1, lr/100.0)\n",
    "D_Y_scheduler = StepLR(D_Y_optimizer, 1, lr/100.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fykPc1sCQQWU"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "   ReflectionPad2d-1          [-1, 3, 264, 264]               0\n",
      "            Conv2d-2         [-1, 32, 256, 256]           7,808\n",
      "    InstanceNorm2d-3         [-1, 32, 256, 256]               0\n",
      "              ReLU-4         [-1, 32, 256, 256]               0\n",
      "            Conv2d-5         [-1, 64, 128, 128]          32,832\n",
      "    InstanceNorm2d-6         [-1, 64, 128, 128]               0\n",
      "              ReLU-7         [-1, 64, 128, 128]               0\n",
      "            Conv2d-8          [-1, 128, 64, 64]         131,200\n",
      "    InstanceNorm2d-9          [-1, 128, 64, 64]               0\n",
      "             ReLU-10          [-1, 128, 64, 64]               0\n",
      "  ReflectionPad2d-11          [-1, 128, 66, 66]               0\n",
      "           Conv2d-12          [-1, 128, 64, 64]         147,584\n",
      "   InstanceNorm2d-13          [-1, 128, 64, 64]               0\n",
      "             ReLU-14          [-1, 128, 64, 64]               0\n",
      "  ReflectionPad2d-15          [-1, 128, 66, 66]               0\n",
      "           Conv2d-16          [-1, 128, 64, 64]         147,584\n",
      "   InstanceNorm2d-17          [-1, 128, 64, 64]               0\n",
      "    ResidualBlock-18          [-1, 128, 64, 64]               0\n",
      "  ReflectionPad2d-19          [-1, 128, 66, 66]               0\n",
      "           Conv2d-20          [-1, 128, 64, 64]         147,584\n",
      "   InstanceNorm2d-21          [-1, 128, 64, 64]               0\n",
      "             ReLU-22          [-1, 128, 64, 64]               0\n",
      "  ReflectionPad2d-23          [-1, 128, 66, 66]               0\n",
      "           Conv2d-24          [-1, 128, 64, 64]         147,584\n",
      "   InstanceNorm2d-25          [-1, 128, 64, 64]               0\n",
      "    ResidualBlock-26          [-1, 128, 64, 64]               0\n",
      "  ReflectionPad2d-27          [-1, 128, 66, 66]               0\n",
      "           Conv2d-28          [-1, 128, 64, 64]         147,584\n",
      "   InstanceNorm2d-29          [-1, 128, 64, 64]               0\n",
      "             ReLU-30          [-1, 128, 64, 64]               0\n",
      "  ReflectionPad2d-31          [-1, 128, 66, 66]               0\n",
      "           Conv2d-32          [-1, 128, 64, 64]         147,584\n",
      "   InstanceNorm2d-33          [-1, 128, 64, 64]               0\n",
      "    ResidualBlock-34          [-1, 128, 64, 64]               0\n",
      "  ReflectionPad2d-35          [-1, 128, 66, 66]               0\n",
      "           Conv2d-36          [-1, 128, 64, 64]         147,584\n",
      "   InstanceNorm2d-37          [-1, 128, 64, 64]               0\n",
      "             ReLU-38          [-1, 128, 64, 64]               0\n",
      "  ReflectionPad2d-39          [-1, 128, 66, 66]               0\n",
      "           Conv2d-40          [-1, 128, 64, 64]         147,584\n",
      "   InstanceNorm2d-41          [-1, 128, 64, 64]               0\n",
      "    ResidualBlock-42          [-1, 128, 64, 64]               0\n",
      "  ReflectionPad2d-43          [-1, 128, 66, 66]               0\n",
      "           Conv2d-44          [-1, 128, 64, 64]         147,584\n",
      "   InstanceNorm2d-45          [-1, 128, 64, 64]               0\n",
      "             ReLU-46          [-1, 128, 64, 64]               0\n",
      "  ReflectionPad2d-47          [-1, 128, 66, 66]               0\n",
      "           Conv2d-48          [-1, 128, 64, 64]         147,584\n",
      "   InstanceNorm2d-49          [-1, 128, 64, 64]               0\n",
      "    ResidualBlock-50          [-1, 128, 64, 64]               0\n",
      "  ReflectionPad2d-51          [-1, 128, 66, 66]               0\n",
      "           Conv2d-52          [-1, 128, 64, 64]         147,584\n",
      "   InstanceNorm2d-53          [-1, 128, 64, 64]               0\n",
      "             ReLU-54          [-1, 128, 64, 64]               0\n",
      "  ReflectionPad2d-55          [-1, 128, 66, 66]               0\n",
      "           Conv2d-56          [-1, 128, 64, 64]         147,584\n",
      "   InstanceNorm2d-57          [-1, 128, 64, 64]               0\n",
      "    ResidualBlock-58          [-1, 128, 64, 64]               0\n",
      "  ConvTranspose2d-59         [-1, 64, 128, 128]         131,136\n",
      "   InstanceNorm2d-60         [-1, 64, 128, 128]               0\n",
      "             ReLU-61         [-1, 64, 128, 128]               0\n",
      "  ConvTranspose2d-62         [-1, 32, 256, 256]          32,800\n",
      "   InstanceNorm2d-63         [-1, 32, 256, 256]               0\n",
      "             ReLU-64         [-1, 32, 256, 256]               0\n",
      "  ReflectionPad2d-65         [-1, 32, 264, 264]               0\n",
      "           Conv2d-66          [-1, 3, 256, 256]           7,779\n",
      "             Tanh-67          [-1, 3, 256, 256]               0\n",
      "================================================================\n",
      "Total params: 2,114,563\n",
      "Trainable params: 2,114,563\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.75\n",
      "Forward/backward pass size (MB): 372.66\n",
      "Params size (MB): 8.07\n",
      "Estimated Total Size (MB): 381.47\n",
      "----------------------------------------------------------------\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 64, 128, 128]           3,136\n",
      "         LeakyReLU-2         [-1, 64, 128, 128]               0\n",
      "            Conv2d-3          [-1, 128, 64, 64]         131,200\n",
      "    InstanceNorm2d-4          [-1, 128, 64, 64]               0\n",
      "         LeakyReLU-5          [-1, 128, 64, 64]               0\n",
      "            Conv2d-6          [-1, 256, 32, 32]         524,544\n",
      "    InstanceNorm2d-7          [-1, 256, 32, 32]               0\n",
      "         LeakyReLU-8          [-1, 256, 32, 32]               0\n",
      "            Conv2d-9          [-1, 512, 31, 31]       2,097,664\n",
      "   InstanceNorm2d-10          [-1, 512, 31, 31]               0\n",
      "        LeakyReLU-11          [-1, 512, 31, 31]               0\n",
      "           Conv2d-12            [-1, 1, 30, 30]           8,193\n",
      "================================================================\n",
      "Total params: 2,764,737\n",
      "Trainable params: 2,764,737\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.75\n",
      "Forward/backward pass size (MB): 45.27\n",
      "Params size (MB): 10.55\n",
      "Estimated Total Size (MB): 56.57\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "summary(G, (3, 256, 256))\n",
    "summary(D_X, (3, 256, 256))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "실습시간에 사용했던 CycleGAN 코드와는 약간 다르지만, Train 코드의 구조는 같습니다. 빈칸으로 뚫어놨던 부분은 실습시간에서도 다뤘던 부분이니 코드를 잘 읽어보고 풀어주세요. (G와 F는 Generator / D_X와 D_Y는 Discriminator입니다.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "61scdU94hpqL"
   },
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (<ipython-input-14-ba86301c80e7>, line 125)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-14-ba86301c80e7>\"\u001b[1;36m, line \u001b[1;32m125\u001b[0m\n\u001b[1;33m    save_image(image_tensor, './i.' nrow=4, padding=2, normalize=True)\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m unexpected indent\n"
     ]
    }
   ],
   "source": [
    "def mean(lst):\n",
    "    return sum(lst)/len(lst)\n",
    "\n",
    "# Prepare some test data, 5 of each kind\n",
    "test_data = [(x.to(device), y.to(device)) for i, (x, y) in enumerate(test_loader) if i<5]\n",
    "\n",
    "# Define target vectors\n",
    "fake_target = 0.0\n",
    "real_target = 1.0\n",
    "for epoch in range(epochs):\n",
    "    G_gan_loss_epoch = []\n",
    "    G_cycle_loss_epoch = []\n",
    "    G_ident_loss_epoch = []\n",
    "    D_X_gan_loss_epoch = []\n",
    "    \n",
    "    # Linear lr decay\n",
    "    if epoch > 99:\n",
    "        GF_scheduler.step()\n",
    "        D_X_scheduler.step()\n",
    "        D_Y_scheduler.step()\n",
    "        \n",
    "    for i, (X, Y) in enumerate(train_loader):\n",
    "        X = X.to(device)\n",
    "        Y = Y.to(device)\n",
    "        #########################################################\n",
    "        # Update generators\n",
    "        #########################################################\n",
    "        GF_optimizer.zero_grad()\n",
    "        \n",
    "        # Translate from X to Y, check D_Y output\n",
    "        G_out = G(X)\n",
    "        D_Y_out = D_Y(G_out.detach())\n",
    "        G_gan_loss = mse_criterion(D_Y_out, torch.ones_like(D_Y_out).to(device))\n",
    "        \n",
    "        # Translate from Y to X, check D_X output\n",
    "        F_out = F(Y)\n",
    "        D_X_out = D_X(F_out.detach())\n",
    "        F_gan_loss = mse_criterion(D_X_out, torch.ones_like(D_X_out).to(device))\n",
    "        \n",
    "        # Translate from X to Y to X, check reconstruction error\n",
    "        X_recon = F(G_out)\n",
    "        G_cycle_loss = l1_criterion(X_recon, X) * lambda_X\n",
    "        \n",
    "        # Translate from Y to X to Y, check reconstruction error\n",
    "        Y_recon = G(F_out)\n",
    "        F_cycle_loss = l1_criterion(Y_recon, Y) * lambda_Y\n",
    "        \n",
    "        # Translate a picture from Y from X to Y, should be Y\n",
    "        Y_ident = G(Y)\n",
    "        G_ident_loss = l1_criterion(Y_ident, Y) * lambda_identity_X * lambda_X\n",
    "        \n",
    "        # Translate a picture from X from Y to X, should be X\n",
    "        X_ident = F(X)\n",
    "        F_ident_loss = l1_criterion(X_ident, X) * lambda_identity_X * lambda_Y\n",
    "        \n",
    "        # Hint : Generator를 학습시키기 위해 어떤 Loss들을 사용했나요?? (Generator G와 F를 한번에 학습시키는 Loss입니다)\n",
    "        GF_loss = G_cycle_loss + F_cycle_loss + G_ident_loss + F_ident_loss + G_gan_loss + F_gan_loss \n",
    "        GF_loss.backward()\n",
    "        GF_optimizer.step()\n",
    "        \n",
    "        #########################################################\n",
    "        # Update discriminators\n",
    "        # D_Y, minimize L_D_Y = E_y (D(y) - 1) ^2 + E_x (D(x))^2\n",
    "        #########################################################\n",
    "        D_Y_optimizer.zero_grad()\n",
    "        \n",
    "        # Test D_Y with fake and real input\n",
    "        G_out = Y_pool.get(G_out)\n",
    "        D_Y_out_fake = D_Y(G_out.detach())\n",
    "        D_Y_out_real = D_Y(Y)\n",
    "        # Calculate loss\n",
    "        D_Y_loss_fake = mse_criterion(D_Y_out_fake, torch.zeros_like(D_Y_out_fake).to(device))\n",
    "        D_Y_loss_real = mse_criterion(D_Y_out_real, torch.ones_like(D_Y_out_real).to(device))\n",
    "        D_Y_gan_loss = (D_Y_loss_real + D_Y_loss_fake)*0.5\n",
    "        \n",
    "        D_Y_gan_loss.backward() # Hint : back propagation 해주기\n",
    "        D_Y_optimizer.step() # Hint : optimizer가 한 스텝 나아가기\n",
    "        \n",
    "        #########################################################\n",
    "        # D_X, minimize L_D_X = E_x (D(x) - 1) ^2 + E_y (D(y))^2\n",
    "        #########################################################\n",
    "        D_X_optimizer.zero_grad()\n",
    "        \n",
    "        # Test D_X with fake and real input\n",
    "        F_out = X_pool.get(F_out)\n",
    "        D_X_out_fake = D_X(F_out.detach())\n",
    "        D_X_out_real = D_X(X)\n",
    "        # Calculate loss\n",
    "        D_X_loss_fake = mse_criterion(D_X_out_fake, torch.zeros_like(D_X_out_fake).to(device))\n",
    "        D_X_loss_real = mse_criterion(D_X_out_real, torch.ones_like(D_X_out_real).to(device))\n",
    "        D_X_gan_loss = (D_X_loss_real + D_X_loss_fake)*0.5\n",
    "        \n",
    "        D_X_gan_loss.backward() # Hint : back propagation 해주기\n",
    "        D_X_optimizer.step() # Hint : optimizer가 한 스텝 나아가기\n",
    "        \n",
    "        # Save losses\n",
    "        G_gan_loss_epoch.append(G_gan_loss.item())\n",
    "        G_cycle_loss_epoch.append(G_cycle_loss.item())\n",
    "        G_ident_loss_epoch.append(G_ident_loss.item())\n",
    "        D_X_gan_loss_epoch.append(D_X_gan_loss.item())\n",
    "        \n",
    "        # Do some test output every 100 batches\n",
    "        if i % 100 == 0:\n",
    "            checkname = 'Epoch [%d/%d], Batch [%d/%d]' % (epoch+1, epochs, i, len(train_loader))\n",
    "            savename = './logs/Epoch%dBatch%d' % (epoch+1, i)\n",
    "            print(checkname)\n",
    "            \n",
    "            image_tensor = None\n",
    "            # Generate test outputs\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                G.eval()\n",
    "                F.eval()\n",
    "                for X, Y in test_data:\n",
    "                    G_out = G(X)\n",
    "                    F_out = F(Y)\n",
    "                    if image_tensor is None:\n",
    "                        image_tensor = torch.cat((X, G_out, Y, F_out), 0)\n",
    "                    else:\n",
    "                        image_tensor = torch.cat((image_tensor, X, G_out, Y, F_out), 0)\n",
    "                G.train()\n",
    "                F.train()\n",
    "            save_image(image_tensor, savename + '.png', nrow=4, padding=50)\n",
    "            \n",
    "#             save_image(image_tensor, './i.' nrow=4, padding=2, normalize=True)\n",
    "#             writer.add_image('test_images', image, i+epoch*len(train_loader))\n",
    "    \n",
    "    # Calculate mean\n",
    "    G_gan_loss_epoch = mean(G_gan_loss_epoch)\n",
    "    G_cycle_loss_epoch = mean(G_cycle_loss_epoch)\n",
    "    G_ident_loss_epoch = mean(G_ident_loss_epoch)\n",
    "    G_loss_epoch = G_gan_loss_epoch + G_cycle_loss_epoch + G_ident_loss_epoch\n",
    "    D_X_gan_loss_epoch = mean(D_X_gan_loss_epoch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JQ2CQ5KnQQWX"
   },
   "outputs": [],
   "source": [
    "torch.save(G.state_dict(), 'G.pt')\n",
    "torch.save(F.state_dict(), 'F.pt')\n",
    "torch.save(D_X.state_dict(), 'D_X.pt')\n",
    "torch.save(D_Y.state_dict(), 'D_Y.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_image(image_tensor, './i.jpg', nrow=4, padding=2, normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "cyclegan_horse2zebra_assign.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
