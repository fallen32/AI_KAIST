{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": " Network_Sparsification_Assignment (황성주 교수님 연구실)",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python2",
      "display_name": "Python 2"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HayeonLee/Samsung_DS_Assignment/blob/master/Network_Sparsification_Assignment_(%ED%99%A9%EC%84%B1%EC%A3%BC_%EA%B5%90%EC%88%98%EB%8B%98_%EC%97%B0%EA%B5%AC%EC%8B%A4).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UD6bQj0iVZM9",
        "colab_type": "text"
      },
      "source": [
        "# Network Sparsification Assignment (황성주 교수님 연구실)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kP0DgMlDV64u",
        "colab_type": "text"
      },
      "source": [
        "## Import Tensorflow and other libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fNpLz5neUrqd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "#기존에 설치된 다른 버전의 tensorflow를 제거합니다.\n",
        "!pip uninstall tensorboard -y\n",
        "!pip uninstall tensorflow-gpu -y\n",
        "!pip uninstall tensorflow -y\n",
        "#tensorflow gpu 버전을 설치합니다\n",
        "!pip install tensorflow-gpu==1.14"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "68oyuhXNWEQ6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf # tensorflow를 import해줍니다\n",
        "tf.__version__ # 내가 사용할 tensorflow의 버전을 나타냅니다"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nkYfIMpyYdPZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# pretrain된 lenet의 체크포인트 파일을 가져옵니다.\n",
        "!mkdir -p results/\n",
        "!wget -O lenet_dense_pretrained.zip https://github.com/HayeonLee/sparsification_samsung/blob/master/lenet_dense_pretrained.zip?raw=true\n",
        "!unzip lenet_dense_pretrained.zip -d results/\n",
        "!rm lenet_dense_pretrained.zip\n",
        "!ls\n",
        "!ls results/pretrained/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XR4m7OXGAt8i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 필요한 라이브러리를 임포트합니다.\n",
        "from __future__ import print_function\n",
        "import time\n",
        "import os\n",
        "from pylab import *\n",
        "import numpy as np\n",
        "from tensorflow.python.client import device_lib\n",
        "from tensorflow.contrib.distributions import RelaxedBernoulli\n",
        "from tensorflow.examples.tutorials.mnist import input_data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gJ0R62L8WvSu",
        "colab_type": "text"
      },
      "source": [
        "## Define the functions and utils"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iFCAzQYqfrzP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 자주 쓰는 텐서플로우 함수의 약어를 지정합니다.\n",
        "logit = lambda x: tf.log(x + 1e-20) - tf.log(1-x + 1e-20)\n",
        "softplus = tf.nn.softplus\n",
        "relu = tf.nn.relu\n",
        "\n",
        "dense = tf.layers.dense\n",
        "flatten = tf.contrib.layers.flatten\n",
        "\n",
        "def conv(x, filters, kernel_size=3, strides=1, **kwargs):\n",
        "    return tf.layers.conv2d(x, filters, kernel_size, strides,\n",
        "            data_format='channels_first', **kwargs)\n",
        "\n",
        "def pool(x, **kwargs):\n",
        "    return tf.layers.max_pooling2d(x, 2, 2,\n",
        "            data_format='channels_first', **kwargs)\n",
        "\n",
        "def global_avg_pool(x):\n",
        "    return tf.reduce_mean(x, axis=[2, 3])\n",
        "\n",
        "layer_norm = tf.contrib.layers.layer_norm"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sJrRN4ujgQh9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# utils/train.py\n",
        "# 필요한 함수를 정의합니다.\n",
        "def cross_entropy(logits, labels):\n",
        "    return tf.losses.softmax_cross_entropy(logits=logits, onehot_labels=labels)\n",
        "\n",
        "def weight_decay(decay, var_list=None):\n",
        "    var_list = tf.trainable_variables() if var_list is None else var_list\n",
        "    return decay*tf.add_n([tf.nn.l2_loss(var) for var in var_list])\n",
        "\n",
        "def accuracy(logits, labels):\n",
        "    correct = tf.equal(tf.argmax(logits, 1), tf.argmax(labels, 1))\n",
        "    return tf.reduce_mean(tf.cast(correct, tf.float32))\n",
        "  \n",
        "def digamma_approx(x):\n",
        "# @MISC {1446110,\n",
        "# TITLE = {Approximating the Digamma function},\n",
        "# AUTHOR = {njuffa (https://math.stackexchange.com/users/114200/njuffa)},\n",
        "# HOWPUBLISHED = {Mathematics Stack Exchange},\n",
        "# NOTE = {URL:https://math.stackexchange.com/q/1446110 (version: 2015-09-22)},\n",
        "# EPRINT = {https://math.stackexchange.com/q/1446110},\n",
        "# URL = {https://math.stackexchange.com/q/1446110}}\n",
        "    def digamma_over_one(x):\n",
        "        return tf.log(x + 0.4849142940227510) \\\n",
        "                - 1/(1.0271785180163817*x)\n",
        "    return digamma_over_one(x+1) - 1./x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "x9RShCp_DDOz",
        "colab": {}
      },
      "source": [
        "# # log를 출력하기 위한 함수를 선언합니다.\n",
        "class Accumulator():\n",
        "    def __init__(self, *args):\n",
        "        self.args = args\n",
        "        self.argdict = {}\n",
        "        for i, arg in enumerate(args):\n",
        "            self.argdict[arg] = i\n",
        "        self.sums = [0]*len(args)\n",
        "        self.cnt = 0\n",
        "\n",
        "    def accum(self, val):\n",
        "        val = [val] if type(val) is not list else val\n",
        "        val = [v for v in val if v is not None]\n",
        "        assert(len(val) == len(self.args))\n",
        "        for i in range(len(val)):\n",
        "            self.sums[i] += val[i]\n",
        "        self.cnt += 1\n",
        "\n",
        "    def clear(self):\n",
        "        self.sums = [0]*len(self.args)\n",
        "        self.cnt = 0\n",
        "\n",
        "    def get(self, arg, avg=True):\n",
        "        i = self.argdict.get(arg, -1)\n",
        "        assert(i is not -1)\n",
        "        return (self.sums[i]/self.cnt if avg else self.sums[i])\n",
        "\n",
        "    def print_(self, header=None, epoch=None, it=None, time=None,\n",
        "            logfile=None, do_not_print=[], as_int=[],\n",
        "            avg=True):\n",
        "        line = '' if header is None else header + ': '\n",
        "        if epoch is not None:\n",
        "            line += ('epoch %d, ' % epoch)\n",
        "        if it is not None:\n",
        "            line += ('iter %d, ' % it)\n",
        "        if time is not None:\n",
        "            line += ('(%.3f secs), ' % time)\n",
        "\n",
        "        args = [arg for arg in self.args if arg not in do_not_print]\n",
        "\n",
        "        for arg in args[:-1]:\n",
        "            val = self.sums[self.argdict[arg]]\n",
        "            if avg:\n",
        "                val /= self.cnt\n",
        "            if arg in as_int:\n",
        "                line += ('%s %d, ' % (arg, int(val)))\n",
        "            else:\n",
        "                line += ('%s %f, ' % (arg, val))\n",
        "        val = self.sums[self.argdict[args[-1]]]\n",
        "        if avg:\n",
        "            val /= self.cnt\n",
        "        if arg in as_int:\n",
        "            line += ('%s %d, ' % (arg, int(val)))\n",
        "        else:\n",
        "            line += ('%s %f' % (args[-1], val))\n",
        "        print(line)\n",
        "\n",
        "        if logfile is not None:\n",
        "            logfile.write(line + '\\n')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZF1WUmZRIdss",
        "colab_type": "text"
      },
      "source": [
        "## Prepare the dataset: MNIST"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q21ZNy8bhTpL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "MNIST_PATH = './mnist'\n",
        "\n",
        "def mnist_input(batch_size):\n",
        "    mnist = input_data.read_data_sets(MNIST_PATH, one_hot=True, validation_size=0)\n",
        "    n_train_batches = mnist.train.num_examples/batch_size\n",
        "    n_test_batches = mnist.test.num_examples/batch_size\n",
        "    return mnist, n_train_batches, n_test_batches"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r3lKC6JjHRKM",
        "colab_type": "text"
      },
      "source": [
        "##Create models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FdS6-u0vIgs2",
        "colab_type": "text"
      },
      "source": [
        "### *아래 코드에서 Dropout Type을 바꿔주시면 됩니다.*\n",
        "- None\n",
        "\n",
        "- 'bbdropout'\n",
        "\n",
        "- 'sbpdropout'"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_mhIZqtYzn9i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# choose type: [None, 'bbdropout', 'sbpdropout']\n",
        "dropout_type = 'bbdropout'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yg7YnEexflxJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# fully connected layers로 구성된 lenet을 선언합니다. \n",
        "def lenet_dense(x, y, training, name='lenet', reuse=None,\n",
        "        dropout=None, **dropout_kwargs):\n",
        "    dropout_ = lambda x, subname: x if dropout is None else \\\n",
        "            dropout(x, training, name=name+subname, reuse=reuse,\n",
        "                    **dropout_kwargs)\n",
        "    x = dense(dropout_(x, '/dropout1'), 500, activation=relu,\n",
        "            name=name+'/dense1', reuse=reuse)\n",
        "    x = dense(dropout_(x, '/dropout2'), 300, activation=relu,\n",
        "            name=name+'/dense2', reuse=reuse)\n",
        "    x = dense(dropout_(x, '/dropout3'), 10, name=name+'/dense3', reuse=reuse)\n",
        "\n",
        "    net = {}\n",
        "    all_vars = tf.get_collection('variables', scope=name)\n",
        "    net['qpi_vars'] = [v for v in all_vars if 'qpi_vars' in v.name]\n",
        "    net['pzx_vars'] = [v for v in all_vars if 'pzx_vars' in v.name]\n",
        "    net['weights'] = [v for v in all_vars \\\n",
        "            if 'qpi_vars' not in v.name and 'pzx_vars' not in v.name]\n",
        "\n",
        "    net['cent'] = cross_entropy(x, y)\n",
        "    net['wd'] = weight_decay(1e-4, var_list=net['weights'])\n",
        "    net['acc'] = accuracy(x, y)\n",
        "\n",
        "    prefix = 'train_' if training else 'test_'\n",
        "    net['kl'] = tf.get_collection('kl')\n",
        "    net['pi'] = tf.get_collection(prefix+'pi')\n",
        "    net['n_active'] = tf.get_collection(prefix+'n_active')\n",
        "\n",
        "    return net"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NA7g-bi7IP6w",
        "colab_type": "text"
      },
      "source": [
        "## Define the Beta-Bernoulli Dropout"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tJjoi_-lga15",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lgamma = tf.lgamma\n",
        "Euler = 0.577215664901532\n",
        "\n",
        "def bbdropout(x, training,\n",
        "        alpha=1e-4, thres=1e-2, a_init=-1., tau=1e-1, center_init=1.0,\n",
        "        approx_digamma=True, scale_kl=None, dep=False,\n",
        "        unit_scale=True, collect=True,\n",
        "        name='bbdropout', reuse=None):\n",
        "\n",
        "    N = tf.shape(x)[0]\n",
        "    K = x.shape[1].value\n",
        "    is_conv = len(x.shape)==4\n",
        "    log = lambda x: tf.log(x + 1e-20)\n",
        "\n",
        "    with tf.variable_scope(name+'/qpi_vars', reuse=reuse):\n",
        "        with tf.device('/cpu:0'):\n",
        "            a = softplus(tf.get_variable('a_uc', shape=[K],\n",
        "                initializer=tf.constant_initializer(a_init)))\n",
        "            b = softplus(tf.get_variable('b_uc', shape=[K]))\n",
        "\n",
        "    _digamma = digamma_approx \n",
        "    kl = (a-alpha)/a * (-Euler - _digamma(b) - 1/b) \\\n",
        "            + log(a*b) - log(alpha) - (b-1)/b\n",
        "    pi = (1 - tf.random_uniform([K])**(1/b))**(1/a) if training else \\\n",
        "            b*tf.exp(lgamma(1+1/a) + lgamma(b) - lgamma(1+1/a+b))\n",
        "    \n",
        "    if training:\n",
        "        z = RelaxedBernoulli(tau, logits=logit(pi)).sample(N)\n",
        "    else:\n",
        "        pi_ = tf.where(tf.greater(pi, thres), pi, tf.zeros_like(pi))\n",
        "        z = tf.tile(tf.expand_dims(pi_, 0), [N, 1])\n",
        "    n_active = tf.reduce_sum(tf.cast(tf.greater(pi, thres), tf.int32))\n",
        "\n",
        "    if scale_kl is None:\n",
        "        kl = tf.reduce_sum(kl)\n",
        "    else:\n",
        "        kl = scale_kl * tf.reduce_mean(kl)\n",
        "\n",
        "    if collect:\n",
        "        if reuse is not True:\n",
        "            tf.add_to_collection('kl', kl)\n",
        "        prefix = 'train_' if training else 'test_'\n",
        "        tf.add_to_collection(prefix+'pi', pi)\n",
        "        tf.add_to_collection(prefix+'n_active', n_active)\n",
        "\n",
        "    z = tf.reshape(z, ([-1, K, 1, 1] if is_conv else [-1, K]))\n",
        "    return x*z\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eKqxcJya6aUc",
        "colab_type": "text"
      },
      "source": [
        "## Define the SBP Dropout"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xh0ibi_i6fAm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# copied from https://github.com/necludov/group-sparsity-sbp\n",
        "from tensorflow.python.ops.distributions import special_math\n",
        "\n",
        "def phi(x):\n",
        "    return 0.5*tf.erfc(-x/tf.sqrt(2.0))\n",
        "\n",
        "def __erfinv(x):\n",
        "    w = -tf.log((1.0-x)*(1.0+x)-1e-5)\n",
        "    p_small = 2.81022636e-08*tf.ones_like(x)\n",
        "    p_small = 3.43273939e-07 + p_small*(w-2.5)\n",
        "    p_small = -3.5233877e-06 + p_small*(w-2.5)\n",
        "    p_small = -4.39150654e-06 + p_small*(w-2.5)\n",
        "    p_small = 0.00021858087 + p_small*(w-2.5)\n",
        "    p_small = -0.00125372503 + p_small*(w-2.5)\n",
        "    p_small = -0.00417768164 + p_small*(w-2.5)\n",
        "    p_small = 0.246640727 + p_small*(w-2.5)\n",
        "    p_small = 1.50140941 + p_small*(w-2.5)\n",
        "\n",
        "    p_big = -0.000200214257*tf.ones_like(x)\n",
        "    p_big = 0.000100950558 + p_big*(tf.sqrt(w) - 3.0)\n",
        "    p_big = 0.00134934322 + p_big*(tf.sqrt(w) - 3.0)\n",
        "    p_big = -0.00367342844 + p_big*(tf.sqrt(w) - 3.0)\n",
        "    p_big = 0.00573950773 + p_big*(tf.sqrt(w) - 3.0)\n",
        "    p_big = -0.0076224613 + p_big*(tf.sqrt(w) - 3.0)\n",
        "    p_big = 0.00943887047 + p_big*(tf.sqrt(w) - 3.0)\n",
        "    p_big = 1.00167406 + p_big*(tf.sqrt(w) - 3.0)\n",
        "    p_big = 2.83297682 + p_big*(tf.sqrt(w) - 3.0)\n",
        "\n",
        "    small_mask = tf.cast(tf.less(w, 5.0*tf.ones_like(w)), tf.float32)\n",
        "    big_mask = tf.cast(tf.greater_equal(w, 5.0*tf.ones_like(w)), tf.float32)\n",
        "    p = p_small*small_mask + p_big*big_mask\n",
        "    return p*x\n",
        "\n",
        "def erfinv(x):\n",
        "    return special_math.ndtri((x+1.)/2.0)/tf.sqrt(2.)\n",
        "\n",
        "def erfcx(x):\n",
        "    \"\"\"M. M. Shepherd and J. G. Laframboise,\n",
        "       MATHEMATICS OF COMPUTATION 36, 249 (1981)\n",
        "    \"\"\"\n",
        "    K = 3.75\n",
        "    y = (tf.abs(x)-K) / (tf.abs(x)+K)\n",
        "    y2 = 2.0*y\n",
        "    (d, dd) = (-0.4e-20, 0.0)\n",
        "    (d, dd) = (y2 * d - dd + 0.3e-20, d)\n",
        "    (d, dd) = (y2 * d - dd + 0.97e-19, d)\n",
        "    (d, dd) = (y2 * d - dd + 0.27e-19, d)\n",
        "    (d, dd) = (y2 * d - dd + -0.2187e-17, d)\n",
        "    (d, dd) = (y2 * d - dd + -0.2237e-17, d)\n",
        "    (d, dd) = (y2 * d - dd + 0.50681e-16, d)\n",
        "    (d, dd) = (y2 * d - dd + 0.74182e-16, d)\n",
        "    (d, dd) = (y2 * d - dd + -0.1250795e-14, d)\n",
        "    (d, dd) = (y2 * d - dd + -0.1864563e-14, d)\n",
        "    (d, dd) = (y2 * d - dd + 0.33478119e-13, d)\n",
        "    (d, dd) = (y2 * d - dd + 0.32525481e-13, d)\n",
        "    (d, dd) = (y2 * d - dd + -0.965469675e-12, d)\n",
        "    (d, dd) = (y2 * d - dd + 0.194558685e-12, d)\n",
        "    (d, dd) = (y2 * d - dd + 0.28687950109e-10, d)\n",
        "    (d, dd) = (y2 * d - dd + -0.63180883409e-10, d)\n",
        "    (d, dd) = (y2 * d - dd + -0.775440020883e-09, d)\n",
        "    (d, dd) = (y2 * d - dd + 0.4521959811218e-08, d)\n",
        "    (d, dd) = (y2 * d - dd + 0.10764999465671e-07, d)\n",
        "    (d, dd) = (y2 * d - dd + -0.218864010492344e-06, d)\n",
        "    (d, dd) = (y2 * d - dd + 0.774038306619849e-06, d)\n",
        "    (d, dd) = (y2 * d - dd + 0.4139027986073010e-05, d)\n",
        "    (d, dd) = (y2 * d - dd + -0.69169733025012064e-04, d)\n",
        "    (d, dd) = (y2 * d - dd + 0.490775836525808632e-03, d)\n",
        "    (d, dd) = (y2 * d - dd + -0.2413163540417608191e-02, d)\n",
        "    (d, dd) = (y2 * d - dd + 0.9074997670705265094e-02, d)\n",
        "    (d, dd) = (y2 * d - dd + -0.26658668435305752277e-01, d)\n",
        "    (d, dd) = (y2 * d - dd + 0.59209939998191890498e-01, d)\n",
        "    (d, dd) = (y2 * d - dd + -0.84249133366517915584e-01, d)\n",
        "    (d, dd) = (y2 * d - dd + -0.4590054580646477331e-02, d)\n",
        "    d = y * d - dd + 0.1177578934567401754080e+01\n",
        "    result = d/(1.0+2.0*tf.abs(x))\n",
        "    result = tf.where(tf.is_nan(result), tf.ones_like(result), result)\n",
        "    result = tf.where(tf.is_inf(result), tf.ones_like(result), result)\n",
        "\n",
        "    negative_mask = tf.cast(tf.less(x, 0.0), tf.float32)\n",
        "    positive_mask = tf.cast(tf.greater_equal(x, 0.0), tf.float32)\n",
        "    negative_result = 2.0*tf.exp(x*x)-result\n",
        "    negative_result = tf.where(tf.is_nan(negative_result), tf.ones_like(negative_result), negative_result)\n",
        "    negative_result = tf.where(tf.is_inf(negative_result), tf.ones_like(negative_result), negative_result)\n",
        "    result = negative_mask * negative_result + positive_mask * result\n",
        "    return result\n",
        "\n",
        "def phi_inv(x):\n",
        "    return tf.sqrt(2.0)*erfinv(2.0*x-1)\n",
        "\n",
        "def mean_truncated_log_normal_straight(mu, sigma, a, b):\n",
        "    alpha = (a - mu)/sigma\n",
        "    beta = (b - mu)/sigma\n",
        "    z = phi(beta) - phi(alpha)\n",
        "    mean = tf.exp(mu+sigma*sigma/2.0)/z*(phi(sigma-alpha) - phi(sigma-beta))\n",
        "    return mean\n",
        "\n",
        "def mean_truncated_log_normal_reduced(mu, sigma, a, b):\n",
        "    alpha = (a - mu)/sigma\n",
        "    beta = (b - mu)/sigma\n",
        "    z = phi(beta) - phi(alpha)\n",
        "    mean = erfcx((sigma-beta)/tf.sqrt(2.0))*tf.exp(b-beta*beta/2)\n",
        "    mean = mean - erfcx((sigma-alpha)/tf.sqrt(2.0))*tf.exp(a-alpha*alpha/2)\n",
        "    mean = mean/(2*z)\n",
        "    return mean\n",
        "\n",
        "def mean_truncated_log_normal(mu, sigma, a, b):\n",
        "    return mean_truncated_log_normal_reduced(mu, sigma, a, b)\n",
        "\n",
        "def median_truncated_log_normal(mu, sigma, a, b):\n",
        "    alpha = (a - mu)/sigma\n",
        "    beta = (b - mu)/sigma\n",
        "    gamma = phi(alpha)+0.5*(phi(beta)-phi(alpha))\n",
        "    return tf.exp(phi_inv(gamma)*sigma+mu)\n",
        "\n",
        "def snr_truncated_log_normal(mu, sigma, a, b):\n",
        "    alpha = (a - mu)/sigma\n",
        "    beta = (b - mu)/sigma\n",
        "    z = phi(beta) - phi(alpha)\n",
        "    ratio = erfcx((sigma-beta)/tf.sqrt(2.0))*tf.exp((b-mu)-beta**2/2.0)\n",
        "    ratio = ratio - erfcx((sigma-alpha)/tf.sqrt(2.0))*tf.exp((a-mu)-alpha**2/2.0)\n",
        "    denominator = 2*z*erfcx((2.0*sigma-beta)/tf.sqrt(2.0))*tf.exp(2.0*(b-mu)-beta**2/2.0)\n",
        "    denominator = denominator - 2*z*erfcx((2.0*sigma-alpha)/tf.sqrt(2.0))*tf.exp(2.0*(a-mu)-alpha**2/2.0)\n",
        "    denominator = denominator - ratio**2\n",
        "    ratio = ratio/tf.sqrt(denominator)\n",
        "    return ratio\n",
        "\n",
        "def sample_truncated_normal(mu, sigma, a, b):\n",
        "    alpha = (a - mu)/sigma\n",
        "    beta = (b - mu)/sigma\n",
        "    gamma = phi(alpha)+tf.random_uniform(mu.shape)*(phi(beta)-phi(alpha))\n",
        "    return tf.clip_by_value(phi_inv(tf.clip_by_value(gamma, 1e-5, 1.0-1e-5))*sigma+mu, a, b)\n",
        "\n",
        "def sbpdropout(x, training,\n",
        "        thres=1.0, scale_kl=None, collect=True,\n",
        "        name='sbpdropout', reuse=None):\n",
        "\n",
        "    min_log = -20.0\n",
        "    max_log = 0.0\n",
        "\n",
        "    axis = 1\n",
        "\n",
        "    params_shape = np.ones(x.get_shape().ndims)\n",
        "    params_shape[axis] = x.get_shape()[axis].value\n",
        "\n",
        "    with tf.variable_scope(name+'/qpi_vars', reuse=reuse):\n",
        "        with tf.device('/cpu:0'):\n",
        "            mu = tf.get_variable('mu', shape=params_shape.tolist(),\n",
        "                                 initializer=tf.zeros_initializer())\n",
        "            log_sigma = tf.get_variable('log_sigma', shape=params_shape.tolist(),\n",
        "                    initializer=tf.constant_initializer(-5.0))\n",
        "\n",
        "    mu = tf.clip_by_value(mu, -20.0, 5.0)\n",
        "    log_sigma = tf.clip_by_value(log_sigma, -20.0, 5.0)\n",
        "    sigma = tf.exp(log_sigma)\n",
        "\n",
        "    # adding loss\n",
        "    alpha = (min_log-mu)/sigma\n",
        "    beta = (max_log-mu)/sigma\n",
        "    z = phi(beta) - phi(alpha)\n",
        "\n",
        "    def pdf(x):\n",
        "        return tf.exp(-x*x/2.0)/tf.sqrt(2.0*np.pi)\n",
        "    kl = -log_sigma-tf.log(z)-(alpha*pdf(alpha)-beta*pdf(beta))/(2.0*z)\n",
        "    kl = kl+tf.log(max_log-min_log)-tf.log(2.0*np.pi*np.e)/2.0\n",
        "    if scale_kl is None:\n",
        "        kl = tf.reduce_sum(kl)\n",
        "    else:\n",
        "        kl = scale_kl*tf.reduce_mean(kl)\n",
        "\n",
        "    if training:\n",
        "        z = tf.exp(sample_truncated_normal(mu, sigma, min_log, max_log))\n",
        "    else:\n",
        "        z = mean_truncated_log_normal(mu, sigma, min_log, max_log)\n",
        "    snr = snr_truncated_log_normal(mu, sigma, min_log, max_log)\n",
        "    mask = tf.cast(tf.greater(snr, thres*tf.ones_like(snr)), tf.float32)\n",
        "\n",
        "    n_active = tf.reduce_sum(tf.cast(mask, tf.int32))\n",
        "\n",
        "    if collect:\n",
        "        if reuse is not True:\n",
        "            tf.add_to_collection('kl', kl)\n",
        "        prefix = 'train_' if training else 'test_'\n",
        "        tf.add_to_collection(prefix+'pi', snr)\n",
        "        tf.add_to_collection(prefix+'n_active', n_active)\n",
        "\n",
        "    if not training:\n",
        "        z = mask*z\n",
        "\n",
        "    return x*z\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "InEG2TA3Iilj",
        "colab_type": "text"
      },
      "source": [
        "## Let's run the code!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rz6djPqUWps9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tf.reset_default_graph() # 기존의 그려진 텐서플로우 그래프를 제거합니다.\n",
        "\n",
        "pretraindir = './results/pretrained' \n",
        "savedir = './results/bbdropout/sample_run' \n",
        "if not os.path.isdir(savedir):\n",
        "    os.makedirs(savedir)\n",
        "\n",
        "batch_size = 100\n",
        "n_epochs = 60\n",
        "save_freq = 20\n",
        "mnist, n_train_batches, n_test_batches = mnist_input(batch_size)\n",
        "x = tf.placeholder(tf.float32, [None, 784])\n",
        "y = tf.placeholder(tf.float32, [None, 10])\n",
        "N = mnist.train.num_examples\n",
        "if dropout_type == None:\n",
        "  dropout = None\n",
        "  print('dropout None')\n",
        "elif dropout_type == 'sbpdropout':\n",
        "  dropout = sbpdropout\n",
        "  print('sbpdropout')\n",
        "elif dropout_type == 'bbdropout':\n",
        "  dropout = bbdropout\n",
        "  print('bbdropout')\n",
        "\n",
        "  \n",
        "net = lenet_dense(x, y, True, dropout=dropout)\n",
        "tnet = lenet_dense(x, y, False, reuse=True, dropout=dropout)\n",
        "\n",
        "def train():\n",
        "    if dropout_type == None:\n",
        "      loss = net['cent'] + net['wd'] \n",
        "    else:\n",
        "      loss = net['cent'] + tf.add_n(net['kl'])/float(N) + net['wd'] \n",
        "    global_step = tf.train.get_or_create_global_step()\n",
        "    bdr = [int(n_train_batches*(n_epochs-1)*r) for r in [0.5, 0.75]]\n",
        "    vals = [1e-2, 1e-3, 1e-4]\n",
        "    lr = tf.train.piecewise_constant(tf.cast(global_step, tf.int32), bdr, vals)\n",
        "\n",
        "    if dropout_type == None:\n",
        "      train_op = tf.train.AdamOptimizer(0.1*lr).minimize(loss,\n",
        "              var_list=net['weights'])\n",
        "    else:\n",
        "      train_op1 = tf.train.AdamOptimizer(lr).minimize(loss,\n",
        "              var_list=net['qpi_vars'], global_step=global_step)\n",
        "      train_op2 = tf.train.AdamOptimizer(0.1*lr).minimize(loss,\n",
        "              var_list=net['weights'])\n",
        "      train_op = tf.group(train_op1, train_op2)\n",
        "\n",
        "    pretrain_saver = tf.train.Saver(net['weights'])\n",
        "    saver = tf.train.Saver(net['weights']+net['qpi_vars'])\n",
        "    logfile = open(os.path.join(savedir, 'train.log'), 'w', 0)\n",
        "\n",
        "    sess = tf.Session()\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "    pretrain_saver.restore(sess, os.path.join(pretraindir, 'model'))\n",
        "\n",
        "    train_logger = Accumulator('cent', 'acc')\n",
        "    train_to_run = [train_op, net['cent'], net['acc']]\n",
        "    test_logger = Accumulator('cent', 'acc')\n",
        "    test_to_run = [tnet['cent'], tnet['acc']]\n",
        "    for i in range(n_epochs):\n",
        "        line = 'Epoch %d start, learning rate %f' % (i+1, sess.run(lr))\n",
        "        #print(line)\n",
        "        logfile.write(line + '\\n')\n",
        "        train_logger.clear()\n",
        "        start = time.time()\n",
        "        for j in range(n_train_batches):\n",
        "            bx, by = mnist.train.next_batch(batch_size)\n",
        "            train_logger.accum(sess.run(train_to_run, {x:bx, y:by}))\n",
        "        train_logger.print_(header='train', epoch=i+1,\n",
        "                time=time.time()-start, logfile=logfile)\n",
        "\n",
        "        test_logger.clear()\n",
        "        for j in range(n_test_batches):\n",
        "            bx, by = mnist.test.next_batch(batch_size)\n",
        "            test_logger.accum(sess.run(test_to_run, {x:bx, y:by}))\n",
        "        test_logger.print_(header='test', epoch=i+1,\n",
        "                time=time.time()-start, logfile=logfile)\n",
        "        #line = 'kl: ' + str(sess.run(tnet['kl'])) + '\\n'\n",
        "        if dropout_type == None:\n",
        "          pass\n",
        "        else:\n",
        "          line += '\\nn_active: ' + str(sess.run(tnet['n_active'])) + '\\n'\n",
        "          print(line)\n",
        "          logfile.write(line+'\\n')\n",
        "\n",
        "        if (i+1)% save_freq == 0:\n",
        "            saver.save(sess, os.path.join(savedir, 'model'))\n",
        "\n",
        "    logfile.close()\n",
        "    saver.save(sess, os.path.join(savedir, 'model'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s_0exUrqhel8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O72sZ36dIosC",
        "colab_type": "text"
      },
      "source": [
        "## Test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iYDT-eQ1kwQz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def test():\n",
        "    sess = tf.Session()\n",
        "    saver = tf.train.Saver(tnet['weights']+tnet['qpi_vars'])\n",
        "    saver.restore(sess, os.path.join(savedir, 'model'))\n",
        "    logger = Accumulator('cent', 'acc')\n",
        "    to_run = [tnet['cent'], tnet['acc']]\n",
        "    for j in range(n_test_batches):\n",
        "        bx, by = mnist.test.next_batch(batch_size)\n",
        "        logger.accum(sess.run(to_run, {x:bx, y:by}))\n",
        "    logger.print_(header='test')\n",
        "    \n",
        "    if dropout_type == None:\n",
        "      pass\n",
        "    else:\n",
        "      n_active = sess.run(tnet['n_active'])\n",
        "      print(\"The percentage of activated neurons per layer:\")\n",
        "      for na, nl in zip(n_active, [784, 500, 300]):\n",
        "        print('{}/{} = {:.2f}%'.format(na, nl, float(na)/nl * 100))\n",
        "    \n",
        "test()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-8y4uwf-IqiR",
        "colab_type": "text"
      },
      "source": [
        "## Visualization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n0xBULDNoNG_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def visualize():\n",
        "    sess = tf.Session()\n",
        "    saver = tf.train.Saver(tnet['weights']+tnet['qpi_vars'])\n",
        "    saver.restore(sess, os.path.join(savedir, 'model'))\n",
        "\n",
        "    n_drop = len(tnet['n_active'])\n",
        "    fig = figure('pi', figsize=(8,6))\n",
        "    axarr = fig.subplots(n_drop)\n",
        "    for i in range(n_drop):\n",
        "        np_pi = sess.run(tnet['pi'][i]).reshape((1,-1))\n",
        "        im = axarr[i].imshow(np_pi, cmap='Blues', aspect='auto')\n",
        "        axarr[i].yaxis.set_visible(False)\n",
        "        axarr[i].xaxis.set_major_locator(MaxNLocator(integer=True))\n",
        "        if i == n_drop-1:\n",
        "            axarr[i].set_xlabel('The Number of Neurons\\nLeNet [784, 500, 300]')\n",
        "        fig.colorbar(im, ax=axarr[i])\n",
        "    show()\n",
        "visualize()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3pfWC53vFdNw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}
