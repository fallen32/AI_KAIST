{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment on Regularization and Optimization of Deep Learning\n",
    "\n",
    "이번 과제에서는 reagularization과 optimization에서 배운 내용들을 이용해 최대한 Deep Models의 성능을 높여보고자 합니다. Layer 4개짜리 MLP (각 hidden layer는 512개의 unit을 가짐) 상황에서 정규화와 최적화 방법론들을 총 동원해 성능을 높여주시면 됩니다.\n",
    "\n",
    "먼저, 아래 코드는 데이터 셋을 셋팅하는 부분입니다. 이 부분은 건드리시면 안됩니다. 이 부분을 건드리시면 0점 처리 됩니다. 외부 데이터 사용하셔도 안됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 32, 32, 3)\n",
      "(1000,)\n",
      "(5000, 32, 32, 3)\n",
      "(5000,)\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import math\n",
    "import random \n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "seed = 1\n",
    "random.seed(seed)\n",
    "np.random.seed(seed=seed)\n",
    "tf.random.set_random_seed(seed)\n",
    "\n",
    "(x_1, y_1), (x_2, y_2) = tf.keras.datasets.cifar100.load_data()\n",
    "x_total = np.concatenate([x_1, x_2], axis=0).astype(np.float64)\n",
    "y_total = np.concatenate([y_1, y_2], axis=0)\n",
    "\n",
    "n_output = 10\n",
    "\n",
    "valid_index, _ = np.where(y_total < n_output)\n",
    "y_total = y_total[valid_index].reshape([-1])\n",
    "x_total = x_total[valid_index]\n",
    "\n",
    "i = np.arange(x_total.shape[0])\n",
    "np.random.shuffle(i)\n",
    "x_total = x_total[i]\n",
    "y_total = y_total[i]\n",
    "\n",
    "train_size = 100 * n_output\n",
    "x_train = x_total[:train_size]\n",
    "y_train = y_total[:train_size]\n",
    "x_test = x_total[train_size:]\n",
    "y_test = y_total[train_size:]\n",
    "\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "print(x_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "validation set을 나눕니다. \n",
    "- 실습시간에 배웠던 것처럼 Validation set 비율은 조정하셔도 됩니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "split = x_train.shape[0] // 5\n",
    "x_valid = x_train[:split]\n",
    "y_valid = y_train[:split]\n",
    "\n",
    "x_train = x_train[split:]\n",
    "y_train = y_train[split:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이미지를 greyscale로 변경합니다. \n",
    "1. RGB 값을 고려한 코드로 변경하셔도 됩니다. \n",
    "2. Augmentation을 고려해보세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(800, 32, 32)\n",
      "(200, 32, 32)\n",
      "(5000, 32, 32)\n",
      "(800, 1024)\n",
      "(200, 1024)\n",
      "(5000, 1024)\n"
     ]
    }
   ],
   "source": [
    "x_train = np.mean(x_train, axis=3)\n",
    "x_valid = np.mean(x_valid, axis=3)\n",
    "x_test = np.mean(x_test, axis=3)\n",
    "print(x_train.shape)\n",
    "print(x_valid.shape)\n",
    "print(x_test.shape)\n",
    "\n",
    "n_input = 32 * 32\n",
    "\n",
    "x_train = x_train.reshape([-1, n_input])\n",
    "x_valid = x_valid.reshape([-1, n_input])\n",
    "x_test = x_test.reshape([-1, n_input])\n",
    "\n",
    "print(x_train.shape)\n",
    "print(x_valid.shape)\n",
    "print(x_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 모델을 만듭니다.\n",
    "\n",
    "1. Optimizer를 다른 걸로 바꿔보세요\n",
    "2. Learning Rate를 바꿔보세요. Learning Rate Scheduling도 고려해보세요.\n",
    "3. Activation Function을 바꿔보세요. \n",
    "4. Dropout, DropConnect, Gaussian Dropout 을 고려해보세요.\n",
    "5. Augmentation을 고려해보세요. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.float32, [None, n_input])\n",
    "y = tf.placeholder(tf.int32, [None])\n",
    "training = tf.placeholder(tf.bool)\n",
    "\n",
    "n_units = [n_input, 512, 512, 512, n_output]\n",
    "\n",
    "weights, biases = [], []\n",
    "for i, (n_in, n_out) in enumerate(zip(n_units[:-1], n_units[1:])):\n",
    "    stddev = math.sqrt(2 / n_in) # Kaiming He Initialization\n",
    "    weight = tf.Variable(tf.random.truncated_normal([n_in, n_out], mean=0, stddev=stddev))\n",
    "    bias = tf.Variable(tf.zeros([n_out]))\n",
    "    weights.append(weight)\n",
    "    biases.append(bias)\n",
    "    \n",
    "layer = x \n",
    "for i, (weight, bias) in enumerate(zip(weights, biases)):\n",
    "    layer = tf.matmul(layer, weight) + bias\n",
    "    if i < len(weights) - 1:\n",
    "        layer = tf.nn.sigmoid(layer)  \n",
    "y_hat = layer\n",
    "\n",
    "y_hot = tf.one_hot(y, n_output)\n",
    "costs = tf.nn.softmax_cross_entropy_with_logits_v2(\n",
    "        labels=y_hot, logits=y_hat)\n",
    "cross_entropy_loss = tf.reduce_mean(costs)\n",
    "loss = cross_entropy_loss \n",
    "\n",
    "y_label = tf.argmax(y_hat, 1)\n",
    "accuracy = tf.count_nonzero(\n",
    "        tf.cast(tf.equal(tf.argmax(y_hot, 1), y_label),\n",
    "                tf.int64)) / tf.cast(tf.shape(y_hot)[0], tf.int64)\n",
    "\n",
    "extra_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "with tf.control_dependencies(extra_ops):\n",
    "    optimizer = tf.train.AdamOptimizer(1e-4)\n",
    "    train_op = optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 2.2726 2.2874 2.2967 0.1163 0.1150 0.0966\n",
      "20 2.2481 2.2824 2.2798 0.2050 0.1250 0.1528\n",
      "30 2.2209 2.2773 2.2655 0.2338 0.1350 0.1756\n",
      "40 2.1894 2.2586 2.2443 0.2437 0.1400 0.1794\n",
      "50 2.1554 2.2410 2.2238 0.2888 0.1600 0.1966\n",
      "60 2.1166 2.2265 2.2013 0.2925 0.1500 0.1996\n",
      "70 2.0738 2.2060 2.1768 0.3412 0.1800 0.2186\n",
      "80 2.0283 2.1880 2.1532 0.3638 0.1850 0.2314\n",
      "90 1.9813 2.1684 2.1303 0.3825 0.2000 0.2464\n",
      "100 1.9332 2.1494 2.1087 0.4213 0.2150 0.2588\n",
      "110 1.8838 2.1299 2.0898 0.4512 0.2350 0.2650\n",
      "120 1.8336 2.1128 2.0717 0.4763 0.2550 0.2736\n",
      "130 1.7822 2.0968 2.0558 0.5038 0.2550 0.2798\n",
      "140 1.7292 2.0842 2.0397 0.5275 0.2500 0.2882\n",
      "150 1.6735 2.0672 2.0257 0.5550 0.2650 0.2948\n",
      "160 1.6181 2.0528 2.0112 0.5825 0.2750 0.3048\n",
      "170 1.5606 2.0429 1.9999 0.6100 0.2750 0.3116\n",
      "180 1.5020 2.0293 1.9885 0.6250 0.2700 0.3134\n",
      "190 1.4427 2.0279 1.9803 0.6425 0.2800 0.3152\n",
      "200 1.3856 2.0248 1.9740 0.6500 0.2700 0.3230\n",
      "210 1.3249 2.0164 1.9677 0.6713 0.2700 0.3250\n",
      "220 1.2656 2.0146 1.9659 0.6963 0.2700 0.3258\n",
      "230 1.2098 2.0137 1.9644 0.7150 0.2700 0.3254\n",
      "240 1.1511 2.0230 1.9635 0.7225 0.2750 0.3280\n",
      "250 1.0933 2.0283 1.9631 0.7512 0.2850 0.3308\n",
      "260 1.0350 2.0347 1.9678 0.7762 0.2950 0.3314\n",
      "270 0.9772 2.0392 1.9733 0.7975 0.2800 0.3276\n",
      "280 0.9274 2.0479 1.9793 0.8125 0.2900 0.3306\n",
      "290 0.8719 2.0678 1.9856 0.8250 0.2950 0.3310\n",
      "300 0.8181 2.0775 1.9966 0.8413 0.2900 0.3298\n",
      "310 0.7692 2.0906 2.0083 0.8575 0.2700 0.3304\n",
      "320 0.7186 2.1074 2.0171 0.8800 0.2800 0.3290\n",
      "330 0.6765 2.1221 2.0317 0.8975 0.2850 0.3284\n",
      "340 0.6342 2.1368 2.0442 0.9050 0.3000 0.3280\n",
      "350 0.5940 2.1690 2.0636 0.9100 0.3000 0.3266\n",
      "360 0.5503 2.1923 2.0729 0.9213 0.3050 0.3288\n",
      "370 0.5060 2.2006 2.0908 0.9263 0.2800 0.3248\n",
      "380 0.4703 2.2304 2.1087 0.9375 0.2750 0.3216\n",
      "390 0.4343 2.2500 2.1259 0.9500 0.2750 0.3212\n",
      "400 0.4027 2.2713 2.1436 0.9600 0.2750 0.3202\n",
      "410 0.3732 2.2727 2.1651 0.9613 0.2750 0.3178\n",
      "420 0.3445 2.2958 2.1823 0.9637 0.2850 0.3180\n",
      "430 0.3193 2.3247 2.2020 0.9675 0.2850 0.3162\n",
      "440 0.2972 2.3413 2.2217 0.9712 0.2800 0.3144\n",
      "450 0.2764 2.3668 2.2448 0.9738 0.2900 0.3130\n",
      "460 0.2594 2.3789 2.2649 0.9725 0.2800 0.3126\n",
      "0.3288\n"
     ]
    }
   ],
   "source": [
    "gpu_options = tf.GPUOptions()\n",
    "gpu_options.allow_growth = True\n",
    "session = tf.Session(config=tf.ConfigProto(gpu_options=gpu_options))\n",
    "session.run(tf.global_variables_initializer())\n",
    "\n",
    "max_valid_epoch_idx = 0\n",
    "max_valid_accuracy = 0.0\n",
    "final_test_accuracy = 0.0\n",
    "for epoch_idx in range(1, 10000 + 1):\n",
    "    \n",
    "    session.run(\n",
    "            train_op,\n",
    "            feed_dict={\n",
    "                x: x_train,\n",
    "                y: y_train,\n",
    "                training: True\n",
    "            })\n",
    "    \n",
    "    if epoch_idx % 10 == 0:\n",
    "        train_loss_value, train_accuracy_value = session.run(\n",
    "            [loss, accuracy],\n",
    "            feed_dict={\n",
    "                x: x_train,\n",
    "                y: y_train,\n",
    "                training: False\n",
    "            })\n",
    "        \n",
    "        valid_loss_value, valid_accuracy_value = session.run(\n",
    "            [loss, accuracy],\n",
    "            feed_dict={\n",
    "                x: x_valid,\n",
    "                y: y_valid,\n",
    "                training: False\n",
    "            })\n",
    "            \n",
    "        test_loss_value, test_accuracy_value = session.run(\n",
    "            [loss, accuracy],\n",
    "            feed_dict={\n",
    "                x: x_test,\n",
    "                y: y_test,\n",
    "                training: False\n",
    "            })\n",
    "\n",
    "        print(epoch_idx, '%.4f' % train_loss_value, '%.4f' % valid_loss_value, '%.4f' % test_loss_value, '%.4f' % train_accuracy_value, '%.4f' % valid_accuracy_value, '%.4f' % test_accuracy_value)\n",
    "        \n",
    "        if max_valid_accuracy < valid_accuracy_value:\n",
    "            max_valid_accuracy = valid_accuracy_value \n",
    "            max_valid_epoch_idx = epoch_idx\n",
    "            final_test_accuracy = test_accuracy_value\n",
    "            \n",
    "    # Early Stop\n",
    "    if max_valid_epoch_idx + 100 < epoch_idx:\n",
    "        break\n",
    "        \n",
    "print(final_test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "32.88% 의 성능을 확인할 수 있습니다. 실습시간 배운 몇 가지 정규화와 최적화 과정을 동원하면 50% 정도의 성능까지는 쉽게 달성할 수 있음을 확인했습니다. 수업시간에 배운 내용들을 사용해 최대한 높은 성능을 나타내는 모델을 만들어보세요! \n",
    "주피터 노트북 파일을 제출해주시면 되며, 성능을 기준으로 점수를 매길 예정입니다. (상대평가)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
