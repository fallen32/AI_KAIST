{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment on Regularization and Optimization of Deep Learning\n",
    "\n",
    "이번 과제에서는 reagularization과 optimization에서 배운 내용들을 이용해 최대한 Deep Models의 성능을 높여보고자 합니다. Layer 4개짜리 MLP (각 hidden layer는 512개의 unit을 가짐) 상황에서 정규화와 최적화 방법론들을 총 동원해 성능을 높여주시면 됩니다.\n",
    "\n",
    "먼저, 아래 코드는 데이터 셋을 셋팅하는 부분입니다. 이 부분은 건드리시면 안됩니다. 이 부분을 건드리시면 0점 처리 됩니다. 외부 데이터 사용하셔도 안됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 32, 32, 3)\n",
      "(1000,)\n",
      "(5000, 32, 32, 3)\n",
      "(5000,)\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import math\n",
    "import random \n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "seed = 1\n",
    "random.seed(seed)\n",
    "np.random.seed(seed=seed)\n",
    "tf.random.set_random_seed(seed)\n",
    "\n",
    "(x_1, y_1), (x_2, y_2) = tf.keras.datasets.cifar100.load_data()\n",
    "x_total = np.concatenate([x_1, x_2], axis=0).astype(np.float64)\n",
    "y_total = np.concatenate([y_1, y_2], axis=0)\n",
    "\n",
    "n_output = 10\n",
    "\n",
    "valid_index, _ = np.where(y_total < n_output)\n",
    "y_total = y_total[valid_index].reshape([-1])\n",
    "x_total = x_total[valid_index]\n",
    "\n",
    "i = np.arange(x_total.shape[0])\n",
    "np.random.shuffle(i)\n",
    "x_total = x_total[i]\n",
    "y_total = y_total[i]\n",
    "\n",
    "train_size = 100 * n_output\n",
    "x_train = x_total[:train_size]\n",
    "y_train = y_total[:train_size]\n",
    "x_test = x_total[train_size:]\n",
    "y_test = y_total[train_size:]\n",
    "\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "print(x_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "validation set을 나눕니다. \n",
    "- 실습시간에 배웠던 것처럼 Validation set 비율은 조정하셔도 됩니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "split = x_train.shape[0] // 5\n",
    "x_valid = x_train[:split]\n",
    "y_valid = y_train[:split]\n",
    "\n",
    "x_train = x_train[split:]\n",
    "y_train = y_train[split:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이미지를 greyscale로 변경합니다. \n",
    "1. RGB 값을 고려한 코드로 변경하셔도 됩니다. \n",
    "2. Augmentation을 고려해보세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(800, 32, 32, 3)\n",
      "(200, 32, 32, 3)\n",
      "(5000, 32, 32, 3)\n"
     ]
    }
   ],
   "source": [
    "# x_train = np.mean(x_train, axis=3)\n",
    "# x_valid = np.mean(x_valid, axis=3)\n",
    "# x_test = np.mean(x_test, axis=3)\n",
    "# Use RGB and normalize pixel values by training data-channel\n",
    "mean = np.zeros(3)\n",
    "std = np.ones(3)\n",
    "for i in range(3):\n",
    "    mean[i] = np.mean(x_train[:, :, :, i])\n",
    "    std[i] = np.std(x_train[:, :, :, i])    \n",
    "x_train = (x_train - mean) / std\n",
    "x_valid = (x_valid - mean) / std\n",
    "x_test = (x_test - mean) / std\n",
    "\n",
    "epoch = 10000\n",
    "batch_size = 64\n",
    "\n",
    "print(x_train.shape)\n",
    "print(x_valid.shape)\n",
    "print(x_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make Data Generator\n",
    "for augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "datagen = tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "    width_shift_range=5,\n",
    "    height_shift_range=5,\n",
    "    fill_mode='constant',\n",
    "    horizontal_flip=True,\n",
    "    vertical_flip=False,\n",
    "    rotation_range=15,\n",
    "    zoom_range=[0.8, 1.2]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(200, 3072)\n",
      "(5000, 3072)\n"
     ]
    }
   ],
   "source": [
    "n_input = 32 * 32 * 3   # H * W * C\n",
    "\n",
    "# x_train = x_train.reshape([-1, n_input]) This will be done after augmentation.\n",
    "x_valid = x_valid.reshape([-1, n_input])\n",
    "x_test = x_test.reshape([-1, n_input])\n",
    "\n",
    "print(x_valid.shape)\n",
    "print(x_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 모델을 만듭니다.\n",
    "\n",
    "1. Optimizer를 다른 걸로 바꿔보세요\n",
    "2. Learning Rate를 바꿔보세요. Learning Rate Scheduling도 고려해보세요.\n",
    "3. Activation Function을 바꿔보세요. \n",
    "4. Dropout, DropConnect, Gaussian Dropout 을 고려해보세요.\n",
    "5. Augmentation을 고려해보세요. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.float32, [None, n_input])\n",
    "y = tf.placeholder(tf.int32, [None])\n",
    "training = tf.placeholder(tf.bool)\n",
    "\n",
    "n_units = [n_input, 512, 512, 512, n_output]\n",
    "\n",
    "weights, biases = [], []\n",
    "for i, (n_in, n_out) in enumerate(zip(n_units[:-1], n_units[1:])):\n",
    "    stddev = math.sqrt(2 / n_in) # Kaiming He Initialization\n",
    "    weight = tf.Variable(tf.random.truncated_normal([n_in, n_out], mean=0, stddev=stddev))\n",
    "    bias = tf.Variable(tf.zeros([n_out]))\n",
    "    weights.append(weight)\n",
    "    biases.append(bias)\n",
    "    \n",
    "layer = x\n",
    "for i, (weight, bias) in enumerate(zip(weights, biases)):\n",
    "    if i < len(weights) - 1:\n",
    "        rate = tf.cond(training, lambda: 0.5, lambda: 0.0)\n",
    "        weight = tf.nn.dropout(weight, rate=rate) * (1 - rate) # DropConnect\n",
    "        layer = tf.matmul(layer, weight) + bias\n",
    "        layer = tf.nn.relu(layer)\n",
    "    else:\n",
    "        layer = tf.matmul(layer, weight) + bias\n",
    "y_hat = layer\n",
    "\n",
    "y_hot = tf.one_hot(y, n_output)\n",
    "costs = tf.nn.softmax_cross_entropy_with_logits_v2(\n",
    "        labels=y_hot, logits=y_hat)\n",
    "cross_entropy_loss = tf.reduce_mean(costs)\n",
    "# L2-regularization\n",
    "l2_loss = 0.0\n",
    "for weight in weights:\n",
    "    l2_loss = l2_loss + tf.nn.l2_loss(weight)\n",
    "loss = tf.reduce_mean(cross_entropy_loss + 0.001 * l2_loss)\n",
    "\n",
    "y_label = tf.argmax(y_hat, 1)\n",
    "accuracy = tf.count_nonzero(\n",
    "        tf.cast(tf.equal(tf.argmax(y_hot, 1), y_label),\n",
    "                tf.int64)) / tf.cast(tf.shape(y_hot)[0], tf.int64)\n",
    "\n",
    "extra_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "with tf.control_dependencies(extra_ops):\n",
    "    global_step = tf.Variable(1, trainable=False)\n",
    "    decay_steps = 5000\n",
    "    learning_rate = 0.0005\n",
    "    lr_decayed = tf.train.cosine_decay(learning_rate, global_step, decay_steps)\n",
    "    optimizer = tf.train.AdamOptimizer(lr_decayed)\n",
    "    train_op = optimizer.minimize(loss, global_step=global_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpu_options = tf.GPUOptions()\n",
    "gpu_options.allow_growth = True\n",
    "session = tf.Session(config=tf.ConfigProto(gpu_options=gpu_options))\n",
    "session.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 6.8505 7.8336 8.0994 0.4188 0.3850 0.3750\n",
      "0.00050\n",
      "20 6.6326 8.8698 8.9562 0.4875 0.3750 0.4026\n",
      "0.00050\n",
      "30 5.7214 7.9309 8.3346 0.5350 0.4100 0.4218\n",
      "0.00049\n",
      "40 5.8884 8.6165 9.7129 0.5563 0.4150 0.4226\n",
      "0.00049\n",
      "50 5.1566 7.7710 8.7944 0.5750 0.4700 0.4424\n",
      "0.00048\n",
      "60 4.9633 8.5178 9.4341 0.6125 0.4600 0.4378\n",
      "0.00047\n",
      "70 4.7657 9.1936 9.9337 0.6212 0.4400 0.4452\n",
      "0.00046\n",
      "80 4.4202 8.7581 10.0747 0.6538 0.5000 0.4450\n",
      "0.00045\n",
      "90 3.6565 8.9016 9.9643 0.6913 0.4700 0.4564\n",
      "0.00044\n",
      "100 3.4735 8.5810 10.0500 0.7037 0.4800 0.4552\n",
      "0.00042\n",
      "110 4.0524 9.8789 11.3054 0.6813 0.4850 0.4424\n",
      "0.00041\n",
      "120 3.2126 9.3165 10.8400 0.7338 0.5100 0.4648\n",
      "0.00039\n",
      "130 3.1242 10.7561 11.3343 0.7188 0.4800 0.4592\n",
      "0.00037\n",
      "140 2.9819 10.4615 11.7449 0.7325 0.4750 0.4562\n",
      "0.00035\n",
      "150 2.9952 10.5204 12.2098 0.7450 0.5250 0.4752\n",
      "0.00033\n",
      "160 3.1889 11.4020 12.8277 0.7312 0.4600 0.4522\n",
      "0.00032\n",
      "170 2.4108 10.8242 12.1635 0.7800 0.5100 0.4672\n",
      "0.00030\n",
      "180 2.6008 12.0307 12.5887 0.7913 0.4900 0.4668\n",
      "0.00027\n",
      "190 2.4890 11.6352 12.7745 0.7800 0.5050 0.4624\n",
      "0.00025\n",
      "200 2.4701 11.8650 13.2103 0.7850 0.5150 0.4630\n",
      "0.00023\n",
      "210 2.4638 12.0502 13.3792 0.7987 0.5150 0.4656\n",
      "0.00021\n",
      "220 2.2421 12.3524 13.6514 0.7987 0.5000 0.4554\n",
      "0.00019\n",
      "230 2.3578 12.8487 14.1112 0.8050 0.5000 0.4576\n",
      "0.00017\n",
      "240 2.1217 12.0055 13.7691 0.8187 0.5250 0.4710\n",
      "0.00015\n",
      "250 1.8083 12.5098 13.7889 0.8375 0.5100 0.4686\n",
      "0.00014\n",
      "0.4752\n"
     ]
    }
   ],
   "source": [
    "max_valid_epoch_idx = 0\n",
    "max_valid_accuracy = 0.0\n",
    "final_test_accuracy = 0.0\n",
    "for epoch_idx in range(1, 10000 + 1):\n",
    "    batches = 0\n",
    "    for x_batch, y_batch in datagen.flow(x_train, y_train, batch_size=batch_size):\n",
    "        x_batch = x_batch.reshape([-1, n_input])\n",
    "        session.run(\n",
    "                train_op,\n",
    "                feed_dict={\n",
    "                    x: x_batch,\n",
    "                    y: y_batch,\n",
    "                    training: True\n",
    "                })\n",
    "        batches += 1\n",
    "        if batches >= len(x_train) / batch_size:\n",
    "            break\n",
    "\n",
    "    if epoch_idx % 10 == 0:\n",
    "        train_loss_value, train_accuracy_value, lr = session.run(\n",
    "            [loss, accuracy, lr_decayed],\n",
    "            feed_dict={\n",
    "                x: x_train.reshape([-1, n_input]),\n",
    "                y: y_train,\n",
    "                training: False\n",
    "            })\n",
    "        \n",
    "        valid_loss_value, valid_accuracy_value = session.run(\n",
    "            [loss, accuracy],\n",
    "            feed_dict={\n",
    "                x: x_valid,\n",
    "                y: y_valid,\n",
    "                training: False\n",
    "            })\n",
    "            \n",
    "        test_loss_value, test_accuracy_value = session.run(\n",
    "            [loss, accuracy],\n",
    "            feed_dict={\n",
    "                x: x_test,\n",
    "                y: y_test,\n",
    "                training: False\n",
    "            })\n",
    "\n",
    "        print(epoch_idx, '%.4f' % train_loss_value, '%.4f' % valid_loss_value, '%.4f' % test_loss_value, '%.4f' % train_accuracy_value, '%.4f' % valid_accuracy_value, '%.4f' % test_accuracy_value)\n",
    "        print('%.5f' % lr)\n",
    "        if max_valid_accuracy < valid_accuracy_value:\n",
    "            max_valid_accuracy = valid_accuracy_value \n",
    "            max_valid_epoch_idx = epoch_idx\n",
    "            final_test_accuracy = test_accuracy_value\n",
    "            \n",
    "    # Early Stop\n",
    "    if max_valid_epoch_idx + 100 < epoch_idx: # Increase Early Stop bound\n",
    "        break\n",
    "        \n",
    "print(final_test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "32.88% 의 성능을 확인할 수 있습니다. 실습시간 배운 몇 가지 정규화와 최적화 과정을 동원하면 50% 정도의 성능까지는 쉽게 달성할 수 있음을 확인했습니다. 수업시간에 배운 내용들을 사용해 최대한 높은 성능을 나타내는 모델을 만들어보세요! \n",
    "주피터 노트북 파일을 제출해주시면 되며, 성능을 기준으로 점수를 매길 예정입니다. (상대평가)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
