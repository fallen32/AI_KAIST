{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment on Regularization and Optimization of Deep Learning\n",
    "\n",
    "이번 과제에서는 reagularization과 optimization에서 배운 내용들을 이용해 최대한 Deep Models의 성능을 높여보고자 합니다. Layer 4개짜리 MLP (각 hidden layer는 512개의 unit을 가짐) 상황에서 정규화와 최적화 방법론들을 총 동원해 성능을 높여주시면 됩니다.\n",
    "\n",
    "먼저, 아래 코드는 데이터 셋을 셋팅하는 부분입니다. 이 부분은 건드리시면 안됩니다. 이 부분을 건드리시면 0점 처리 됩니다. 외부 데이터 사용하셔도 안됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 32, 32, 3)\n",
      "(1000,)\n",
      "(5000, 32, 32, 3)\n",
      "(5000,)\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import math\n",
    "import random \n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "seed = 1\n",
    "random.seed(seed)\n",
    "np.random.seed(seed=seed)\n",
    "tf.random.set_random_seed(seed)\n",
    "\n",
    "(x_1, y_1), (x_2, y_2) = tf.keras.datasets.cifar100.load_data()\n",
    "x_total = np.concatenate([x_1, x_2], axis=0).astype(np.float64)\n",
    "y_total = np.concatenate([y_1, y_2], axis=0)\n",
    "\n",
    "n_output = 10\n",
    "\n",
    "valid_index, _ = np.where(y_total < n_output)\n",
    "y_total = y_total[valid_index].reshape([-1])\n",
    "x_total = x_total[valid_index]\n",
    "\n",
    "i = np.arange(x_total.shape[0])\n",
    "np.random.shuffle(i)\n",
    "x_total = x_total[i]\n",
    "y_total = y_total[i]\n",
    "\n",
    "train_size = 100 * n_output\n",
    "x_train = x_total[:train_size]\n",
    "y_train = y_total[:train_size]\n",
    "x_test = x_total[train_size:]\n",
    "y_test = y_total[train_size:]\n",
    "\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "print(x_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "validation set을 나눕니다. \n",
    "- 실습시간에 배웠던 것처럼 Validation set 비율은 조정하셔도 됩니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "split = x_train.shape[0] // 5\n",
    "x_valid = x_train[:split]\n",
    "y_valid = y_train[:split]\n",
    "\n",
    "x_train = x_train[split:]\n",
    "y_train = y_train[split:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이미지를 greyscale로 변경합니다. \n",
    "1. RGB 값을 고려한 코드로 변경하셔도 됩니다. \n",
    "2. Augmentation을 고려해보세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(800, 32, 32, 3)\n",
      "(200, 32, 32, 3)\n",
      "(5000, 32, 32, 3)\n"
     ]
    }
   ],
   "source": [
    "# x_train = np.mean(x_train, axis=3, keepdims=True)\n",
    "# x_valid = np.mean(x_valid, axis=3, keepdims=True)\n",
    "# x_test = np.mean(x_test, axis=3, keepdims=True)\n",
    "# Use RGB and normalize pixel values by training data-channel\n",
    "mean = np.zeros(3)\n",
    "std = np.ones(3)\n",
    "for i in range(3):\n",
    "    mean[i] = np.mean(x_train[:, :, :, i])\n",
    "    std[i] = np.std(x_train[:, :, :, i])    \n",
    "x_train = (x_train - mean) / std\n",
    "x_valid = (x_valid - mean) / std\n",
    "x_test = (x_test - mean) / std\n",
    "\n",
    "epoch = 10000\n",
    "batch_size = 128\n",
    "\n",
    "print(x_train.shape)\n",
    "print(x_valid.shape)\n",
    "print(x_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make Data Generator\n",
    "for augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "datagen = tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "    width_shift_range=5,\n",
    "    height_shift_range=5,\n",
    "    fill_mode='constant',\n",
    "    horizontal_flip=True,\n",
    "    vertical_flip=False,\n",
    "    rotation_range=10,\n",
    "    zoom_range=[0.8, 1.2]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(200, 3072)\n",
      "(5000, 3072)\n"
     ]
    }
   ],
   "source": [
    "n_input = 32 * 32 * 3   # H * W * C\n",
    "\n",
    "# x_train = x_train.reshape([-1, n_input]) This will be done after augmentation.\n",
    "x_valid = x_valid.reshape([-1, n_input])\n",
    "x_test = x_test.reshape([-1, n_input])\n",
    "\n",
    "print(x_valid.shape)\n",
    "print(x_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 모델을 만듭니다.\n",
    "\n",
    "1. Optimizer를 다른 걸로 바꿔보세요\n",
    "2. Learning Rate를 바꿔보세요. Learning Rate Scheduling도 고려해보세요.\n",
    "3. Activation Function을 바꿔보세요. \n",
    "4. Dropout, DropConnect, Gaussian Dropout 을 고려해보세요.\n",
    "5. Augmentation을 고려해보세요. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.float32, [None, n_input])\n",
    "y = tf.placeholder(tf.int32, [None])\n",
    "training = tf.placeholder(tf.bool)\n",
    "\n",
    "n_units = [n_input, 512, 512, 512, n_output]\n",
    "\n",
    "weights, biases = [], []\n",
    "for i, (n_in, n_out) in enumerate(zip(n_units[:-1], n_units[1:])):\n",
    "    stddev = math.sqrt(2 / n_in) # Kaiming He Initialization\n",
    "    weight = tf.Variable(tf.random.truncated_normal([n_in, n_out], mean=0, stddev=stddev))\n",
    "    bias = tf.Variable(tf.zeros([n_out]))\n",
    "    weights.append(weight)\n",
    "    biases.append(bias)\n",
    "    \n",
    "layer = x\n",
    "for i, (weight, bias) in enumerate(zip(weights, biases)):\n",
    "    if i < len(weights) - 1:\n",
    "        rate = tf.cond(training, lambda: 0.5, lambda: 0.0)\n",
    "        #maxnorm = tf.keras.constraints.MaxNorm(2)\n",
    "        #weight = maxnorm(weight)\n",
    "        # weight = tf.nn.dropout(weight, rate=rate) * (1 - rate) # DropConnect\n",
    "        layer = tf.matmul(layer, weight) + bias\n",
    "        layer = tf.nn.relu(layer)\n",
    "        layer = tf.keras.layers.GaussianDropout(0.5)(layer, training)\n",
    "        #layer = tf.nn.dropout(layer, rate=rate)\n",
    "    else:\n",
    "        layer = tf.matmul(layer, weight) + bias\n",
    "y_hat = layer\n",
    "\n",
    "y_hot = tf.one_hot(y, n_output)\n",
    "costs = tf.nn.softmax_cross_entropy_with_logits_v2(\n",
    "        labels=y_hot, logits=y_hat)\n",
    "cross_entropy_loss = tf.reduce_mean(costs)\n",
    "# L2-regularization\n",
    "l2_loss = 0.0\n",
    "for weight in weights:\n",
    "    l2_loss = l2_loss + tf.nn.l2_loss(weight)\n",
    "loss = cross_entropy_loss + 0.01 * l2_loss\n",
    "\n",
    "y_label = tf.argmax(y_hat, 1)\n",
    "accuracy = tf.count_nonzero(\n",
    "        tf.cast(tf.equal(tf.argmax(y_hot, 1), y_label),\n",
    "                tf.int64)) / tf.cast(tf.shape(y_hot)[0], tf.int64)\n",
    "\n",
    "extra_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "with tf.control_dependencies(extra_ops):\n",
    "    global_step = tf.Variable(1, trainable=False)\n",
    "    decay_steps = 10000\n",
    "    learning_rate = 0.0002\n",
    "    lr_decayed = tf.train.cosine_decay(learning_rate, global_step, decay_steps)\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=lr_decayed, beta1=0.85)\n",
    "    train_op = optimizer.minimize(loss, global_step=global_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpu_options = tf.GPUOptions()\n",
    "gpu_options.allow_growth = True\n",
    "session = tf.Session(config=tf.ConfigProto(gpu_options=gpu_options))\n",
    "session.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 13.1907 13.3236 13.2758 0.3825 0.3400 0.3482\n",
      "0.00020\n",
      "20 12.5561 12.6885 12.6462 0.3937 0.3550 0.3594\n",
      "0.00020\n",
      "30 11.9235 12.0793 12.0299 0.4475 0.3950 0.3840\n",
      "0.00020\n",
      "40 11.3233 11.5097 11.4537 0.4363 0.3550 0.3730\n",
      "0.00020\n",
      "50 10.7045 10.8992 10.8552 0.4637 0.3550 0.3998\n",
      "0.00020\n",
      "60 10.1212 10.3410 10.2863 0.4537 0.3900 0.3930\n",
      "0.00020\n",
      "70 9.5568 9.7926 9.7394 0.4825 0.4000 0.3996\n",
      "0.00020\n",
      "80 9.0162 9.2535 9.2092 0.4875 0.3750 0.4088\n",
      "0.00020\n",
      "90 8.5056 8.7742 8.7112 0.4875 0.3750 0.4124\n",
      "0.00020\n",
      "100 7.9949 8.2679 8.2264 0.5025 0.3950 0.4126\n",
      "0.00020\n",
      "110 7.5268 7.8108 7.7721 0.5075 0.3900 0.4136\n",
      "0.00020\n",
      "120 7.0615 7.3618 7.3233 0.5212 0.4050 0.4210\n",
      "0.00020\n",
      "130 6.6321 6.9397 6.9016 0.5275 0.4200 0.4292\n",
      "0.00020\n",
      "140 6.2294 6.5504 6.5234 0.5450 0.4100 0.4282\n",
      "0.00020\n",
      "150 5.8491 6.1731 6.1533 0.5525 0.4600 0.4364\n",
      "0.00019\n",
      "160 5.5077 5.8170 5.8206 0.5663 0.4400 0.4358\n",
      "0.00019\n",
      "170 5.1619 5.4923 5.4975 0.5800 0.4600 0.4468\n",
      "0.00019\n",
      "180 4.8596 5.2249 5.2211 0.5750 0.4400 0.4444\n",
      "0.00019\n",
      "190 4.5734 4.9502 4.9389 0.5850 0.4500 0.4514\n",
      "0.00019\n",
      "200 4.3119 4.7139 4.7044 0.5837 0.4600 0.4476\n",
      "0.00019\n",
      "210 4.0664 4.4593 4.4795 0.5925 0.4850 0.4490\n",
      "0.00019\n",
      "220 3.8218 4.2044 4.2266 0.6075 0.4900 0.4606\n",
      "0.00019\n",
      "230 3.6157 4.0669 4.0577 0.6012 0.4850 0.4548\n",
      "0.00019\n",
      "240 3.4260 3.8113 3.8512 0.6062 0.5150 0.4688\n",
      "0.00019\n",
      "250 3.2225 3.6431 3.6832 0.6312 0.4750 0.4626\n",
      "0.00019\n",
      "260 3.0561 3.4825 3.5329 0.6275 0.4900 0.4668\n",
      "0.00018\n",
      "270 2.9236 3.3431 3.3978 0.6225 0.4750 0.4652\n",
      "0.00018\n",
      "280 2.7838 3.2439 3.2968 0.6375 0.4600 0.4602\n",
      "0.00018\n",
      "290 2.6265 3.0753 3.1345 0.6700 0.5000 0.4760\n",
      "0.00018\n",
      "300 2.5313 2.9985 3.0714 0.6325 0.5000 0.4564\n",
      "0.00018\n",
      "310 2.3995 2.8600 2.9326 0.6700 0.5050 0.4792\n",
      "0.00018\n",
      "320 2.3227 2.8010 2.8738 0.6562 0.4850 0.4646\n",
      "0.00018\n",
      "330 2.2153 2.6924 2.7764 0.6787 0.4950 0.4750\n",
      "0.00017\n",
      "340 2.1330 2.6571 2.7003 0.6825 0.5050 0.4780\n",
      "0.00017\n",
      "350 2.0750 2.5996 2.6445 0.6787 0.5050 0.4690\n",
      "0.00017\n",
      "360 2.0098 2.5419 2.6166 0.6600 0.5100 0.4670\n",
      "0.00017\n",
      "370 1.9215 2.4435 2.5057 0.6937 0.5300 0.4836\n",
      "0.00017\n",
      "380 1.8909 2.3984 2.4813 0.6687 0.5150 0.4766\n",
      "0.00017\n",
      "390 1.7907 2.3336 2.4113 0.6825 0.5100 0.4812\n",
      "0.00017\n",
      "400 1.7648 2.3351 2.3955 0.6900 0.5250 0.4780\n",
      "0.00016\n",
      "410 1.7120 2.2779 2.3297 0.6950 0.5200 0.4842\n",
      "0.00016\n",
      "420 1.6776 2.2434 2.3141 0.6900 0.5200 0.4832\n",
      "0.00016\n",
      "430 1.6373 2.1914 2.2800 0.6987 0.5050 0.4802\n",
      "0.00016\n",
      "440 1.5907 2.1585 2.2516 0.7013 0.5300 0.4844\n",
      "0.00016\n",
      "450 1.5761 2.1484 2.2228 0.7063 0.5250 0.4850\n",
      "0.00015\n",
      "460 1.5174 2.0783 2.1662 0.7338 0.5300 0.4970\n",
      "0.00015\n",
      "470 1.5169 2.0894 2.1830 0.7050 0.5100 0.4864\n",
      "0.00015\n",
      "480 1.4918 2.0706 2.1538 0.7063 0.5100 0.4910\n",
      "0.00015\n",
      "490 1.4435 2.0232 2.1355 0.7137 0.5400 0.4926\n",
      "0.00015\n",
      "500 1.4317 1.9854 2.1086 0.7150 0.5500 0.4964\n",
      "0.00015\n",
      "510 1.4137 2.0227 2.1349 0.7225 0.5100 0.4798\n",
      "0.00014\n",
      "520 1.3891 1.9737 2.0895 0.7288 0.5400 0.4872\n",
      "0.00014\n",
      "530 1.3540 1.9535 2.0809 0.7325 0.5600 0.4934\n",
      "0.00014\n",
      "540 1.3688 1.9702 2.0881 0.7238 0.5200 0.4892\n",
      "0.00014\n",
      "550 1.3208 1.9461 2.0548 0.7375 0.5550 0.5014\n",
      "0.00014\n",
      "560 1.3091 1.8865 2.0210 0.7462 0.5450 0.5070\n",
      "0.00013\n",
      "570 1.3060 1.9625 2.0509 0.7462 0.5250 0.4944\n",
      "0.00013\n",
      "580 1.2828 1.8927 2.0236 0.7512 0.5450 0.4972\n",
      "0.00013\n",
      "590 1.2655 1.8852 1.9944 0.7450 0.5550 0.5084\n",
      "0.00013\n",
      "600 1.2854 1.9309 2.0352 0.7388 0.5200 0.4996\n",
      "0.00012\n",
      "610 1.2371 1.8914 1.9984 0.7475 0.5450 0.5020\n",
      "0.00012\n",
      "620 1.2162 1.8618 1.9750 0.7738 0.5300 0.5036\n",
      "0.00012\n",
      "630 1.2113 1.9036 1.9927 0.7688 0.5350 0.5030\n",
      "0.00012\n",
      "640 1.1951 1.8722 1.9813 0.7712 0.5550 0.5058\n",
      "0.00012\n",
      "650 1.2092 1.8919 2.0050 0.7575 0.5300 0.5026\n",
      "0.00011\n",
      "660 1.1862 1.8267 1.9711 0.7700 0.5800 0.5048\n",
      "0.00011\n",
      "670 1.1716 1.8477 1.9649 0.7700 0.5650 0.5042\n",
      "0.00011\n",
      "680 1.1854 1.8256 1.9798 0.7712 0.5500 0.5034\n",
      "0.00011\n",
      "690 1.1537 1.8534 1.9764 0.7712 0.5450 0.5016\n",
      "0.00011\n",
      "700 1.1498 1.8729 1.9868 0.7638 0.5350 0.5026\n",
      "0.00010\n",
      "710 1.1492 1.8488 1.9726 0.7738 0.5600 0.5096\n",
      "0.00010\n",
      "720 1.1634 1.8854 1.9932 0.7612 0.5150 0.4968\n",
      "0.00010\n",
      "730 1.1355 1.8475 1.9852 0.7738 0.5450 0.5016\n",
      "0.00010\n",
      "750 1.0989 1.7646 1.9219 0.8063 0.5750 0.5132\n",
      "0.00009\n",
      "760 1.0969 1.7938 1.9496 0.7950 0.5450 0.5096\n",
      "0.00009\n",
      "770 1.0937 1.8339 1.9550 0.7937 0.5450 0.5064\n",
      "0.00009\n",
      "780 1.0769 1.7872 1.9409 0.7975 0.5650 0.5136\n",
      "0.00009\n",
      "790 1.0799 1.8350 1.9599 0.7975 0.5150 0.5094\n",
      "0.00008\n",
      "800 1.0658 1.7947 1.9336 0.8125 0.5450 0.5128\n",
      "0.00008\n",
      "810 1.0554 1.7851 1.9233 0.8050 0.5500 0.5174\n",
      "0.00008\n",
      "820 1.0613 1.7818 1.9301 0.8000 0.5350 0.5126\n",
      "0.00008\n",
      "830 1.0397 1.7821 1.9126 0.8213 0.5650 0.5232\n",
      "0.00007\n",
      "840 1.0418 1.7740 1.9067 0.8100 0.5600 0.5216\n",
      "0.00007\n",
      "850 1.0340 1.7784 1.9173 0.8200 0.5700 0.5180\n",
      "0.00007\n",
      "860 1.0226 1.7775 1.9141 0.8263 0.5800 0.5200\n",
      "0.00007\n",
      "870 1.0389 1.7838 1.9359 0.8113 0.5400 0.5128\n",
      "0.00007\n",
      "880 1.0087 1.7405 1.9052 0.8225 0.5450 0.5254\n",
      "0.00006\n",
      "890 1.0188 1.7529 1.9211 0.8250 0.5450 0.5198\n",
      "0.00006\n",
      "900 1.0244 1.7950 1.9440 0.8137 0.5350 0.5136\n",
      "0.00006\n",
      "910 1.0029 1.7475 1.9189 0.8175 0.5600 0.5232\n",
      "0.00006\n",
      "920 1.0090 1.7716 1.9277 0.8150 0.5700 0.5172\n",
      "0.00006\n",
      "930 0.9936 1.7588 1.9211 0.8337 0.5500 0.5144\n",
      "0.00005\n",
      "940 0.9932 1.7868 1.9252 0.8263 0.5550 0.5182\n",
      "0.00005\n",
      "950 0.9954 1.7718 1.9282 0.8237 0.5300 0.5146\n",
      "0.00005\n",
      "960 0.9828 1.7617 1.9156 0.8287 0.5400 0.5200\n",
      "0.00005\n",
      "970 0.9821 1.7463 1.9129 0.8387 0.5450 0.5196\n",
      "0.00005\n",
      "980 0.9750 1.7633 1.9247 0.8313 0.5350 0.5206\n",
      "0.00004\n",
      "990 0.9736 1.7634 1.9061 0.8375 0.5400 0.5212\n",
      "0.00004\n",
      "1000 0.9656 1.7522 1.8988 0.8425 0.5500 0.5270\n",
      "0.00004\n",
      "1010 0.9684 1.7852 1.9269 0.8337 0.5550 0.5212\n",
      "0.00004\n",
      "1020 0.9586 1.7530 1.9076 0.8462 0.5500 0.5222\n",
      "0.00004\n",
      "1030 0.9616 1.7717 1.9219 0.8400 0.5550 0.5172\n",
      "0.00004\n",
      "1040 0.9556 1.7448 1.9193 0.8438 0.5650 0.5184\n",
      "0.00003\n",
      "1050 0.9565 1.7689 1.9177 0.8313 0.5400 0.5198\n",
      "0.00003\n",
      "1060 0.9463 1.7552 1.9050 0.8375 0.5400 0.5234\n",
      "0.00003\n",
      "1070 0.9452 1.7682 1.9125 0.8313 0.5500 0.5202\n",
      "0.00003\n",
      "1080 0.9400 1.7620 1.9082 0.8438 0.5550 0.5254\n",
      "0.00003\n",
      "1090 0.9392 1.7531 1.9123 0.8488 0.5550 0.5240\n",
      "0.00003\n",
      "1100 0.9305 1.7444 1.9020 0.8612 0.5500 0.5248\n",
      "0.00002\n",
      "1110 0.9267 1.7474 1.9090 0.8525 0.5500 0.5248\n",
      "0.00002\n",
      "1120 0.9271 1.7676 1.9125 0.8475 0.5500 0.5238\n",
      "0.00002\n",
      "1130 0.9279 1.7525 1.9056 0.8500 0.5500 0.5238\n",
      "0.00002\n",
      "1140 0.9266 1.7585 1.9072 0.8525 0.5350 0.5262\n",
      "0.00002\n",
      "1150 0.9265 1.7500 1.9059 0.8450 0.5350 0.5202\n",
      "0.00002\n",
      "1160 0.9173 1.7406 1.8932 0.8538 0.5550 0.5272\n",
      "0.00002\n",
      "0.5048\n"
     ]
    }
   ],
   "source": [
    "max_valid_epoch_idx = 0\n",
    "max_valid_accuracy = 0.0\n",
    "final_test_accuracy = 0.0\n",
    "for epoch_idx in range(1, 10000 + 1):\n",
    "    batches = 0\n",
    "    for x_batch, y_batch in datagen.flow(x_train, y_train, batch_size=batch_size):\n",
    "        x_batch = x_batch.reshape([-1, n_input])\n",
    "        session.run(\n",
    "                train_op,\n",
    "                feed_dict={\n",
    "                    x: x_batch,\n",
    "                    y: y_batch,\n",
    "                    training: True\n",
    "                })\n",
    "        batches += 1\n",
    "        if batches >= len(x_train) / batch_size:\n",
    "            break\n",
    "\n",
    "    if epoch_idx % 10 == 0:\n",
    "        train_loss_value, train_accuracy_value, lr = session.run(\n",
    "            [loss, accuracy, lr_decayed],\n",
    "            feed_dict={\n",
    "                x: x_train.reshape([-1, n_input]),\n",
    "                y: y_train,\n",
    "                training: False\n",
    "            })\n",
    "        \n",
    "        valid_loss_value, valid_accuracy_value = session.run(\n",
    "            [loss, accuracy],\n",
    "            feed_dict={\n",
    "                x: x_valid,\n",
    "                y: y_valid,\n",
    "                training: False\n",
    "            })\n",
    "            \n",
    "        test_loss_value, test_accuracy_value = session.run(\n",
    "            [loss, accuracy],\n",
    "            feed_dict={\n",
    "                x: x_test,\n",
    "                y: y_test,\n",
    "                training: False\n",
    "            })\n",
    "\n",
    "        print(epoch_idx, '%.4f' % train_loss_value, '%.4f' % valid_loss_value, '%.4f' % test_loss_value, '%.4f' % train_accuracy_value, '%.4f' % valid_accuracy_value, '%.4f' % test_accuracy_value)\n",
    "        print('%.5f' % lr)\n",
    "        if max_valid_accuracy < valid_accuracy_value:\n",
    "            max_valid_accuracy = valid_accuracy_value \n",
    "            max_valid_epoch_idx = epoch_idx\n",
    "            final_test_accuracy = test_accuracy_value\n",
    "            \n",
    "    # Early Stop\n",
    "    if max_valid_epoch_idx + 500 < epoch_idx: # Increase Early Stop bound\n",
    "        break\n",
    "        \n",
    "print(final_test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    32.88% 의 성능을 확인할 수 있습니다. 실습시간 배운 몇 가지 정규화와 최적화 과정을 동원하면 50% 정도의 성능까지는 쉽게 달성할 수 있음을 확인했습니다. 수업시간에 배운 내용들을 사용해 최대한 높은 성능을 나타내는 모델을 만들어보세요! \n",
    "주피터 노트북 파일을 제출해주시면 되며, 성능을 기준으로 점수를 매길 예정입니다. (상대평가)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
