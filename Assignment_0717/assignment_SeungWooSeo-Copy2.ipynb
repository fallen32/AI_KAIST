{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment on Regularization and Optimization of Deep Learning\n",
    "\n",
    "이번 과제에서는 reagularization과 optimization에서 배운 내용들을 이용해 최대한 Deep Models의 성능을 높여보고자 합니다. Layer 4개짜리 MLP (각 hidden layer는 512개의 unit을 가짐) 상황에서 정규화와 최적화 방법론들을 총 동원해 성능을 높여주시면 됩니다.\n",
    "\n",
    "먼저, 아래 코드는 데이터 셋을 셋팅하는 부분입니다. 이 부분은 건드리시면 안됩니다. 이 부분을 건드리시면 0점 처리 됩니다. 외부 데이터 사용하셔도 안됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 32, 32, 3)\n",
      "(1000,)\n",
      "(5000, 32, 32, 3)\n",
      "(5000,)\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import math\n",
    "import random \n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "seed = 1\n",
    "random.seed(seed)\n",
    "np.random.seed(seed=seed)\n",
    "tf.random.set_random_seed(seed)\n",
    "\n",
    "(x_1, y_1), (x_2, y_2) = tf.keras.datasets.cifar100.load_data()\n",
    "x_total = np.concatenate([x_1, x_2], axis=0).astype(np.float64)\n",
    "y_total = np.concatenate([y_1, y_2], axis=0)\n",
    "\n",
    "n_output = 10\n",
    "\n",
    "valid_index, _ = np.where(y_total < n_output)\n",
    "y_total = y_total[valid_index].reshape([-1])\n",
    "x_total = x_total[valid_index]\n",
    "\n",
    "i = np.arange(x_total.shape[0])\n",
    "np.random.shuffle(i)\n",
    "x_total = x_total[i]\n",
    "y_total = y_total[i]\n",
    "\n",
    "train_size = 100 * n_output\n",
    "x_train = x_total[:train_size]\n",
    "y_train = y_total[:train_size]\n",
    "x_test = x_total[train_size:]\n",
    "y_test = y_total[train_size:]\n",
    "\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "print(x_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "validation set을 나눕니다. \n",
    "- 실습시간에 배웠던 것처럼 Validation set 비율은 조정하셔도 됩니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "split = x_train.shape[0] // 5\n",
    "x_valid = x_train[:split]\n",
    "y_valid = y_train[:split]\n",
    "\n",
    "x_train = x_train[split:]\n",
    "y_train = y_train[split:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이미지를 greyscale로 변경합니다. \n",
    "1. RGB 값을 고려한 코드로 변경하셔도 됩니다. \n",
    "2. Augmentation을 고려해보세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(800, 32, 32, 3)\n",
      "(200, 32, 32, 3)\n",
      "(5000, 32, 32, 3)\n"
     ]
    }
   ],
   "source": [
    "# x_train = np.mean(x_train, axis=3, keepdims=True)\n",
    "# x_valid = np.mean(x_valid, axis=3, keepdims=True)\n",
    "# x_test = np.mean(x_test, axis=3, keepdims=True)\n",
    "# Use RGB and normalize pixel values by training data-channel\n",
    "mean = np.zeros(3)\n",
    "std = np.ones(3)\n",
    "for i in range(3):\n",
    "    mean[i] = np.mean(x_train[:, :, :, i])\n",
    "    std[i] = np.std(x_train[:, :, :, i])    \n",
    "x_train = (x_train - mean) / std\n",
    "x_valid = (x_valid - mean) / std\n",
    "x_test = (x_test - mean) / std\n",
    "\n",
    "epoch = 10000\n",
    "batch_size = 64\n",
    "\n",
    "print(x_train.shape)\n",
    "print(x_valid.shape)\n",
    "print(x_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make Data Generator\n",
    "for augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "datagen = tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "    width_shift_range=5,\n",
    "    height_shift_range=5,\n",
    "    fill_mode='constant',\n",
    "    horizontal_flip=True,\n",
    "    vertical_flip=False,\n",
    "    rotation_range=15,\n",
    "    zoom_range=[0.8, 1.2]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(200, 3072)\n",
      "(5000, 3072)\n"
     ]
    }
   ],
   "source": [
    "n_input = 32 * 32 * 3   # H * W * C\n",
    "\n",
    "# x_train = x_train.reshape([-1, n_input]) This will be done after augmentation.\n",
    "x_valid = x_valid.reshape([-1, n_input])\n",
    "x_test = x_test.reshape([-1, n_input])\n",
    "\n",
    "print(x_valid.shape)\n",
    "print(x_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 모델을 만듭니다.\n",
    "\n",
    "1. Optimizer를 다른 걸로 바꿔보세요\n",
    "2. Learning Rate를 바꿔보세요. Learning Rate Scheduling도 고려해보세요.\n",
    "3. Activation Function을 바꿔보세요. \n",
    "4. Dropout, DropConnect, Gaussian Dropout 을 고려해보세요.\n",
    "5. Augmentation을 고려해보세요. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.float32, [None, n_input])\n",
    "y = tf.placeholder(tf.int32, [None])\n",
    "training = tf.placeholder(tf.bool)\n",
    "\n",
    "n_units = [n_input, 512, 512, 512, n_output]\n",
    "\n",
    "weights, biases = [], []\n",
    "for i, (n_in, n_out) in enumerate(zip(n_units[:-1], n_units[1:])):\n",
    "    stddev = math.sqrt(2 / n_in) # Kaiming He Initialization\n",
    "    weight = tf.Variable(tf.random.truncated_normal([n_in, n_out], mean=0, stddev=stddev))\n",
    "    bias = tf.Variable(tf.zeros([n_out]))\n",
    "    weights.append(weight)\n",
    "    biases.append(bias)\n",
    "    \n",
    "layer = x\n",
    "for i, (weight, bias) in enumerate(zip(weights, biases)):\n",
    "    if i < len(weights) - 1:\n",
    "        rate = tf.cond(training, lambda: 0.5, lambda: 0.0)\n",
    "        weight = tf.nn.dropout(weight, rate=rate) * (1 - rate) # DropConnect\n",
    "        layer = tf.matmul(layer, weight) + bias\n",
    "        layer = tf.nn.relu(layer)\n",
    "    else:\n",
    "        layer = tf.matmul(layer, weight) + bias\n",
    "y_hat = layer\n",
    "\n",
    "y_hot = tf.one_hot(y, n_output)\n",
    "costs = tf.nn.softmax_cross_entropy_with_logits_v2(\n",
    "        labels=y_hot, logits=y_hat)\n",
    "cross_entropy_loss = tf.reduce_mean(costs)\n",
    "# L2-regularization\n",
    "l2_loss = 0.0\n",
    "for weight in weights:\n",
    "    l2_loss = l2_loss + tf.nn.l2_loss(weight)\n",
    "loss = tf.reduce_mean(cross_entropy_loss + 0.001 * l2_loss)\n",
    "\n",
    "y_label = tf.argmax(y_hat, 1)\n",
    "accuracy = tf.count_nonzero(\n",
    "        tf.cast(tf.equal(tf.argmax(y_hot, 1), y_label),\n",
    "                tf.int64)) / tf.cast(tf.shape(y_hot)[0], tf.int64)\n",
    "\n",
    "extra_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "with tf.control_dependencies(extra_ops):\n",
    "    global_step = tf.Variable(1, trainable=False)\n",
    "    decay_steps = 50000\n",
    "    learning_rate = 0.0001\n",
    "    lr_decayed = tf.train.cosine_decay(learning_rate, global_step, decay_steps)\n",
    "    optimizer = tf.train.AdamOptimizer(lr_decayed)\n",
    "    train_op = optimizer.minimize(loss, global_step=global_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpu_options = tf.GPUOptions()\n",
    "gpu_options.allow_growth = True\n",
    "session = tf.Session(config=tf.ConfigProto(gpu_options=gpu_options))\n",
    "session.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 5.1349 5.4907 5.3698 0.3225 0.2900 0.2942\n",
      "0.00010\n",
      "20 6.5508 7.4678 7.1394 0.3812 0.3100 0.3400\n",
      "0.00010\n",
      "30 6.7322 7.9138 7.7379 0.4163 0.3350 0.3718\n",
      "0.00010\n",
      "40 6.9494 8.0710 8.2941 0.4450 0.3700 0.3932\n",
      "0.00010\n",
      "50 7.0061 8.2878 8.6709 0.4662 0.3850 0.4036\n",
      "0.00010\n",
      "60 6.8437 8.5515 8.8495 0.4788 0.3800 0.4084\n",
      "0.00010\n",
      "70 6.5268 8.5816 8.9344 0.4950 0.3850 0.4076\n",
      "0.00010\n",
      "80 6.4803 8.4495 9.0404 0.5038 0.4350 0.4218\n",
      "0.00010\n",
      "90 6.7956 9.0715 9.7076 0.5138 0.4100 0.4146\n",
      "0.00010\n",
      "100 6.0640 8.2558 9.1370 0.5262 0.4500 0.4252\n",
      "0.00010\n",
      "110 6.1583 8.7615 9.4902 0.5300 0.4450 0.4226\n",
      "0.00010\n",
      "120 5.9347 8.6623 9.5268 0.5575 0.4600 0.4294\n",
      "0.00010\n",
      "130 5.7374 8.9362 9.4924 0.5637 0.4550 0.4316\n",
      "0.00010\n",
      "140 5.5874 8.6649 9.6636 0.5800 0.4500 0.4362\n",
      "0.00010\n",
      "150 5.3928 8.6939 9.6157 0.5975 0.4700 0.4424\n",
      "0.00010\n",
      "160 5.4659 9.0024 9.9041 0.5850 0.4450 0.4348\n",
      "0.00010\n",
      "170 5.0970 8.7022 9.7021 0.6200 0.4600 0.4482\n",
      "0.00010\n",
      "180 5.2280 9.2686 10.1750 0.6050 0.4450 0.4466\n",
      "0.00010\n",
      "190 4.9346 8.9417 9.8379 0.6212 0.4700 0.4480\n",
      "0.00010\n",
      "200 5.0674 9.2711 10.2114 0.6300 0.4500 0.4458\n",
      "0.00010\n",
      "210 4.7403 8.8249 10.0334 0.6412 0.4800 0.4512\n",
      "0.00010\n",
      "220 4.7093 8.8135 10.1076 0.6438 0.4750 0.4462\n",
      "0.00010\n",
      "230 4.9122 9.7352 10.6029 0.6425 0.4850 0.4508\n",
      "0.00010\n",
      "240 4.3970 9.1101 10.0095 0.6637 0.4800 0.4618\n",
      "0.00010\n",
      "250 4.3514 8.9990 10.3223 0.6787 0.4900 0.4536\n",
      "0.00010\n",
      "260 4.1589 8.9126 10.1240 0.6825 0.4850 0.4630\n",
      "0.00010\n",
      "270 4.2169 9.1629 10.2807 0.6613 0.4850 0.4570\n",
      "0.00010\n",
      "280 4.1079 9.3251 10.3822 0.6775 0.4950 0.4596\n",
      "0.00010\n",
      "290 3.7140 9.0680 10.1971 0.7025 0.5150 0.4640\n",
      "0.00010\n",
      "300 3.8047 9.4719 10.5239 0.6975 0.5100 0.4606\n",
      "0.00010\n",
      "310 3.8674 9.3517 10.6922 0.7000 0.5100 0.4650\n",
      "0.00010\n",
      "320 3.8323 9.5760 10.8289 0.6975 0.4950 0.4578\n",
      "0.00010\n",
      "330 3.6770 9.3906 10.6239 0.7137 0.5100 0.4702\n",
      "0.00010\n",
      "340 3.3223 9.3068 10.6270 0.7250 0.5350 0.4682\n",
      "0.00010\n",
      "350 3.5022 9.6291 10.8771 0.7275 0.5550 0.4648\n",
      "0.00010\n",
      "360 3.6008 9.9870 11.3845 0.7137 0.5250 0.4672\n",
      "0.00010\n",
      "370 3.4529 9.7781 11.0998 0.7250 0.5550 0.4670\n",
      "0.00010\n",
      "380 3.6669 10.2332 11.7335 0.7288 0.5450 0.4700\n",
      "0.00010\n",
      "390 3.1509 9.8144 11.0948 0.7538 0.5400 0.4778\n",
      "0.00010\n",
      "400 3.4757 10.4224 11.9482 0.7312 0.5250 0.4688\n",
      "0.00010\n",
      "410 3.2917 10.1298 11.6924 0.7550 0.5250 0.4730\n",
      "0.00010\n",
      "420 3.1452 10.2714 11.8902 0.7425 0.5250 0.4692\n",
      "0.00010\n",
      "430 2.8760 10.2485 11.6877 0.7700 0.5300 0.4720\n",
      "0.00010\n",
      "440 3.1952 10.5930 12.1371 0.7588 0.5650 0.4716\n",
      "0.00010\n",
      "450 2.8399 10.3026 11.7023 0.7725 0.5500 0.4764\n",
      "0.00010\n",
      "460 3.0444 10.9284 12.1735 0.7650 0.5300 0.4734\n",
      "0.00010\n",
      "470 2.8960 10.6648 12.0799 0.7662 0.5400 0.4776\n",
      "0.00010\n",
      "480 2.4692 10.3084 11.6486 0.7987 0.5200 0.4846\n",
      "0.00010\n",
      "490 2.5727 10.4883 11.8928 0.7875 0.5600 0.4846\n",
      "0.00010\n",
      "500 2.6526 10.7165 12.2958 0.7875 0.5350 0.4736\n",
      "0.00010\n",
      "510 3.1895 11.4723 13.2719 0.7712 0.5150 0.4678\n",
      "0.00010\n",
      "520 2.8626 11.2360 12.7148 0.7850 0.5350 0.4768\n",
      "0.00010\n",
      "530 2.3204 10.6595 12.1119 0.8063 0.5600 0.4868\n",
      "0.00010\n",
      "540 2.3290 10.7588 12.1392 0.8025 0.5700 0.4880\n",
      "0.00010\n",
      "550 2.5589 11.2357 12.8512 0.8000 0.5500 0.4774\n",
      "0.00010\n",
      "560 2.5075 11.2877 12.9771 0.8063 0.5600 0.4772\n",
      "0.00009\n",
      "570 2.2443 10.9631 12.6060 0.8200 0.5550 0.4814\n",
      "0.00009\n",
      "580 2.4826 11.4635 12.9626 0.7950 0.5550 0.4818\n",
      "0.00009\n",
      "590 2.2554 11.3701 12.9367 0.8137 0.5800 0.4808\n",
      "0.00009\n",
      "600 2.3727 11.8816 13.4647 0.8013 0.5450 0.4732\n",
      "0.00009\n",
      "610 2.2751 11.5542 13.0857 0.8163 0.5300 0.4842\n",
      "0.00009\n",
      "620 2.1446 11.6890 12.9831 0.8237 0.5500 0.4798\n",
      "0.00009\n",
      "630 2.2217 11.9360 13.3481 0.8225 0.5450 0.4832\n",
      "0.00009\n",
      "640 2.1804 12.0850 13.3640 0.8237 0.5400 0.4826\n",
      "0.00009\n",
      "650 2.1763 12.2557 13.4789 0.8187 0.5300 0.4810\n",
      "0.00009\n",
      "660 2.1940 12.2608 13.6785 0.8137 0.5150 0.4806\n",
      "0.00009\n",
      "670 2.1979 12.3504 13.7675 0.8200 0.5350 0.4822\n",
      "0.00009\n",
      "680 2.0525 11.9359 13.6447 0.8287 0.5250 0.4882\n",
      "0.00009\n",
      "690 1.9597 12.4234 13.7007 0.8413 0.5350 0.4838\n",
      "0.00009\n",
      "0.4808\n"
     ]
    }
   ],
   "source": [
    "max_valid_epoch_idx = 0\n",
    "max_valid_accuracy = 0.0\n",
    "final_test_accuracy = 0.0\n",
    "for epoch_idx in range(1, 10000 + 1):\n",
    "    batches = 0\n",
    "    for x_batch, y_batch in datagen.flow(x_train, y_train, batch_size=batch_size):\n",
    "        x_batch = x_batch.reshape([-1, n_input])\n",
    "        session.run(\n",
    "                train_op,\n",
    "                feed_dict={\n",
    "                    x: x_batch,\n",
    "                    y: y_batch,\n",
    "                    training: True\n",
    "                })\n",
    "        batches += 1\n",
    "        if batches >= len(x_train) / batch_size:\n",
    "            break\n",
    "\n",
    "    if epoch_idx % 10 == 0:\n",
    "        train_loss_value, train_accuracy_value, lr = session.run(\n",
    "            [loss, accuracy, lr_decayed],\n",
    "            feed_dict={\n",
    "                x: x_train.reshape([-1, n_input]),\n",
    "                y: y_train,\n",
    "                training: False\n",
    "            })\n",
    "        \n",
    "        valid_loss_value, valid_accuracy_value = session.run(\n",
    "            [loss, accuracy],\n",
    "            feed_dict={\n",
    "                x: x_valid,\n",
    "                y: y_valid,\n",
    "                training: False\n",
    "            })\n",
    "            \n",
    "        test_loss_value, test_accuracy_value = session.run(\n",
    "            [loss, accuracy],\n",
    "            feed_dict={\n",
    "                x: x_test,\n",
    "                y: y_test,\n",
    "                training: False\n",
    "            })\n",
    "\n",
    "        print(epoch_idx, '%.4f' % train_loss_value, '%.4f' % valid_loss_value, '%.4f' % test_loss_value, '%.4f' % train_accuracy_value, '%.4f' % valid_accuracy_value, '%.4f' % test_accuracy_value)\n",
    "        print('%.5f' % lr)\n",
    "        if max_valid_accuracy < valid_accuracy_value:\n",
    "            max_valid_accuracy = valid_accuracy_value \n",
    "            max_valid_epoch_idx = epoch_idx\n",
    "            final_test_accuracy = test_accuracy_value\n",
    "            \n",
    "    # Early Stop\n",
    "    if max_valid_epoch_idx + 100 < epoch_idx: # Increase Early Stop bound\n",
    "        break\n",
    "        \n",
    "print(final_test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "32.88% 의 성능을 확인할 수 있습니다. 실습시간 배운 몇 가지 정규화와 최적화 과정을 동원하면 50% 정도의 성능까지는 쉽게 달성할 수 있음을 확인했습니다. 수업시간에 배운 내용들을 사용해 최대한 높은 성능을 나타내는 모델을 만들어보세요! \n",
    "주피터 노트북 파일을 제출해주시면 되며, 성능을 기준으로 점수를 매길 예정입니다. (상대평가)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
