{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment on Regularization and Optimization of Deep Learning\n",
    "\n",
    "이번 과제에서는 reagularization과 optimization에서 배운 내용들을 이용해 최대한 Deep Models의 성능을 높여보고자 합니다. Layer 4개짜리 MLP (각 hidden layer는 512개의 unit을 가짐) 상황에서 정규화와 최적화 방법론들을 총 동원해 성능을 높여주시면 됩니다.\n",
    "\n",
    "먼저, 아래 코드는 데이터 셋을 셋팅하는 부분입니다. 이 부분은 건드리시면 안됩니다. 이 부분을 건드리시면 0점 처리 됩니다. 외부 데이터 사용하셔도 안됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 32, 32, 3)\n",
      "(1000,)\n",
      "(5000, 32, 32, 3)\n",
      "(5000,)\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import math\n",
    "import random \n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "seed = 1\n",
    "random.seed(seed)\n",
    "np.random.seed(seed=seed)\n",
    "tf.random.set_random_seed(seed)\n",
    "\n",
    "(x_1, y_1), (x_2, y_2) = tf.keras.datasets.cifar100.load_data()\n",
    "x_total = np.concatenate([x_1, x_2], axis=0).astype(np.float64)\n",
    "y_total = np.concatenate([y_1, y_2], axis=0)\n",
    "\n",
    "n_output = 10\n",
    "\n",
    "valid_index, _ = np.where(y_total < n_output)\n",
    "y_total = y_total[valid_index].reshape([-1])\n",
    "x_total = x_total[valid_index]\n",
    "\n",
    "i = np.arange(x_total.shape[0])\n",
    "np.random.shuffle(i)\n",
    "x_total = x_total[i]\n",
    "y_total = y_total[i]\n",
    "\n",
    "train_size = 100 * n_output\n",
    "x_train = x_total[:train_size]\n",
    "y_train = y_total[:train_size]\n",
    "x_test = x_total[train_size:]\n",
    "y_test = y_total[train_size:]\n",
    "\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "print(x_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "validation set을 나눕니다. \n",
    "- 실습시간에 배웠던 것처럼 Validation set 비율은 조정하셔도 됩니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "split = x_train.shape[0] // 5\n",
    "x_valid = x_train[:split]\n",
    "y_valid = y_train[:split]\n",
    "\n",
    "x_train = x_train[split:]\n",
    "y_train = y_train[split:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이미지를 greyscale로 변경합니다. \n",
    "1. RGB 값을 고려한 코드로 변경하셔도 됩니다. \n",
    "2. Augmentation을 고려해보세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(800, 32, 32, 3)\n",
      "(200, 32, 32, 3)\n",
      "(5000, 32, 32, 3)\n"
     ]
    }
   ],
   "source": [
    "# x_train = np.mean(x_train, axis=3, keepdims=True)\n",
    "# x_valid = np.mean(x_valid, axis=3, keepdims=True)\n",
    "# x_test = np.mean(x_test, axis=3, keepdims=True)\n",
    "# Use RGB and normalize pixel values by training data-channel\n",
    "mean = np.zeros(3)\n",
    "std = np.ones(3)\n",
    "for i in range(3):\n",
    "    mean[i] = np.mean(x_train[:, :, :, i])\n",
    "    std[i] = np.std(x_train[:, :, :, i])    \n",
    "x_train = (x_train - mean) / std\n",
    "x_valid = (x_valid - mean) / std\n",
    "x_test = (x_test - mean) / std\n",
    "\n",
    "epoch = 10000\n",
    "batch_size = 64\n",
    "\n",
    "print(x_train.shape)\n",
    "print(x_valid.shape)\n",
    "print(x_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make Data Generator\n",
    "for augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "datagen = tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "    width_shift_range=5,\n",
    "    height_shift_range=5,\n",
    "    fill_mode='constant',\n",
    "    horizontal_flip=True,\n",
    "    vertical_flip=False,\n",
    "    rotation_range=10,\n",
    "    zoom_range=[0.8, 1.2]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(200, 3072)\n",
      "(5000, 3072)\n"
     ]
    }
   ],
   "source": [
    "n_input = 32 * 32 * 3   # H * W * C\n",
    "\n",
    "# x_train = x_train.reshape([-1, n_input]) This will be done after augmentation.\n",
    "x_valid = x_valid.reshape([-1, n_input])\n",
    "x_test = x_test.reshape([-1, n_input])\n",
    "\n",
    "print(x_valid.shape)\n",
    "print(x_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 모델을 만듭니다.\n",
    "\n",
    "1. Optimizer를 다른 걸로 바꿔보세요\n",
    "2. Learning Rate를 바꿔보세요. Learning Rate Scheduling도 고려해보세요.\n",
    "3. Activation Function을 바꿔보세요. \n",
    "4. Dropout, DropConnect, Gaussian Dropout 을 고려해보세요.\n",
    "5. Augmentation을 고려해보세요. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0724 15:19:55.315403  6424 deprecation.py:323] From c:\\users\\ironm\\tf-nightly\\lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    }
   ],
   "source": [
    "x = tf.placeholder(tf.float32, [None, n_input])\n",
    "y = tf.placeholder(tf.int32, [None])\n",
    "training = tf.placeholder(tf.bool)\n",
    "\n",
    "n_units = [n_input, 512, 512, 512, n_output]\n",
    "\n",
    "weights, biases = [], []\n",
    "for i, (n_in, n_out) in enumerate(zip(n_units[:-1], n_units[1:])):\n",
    "    stddev = math.sqrt(2 / n_in) # Kaiming He Initialization\n",
    "    weight = tf.Variable(tf.random.truncated_normal([n_in, n_out], mean=0, stddev=stddev))\n",
    "    bias = tf.Variable(tf.zeros([n_out]))\n",
    "    weights.append(weight)\n",
    "    biases.append(bias)\n",
    "    \n",
    "layer = x\n",
    "for i, (weight, bias) in enumerate(zip(weights, biases)):\n",
    "    if i < len(weights) - 1:\n",
    "        rate = tf.cond(training, lambda: 0.5, lambda: 0.0)\n",
    "        maxnorm = tf.keras.constraints.MaxNorm(2)\n",
    "        weight = maxnorm(weight)\n",
    "        # weight = tf.nn.dropout(weight, rate=rate) * (1 - rate) # DropConnect\n",
    "        layer = tf.matmul(layer, weight) + bias\n",
    "        layer = tf.nn.relu(layer)\n",
    "        layer = tf.keras.layers.GaussianDropout(0.5)(layer, training)\n",
    "        #layer = tf.nn.dropout(layer, rate=rate)\n",
    "    else:\n",
    "        layer = tf.matmul(layer, weight) + bias\n",
    "y_hat = layer\n",
    "\n",
    "y_hot = tf.one_hot(y, n_output)\n",
    "costs = tf.nn.softmax_cross_entropy_with_logits_v2(\n",
    "        labels=y_hot, logits=y_hat)\n",
    "cross_entropy_loss = tf.reduce_mean(costs)\n",
    "# L2-regularization\n",
    "l2_loss = 0.0\n",
    "for weight in weights:\n",
    "    l2_loss = l2_loss + tf.nn.l2_loss(weight)\n",
    "loss = cross_entropy_loss + 0.009 * l2_loss\n",
    "\n",
    "y_label = tf.argmax(y_hat, 1)\n",
    "accuracy = tf.count_nonzero(\n",
    "        tf.cast(tf.equal(tf.argmax(y_hot, 1), y_label),\n",
    "                tf.int64)) / tf.cast(tf.shape(y_hot)[0], tf.int64)\n",
    "\n",
    "extra_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "with tf.control_dependencies(extra_ops):\n",
    "    global_step = tf.Variable(1, trainable=False)\n",
    "    decay_steps = 20000\n",
    "    learning_rate = 0.0001\n",
    "    lr_decayed = tf.train.cosine_decay(learning_rate, global_step, decay_steps)\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=lr_decayed, beta1=0.85)\n",
    "    train_op = optimizer.minimize(loss, global_step=global_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpu_options = tf.GPUOptions()\n",
    "gpu_options.allow_growth = True\n",
    "session = tf.Session(config=tf.ConfigProto(gpu_options=gpu_options))\n",
    "session.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 12.2525 12.3599 12.3168 0.3837 0.3450 0.3466\n",
      "0.00010\n",
      "20 11.8040 11.9362 11.8870 0.4163 0.3550 0.3628\n",
      "0.00010\n",
      "30 11.3618 11.5150 11.4724 0.4512 0.3800 0.3788\n",
      "0.00010\n",
      "40 10.9529 11.1113 11.0750 0.4487 0.3950 0.3806\n",
      "0.00010\n",
      "50 10.5034 10.6975 10.6505 0.4725 0.3850 0.3972\n",
      "0.00010\n",
      "60 10.0953 10.2973 10.2550 0.4838 0.3850 0.3996\n",
      "0.00010\n",
      "70 9.6988 9.9178 9.8738 0.4763 0.4150 0.4006\n",
      "0.00010\n",
      "80 9.2960 9.5153 9.4813 0.4938 0.4200 0.4092\n",
      "0.00010\n",
      "90 8.9156 9.1556 9.1126 0.4963 0.4050 0.4054\n",
      "0.00010\n",
      "100 8.5214 8.7723 8.7486 0.4950 0.4150 0.4046\n",
      "0.00010\n",
      "110 8.1474 8.3899 8.3760 0.5175 0.4350 0.4192\n",
      "0.00010\n",
      "120 7.7959 8.0593 8.0347 0.5212 0.4450 0.4222\n",
      "0.00010\n",
      "130 7.4355 7.7164 7.6947 0.5300 0.4450 0.4274\n",
      "0.00010\n",
      "140 7.0915 7.3912 7.3692 0.5413 0.4750 0.4364\n",
      "0.00010\n",
      "150 6.7821 7.1037 7.0680 0.5437 0.4600 0.4314\n",
      "0.00010\n",
      "160 6.4661 6.7606 6.7610 0.5450 0.4550 0.4414\n",
      "0.00010\n",
      "170 6.1518 6.4779 6.4692 0.5563 0.4800 0.4428\n",
      "0.00010\n",
      "180 5.8670 6.2171 6.2060 0.5687 0.4700 0.4426\n",
      "0.00010\n",
      "190 5.5977 5.9497 5.9406 0.5813 0.4800 0.4478\n",
      "0.00010\n",
      "200 5.3414 5.7171 5.7045 0.5750 0.5050 0.4482\n",
      "0.00010\n",
      "210 5.0923 5.4626 5.4741 0.5863 0.4850 0.4498\n",
      "0.00010\n",
      "220 4.8414 5.2235 5.2378 0.5988 0.5000 0.4570\n",
      "0.00010\n",
      "230 4.6190 5.0492 5.0495 0.5988 0.4900 0.4500\n",
      "0.00009\n",
      "240 4.4097 4.7933 4.8348 0.6138 0.4800 0.4580\n",
      "0.00009\n",
      "250 4.2008 4.6119 4.6402 0.6225 0.4800 0.4636\n",
      "0.00009\n",
      "260 4.0106 4.4357 4.4711 0.6325 0.4900 0.4586\n",
      "0.00009\n",
      "270 3.8495 4.2729 4.3101 0.6362 0.4900 0.4584\n",
      "0.00009\n",
      "280 3.6729 4.1153 4.1689 0.6312 0.4900 0.4636\n",
      "0.00009\n",
      "290 3.4931 3.9364 3.9947 0.6562 0.4750 0.4716\n",
      "0.00009\n",
      "300 3.3431 3.8037 3.8557 0.6575 0.5050 0.4664\n",
      "0.00009\n",
      "310 3.2109 3.6510 3.7203 0.6663 0.5200 0.4752\n",
      "0.00009\n",
      "320 3.0900 3.5467 3.6314 0.6425 0.5100 0.4634\n",
      "0.00009\n",
      "340 2.8465 3.3671 3.3963 0.6737 0.5100 0.4764\n",
      "0.00009\n",
      "350 2.7481 3.2576 3.3006 0.6675 0.5100 0.4692\n",
      "0.00009\n",
      "360 2.6303 3.1491 3.2263 0.6700 0.5200 0.4790\n",
      "0.00009\n",
      "370 2.5228 3.0550 3.1124 0.6887 0.5250 0.4832\n",
      "0.00009\n",
      "380 2.4285 2.9497 3.0275 0.6875 0.5350 0.4876\n",
      "0.00009\n",
      "390 2.3557 2.8889 2.9758 0.6925 0.5350 0.4818\n",
      "0.00008\n",
      "400 2.2759 2.8353 2.9030 0.6963 0.5250 0.4906\n",
      "0.00008\n",
      "410 2.1900 2.7638 2.8252 0.7063 0.5300 0.4920\n",
      "0.00008\n",
      "420 2.1393 2.7261 2.7873 0.7200 0.5300 0.4888\n",
      "0.00008\n",
      "430 2.0798 2.6445 2.7351 0.7050 0.5100 0.4804\n",
      "0.00008\n",
      "440 2.0163 2.5757 2.6790 0.7050 0.5400 0.4894\n",
      "0.00008\n",
      "450 1.9576 2.5199 2.6202 0.7100 0.5550 0.4902\n",
      "0.00008\n",
      "460 1.8993 2.4975 2.5670 0.7225 0.5350 0.4980\n",
      "0.00008\n",
      "470 1.8778 2.4775 2.5523 0.7075 0.5200 0.4950\n",
      "0.00008\n",
      "480 1.8256 2.4341 2.5136 0.7212 0.5200 0.4922\n",
      "0.00008\n",
      "490 1.7592 2.3477 2.4642 0.7350 0.5550 0.4962\n",
      "0.00008\n",
      "500 1.7253 2.3114 2.4221 0.7350 0.5300 0.4996\n",
      "0.00008\n",
      "510 1.6991 2.3021 2.4073 0.7388 0.5250 0.4938\n",
      "0.00008\n",
      "520 1.6509 2.2291 2.3524 0.7388 0.5300 0.5008\n",
      "0.00007\n",
      "530 1.6215 2.2217 2.3295 0.7375 0.5400 0.5018\n",
      "0.00007\n",
      "540 1.5782 2.1838 2.3047 0.7475 0.5350 0.5010\n",
      "0.00007\n",
      "550 1.5733 2.2066 2.3195 0.7362 0.5150 0.4994\n",
      "0.00007\n",
      "560 1.5257 2.1602 2.2742 0.7575 0.5500 0.5092\n",
      "0.00007\n",
      "570 1.4965 2.1628 2.2630 0.7575 0.5250 0.5026\n",
      "0.00007\n",
      "580 1.4779 2.1231 2.2435 0.7562 0.5300 0.5064\n",
      "0.00007\n",
      "590 1.4448 2.0845 2.2235 0.7638 0.5650 0.5078\n",
      "0.00007\n",
      "600 1.4301 2.0899 2.2196 0.7700 0.5350 0.5036\n",
      "0.00007\n",
      "610 1.4016 2.0642 2.1921 0.7688 0.5650 0.5010\n",
      "0.00007\n",
      "620 1.3856 2.0149 2.1515 0.7762 0.5550 0.5132\n",
      "0.00006\n",
      "630 1.3492 2.0233 2.1498 0.7913 0.5650 0.5104\n",
      "0.00006\n",
      "640 1.3318 2.0153 2.1371 0.7825 0.5450 0.5058\n",
      "0.00006\n",
      "650 1.3260 1.9930 2.1256 0.7825 0.5550 0.5168\n",
      "0.00006\n",
      "660 1.3131 1.9747 2.1183 0.7762 0.5450 0.5088\n",
      "0.00006\n",
      "670 1.3063 1.9893 2.1126 0.7788 0.5400 0.5070\n",
      "0.00006\n",
      "680 1.2804 1.9399 2.0907 0.7825 0.5400 0.5136\n",
      "0.00006\n",
      "690 1.2649 1.9477 2.0877 0.7775 0.5450 0.5094\n",
      "0.00006\n",
      "700 1.2491 1.9506 2.0971 0.7788 0.5500 0.5056\n",
      "0.00006\n",
      "710 1.2442 1.9283 2.0788 0.7900 0.5600 0.5128\n",
      "0.00006\n",
      "720 1.2396 1.9322 2.0620 0.7812 0.5500 0.5128\n",
      "0.00006\n",
      "730 1.2123 1.9100 2.0657 0.7975 0.5600 0.5136\n",
      "0.00005\n",
      "740 1.2104 1.9288 2.0625 0.7963 0.5600 0.5066\n",
      "0.00005\n",
      "750 1.1808 1.8691 2.0155 0.8125 0.5550 0.5200\n",
      "0.00005\n",
      "760 1.1695 1.8679 2.0216 0.8163 0.5750 0.5154\n",
      "0.00005\n",
      "770 1.1676 1.8844 2.0178 0.7987 0.5500 0.5170\n",
      "0.00005\n",
      "780 1.1616 1.8871 2.0282 0.8037 0.5450 0.5136\n",
      "0.00005\n",
      "790 1.1422 1.8873 2.0172 0.8000 0.5550 0.5142\n",
      "0.00005\n",
      "800 1.1261 1.8666 1.9984 0.8087 0.5400 0.5138\n",
      "0.00005\n",
      "810 1.1069 1.8593 1.9897 0.8175 0.5800 0.5234\n",
      "0.00005\n",
      "820 1.1177 1.8748 2.0011 0.8137 0.5350 0.5186\n",
      "0.00004\n",
      "830 1.1031 1.8606 1.9864 0.8150 0.5500 0.5240\n",
      "0.00004\n",
      "840 1.1000 1.8707 1.9927 0.8237 0.5300 0.5232\n",
      "0.00004\n",
      "850 1.0822 1.8562 1.9733 0.8300 0.5500 0.5252\n",
      "0.00004\n",
      "860 1.0749 1.8431 1.9692 0.8213 0.5650 0.5258\n",
      "0.00004\n",
      "870 1.0766 1.8386 1.9799 0.8087 0.5550 0.5182\n",
      "0.00004\n",
      "880 1.0557 1.8228 1.9626 0.8287 0.5500 0.5256\n",
      "0.00004\n",
      "890 1.0672 1.8243 1.9710 0.8263 0.5600 0.5194\n",
      "0.00004\n",
      "900 1.0639 1.8583 1.9899 0.8137 0.5550 0.5148\n",
      "0.00004\n",
      "910 1.0360 1.8179 1.9501 0.8250 0.5500 0.5286\n",
      "0.00004\n",
      "920 1.0361 1.8333 1.9689 0.8275 0.5450 0.5192\n",
      "0.00003\n",
      "930 1.0383 1.8336 1.9767 0.8263 0.5350 0.5156\n",
      "0.00003\n",
      "940 1.0322 1.8358 1.9684 0.8287 0.5450 0.5196\n",
      "0.00003\n",
      "950 1.0152 1.8247 1.9545 0.8387 0.5350 0.5254\n",
      "0.00003\n",
      "960 1.0140 1.8104 1.9577 0.8375 0.5500 0.5226\n",
      "0.00003\n",
      "970 1.0128 1.8073 1.9507 0.8313 0.5500 0.5210\n",
      "0.00003\n",
      "980 0.9992 1.8205 1.9550 0.8350 0.5450 0.5208\n",
      "0.00003\n",
      "990 1.0046 1.8149 1.9499 0.8325 0.5500 0.5248\n",
      "0.00003\n",
      "1000 0.9960 1.8164 1.9429 0.8425 0.5450 0.5234\n",
      "0.00003\n",
      "1010 0.9923 1.8167 1.9526 0.8400 0.5550 0.5210\n",
      "0.00003\n",
      "1020 0.9795 1.7939 1.9375 0.8488 0.5600 0.5246\n",
      "0.00003\n",
      "1030 0.9844 1.8035 1.9441 0.8313 0.5600 0.5252\n",
      "0.00002\n",
      "1040 0.9776 1.7845 1.9354 0.8350 0.5750 0.5244\n",
      "0.00002\n",
      "1050 0.9727 1.7908 1.9351 0.8450 0.5550 0.5256\n",
      "0.00002\n",
      "1060 0.9657 1.7890 1.9317 0.8550 0.5600 0.5284\n",
      "0.00002\n",
      "1070 0.9782 1.8089 1.9461 0.8413 0.5500 0.5242\n",
      "0.00002\n",
      "1080 0.9597 1.7887 1.9273 0.8500 0.5550 0.5278\n",
      "0.00002\n",
      "1090 0.9545 1.7845 1.9301 0.8600 0.5700 0.5270\n",
      "0.00002\n",
      "1100 0.9540 1.7924 1.9356 0.8425 0.5600 0.5312\n",
      "0.00002\n",
      "1110 0.9480 1.7854 1.9311 0.8500 0.5800 0.5330\n",
      "0.00002\n",
      "0.5234\n"
     ]
    }
   ],
   "source": [
    "max_valid_epoch_idx = 0\n",
    "max_valid_accuracy = 0.0\n",
    "final_test_accuracy = 0.0\n",
    "for epoch_idx in range(1, 10000 + 1):\n",
    "    batches = 0\n",
    "    for x_batch, y_batch in datagen.flow(x_train, y_train, batch_size=batch_size):\n",
    "        x_batch = x_batch.reshape([-1, n_input])\n",
    "        session.run(\n",
    "                train_op,\n",
    "                feed_dict={\n",
    "                    x: x_batch,\n",
    "                    y: y_batch,\n",
    "                    training: True\n",
    "                })\n",
    "        batches += 1\n",
    "        if batches >= len(x_train) / batch_size:\n",
    "            break\n",
    "\n",
    "    if epoch_idx % 10 == 0:\n",
    "        train_loss_value, train_accuracy_value, lr = session.run(\n",
    "            [loss, accuracy, lr_decayed],\n",
    "            feed_dict={\n",
    "                x: x_train.reshape([-1, n_input]),\n",
    "                y: y_train,\n",
    "                training: False\n",
    "            })\n",
    "        \n",
    "        valid_loss_value, valid_accuracy_value = session.run(\n",
    "            [loss, accuracy],\n",
    "            feed_dict={\n",
    "                x: x_valid,\n",
    "                y: y_valid,\n",
    "                training: False\n",
    "            })\n",
    "            \n",
    "        test_loss_value, test_accuracy_value = session.run(\n",
    "            [loss, accuracy],\n",
    "            feed_dict={\n",
    "                x: x_test,\n",
    "                y: y_test,\n",
    "                training: False\n",
    "            })\n",
    "\n",
    "        print(epoch_idx, '%.4f' % train_loss_value, '%.4f' % valid_loss_value, '%.4f' % test_loss_value, '%.4f' % train_accuracy_value, '%.4f' % valid_accuracy_value, '%.4f' % test_accuracy_value)\n",
    "        print('%.5f' % lr)\n",
    "        if max_valid_accuracy < valid_accuracy_value:\n",
    "            max_valid_accuracy = valid_accuracy_value \n",
    "            max_valid_epoch_idx = epoch_idx\n",
    "            final_test_accuracy = test_accuracy_value\n",
    "            \n",
    "    # Early Stop\n",
    "    if max_valid_epoch_idx + 300 < epoch_idx: # Increase Early Stop bound\n",
    "        break\n",
    "        \n",
    "print(final_test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    32.88% 의 성능을 확인할 수 있습니다. 실습시간 배운 몇 가지 정규화와 최적화 과정을 동원하면 50% 정도의 성능까지는 쉽게 달성할 수 있음을 확인했습니다. 수업시간에 배운 내용들을 사용해 최대한 높은 성능을 나타내는 모델을 만들어보세요! \n",
    "주피터 노트북 파일을 제출해주시면 되며, 성능을 기준으로 점수를 매길 예정입니다. (상대평가)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
