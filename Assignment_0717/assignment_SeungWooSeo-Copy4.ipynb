{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment on Regularization and Optimization of Deep Learning\n",
    "\n",
    "이번 과제에서는 reagularization과 optimization에서 배운 내용들을 이용해 최대한 Deep Models의 성능을 높여보고자 합니다. Layer 4개짜리 MLP (각 hidden layer는 512개의 unit을 가짐) 상황에서 정규화와 최적화 방법론들을 총 동원해 성능을 높여주시면 됩니다.\n",
    "\n",
    "먼저, 아래 코드는 데이터 셋을 셋팅하는 부분입니다. 이 부분은 건드리시면 안됩니다. 이 부분을 건드리시면 0점 처리 됩니다. 외부 데이터 사용하셔도 안됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 32, 32, 3)\n",
      "(1000,)\n",
      "(5000, 32, 32, 3)\n",
      "(5000,)\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import math\n",
    "import random \n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "seed = 1\n",
    "random.seed(seed)\n",
    "np.random.seed(seed=seed)\n",
    "tf.random.set_random_seed(seed)\n",
    "\n",
    "(x_1, y_1), (x_2, y_2) = tf.keras.datasets.cifar100.load_data()\n",
    "x_total = np.concatenate([x_1, x_2], axis=0).astype(np.float64)\n",
    "y_total = np.concatenate([y_1, y_2], axis=0)\n",
    "\n",
    "n_output = 10\n",
    "\n",
    "valid_index, _ = np.where(y_total < n_output)\n",
    "y_total = y_total[valid_index].reshape([-1])\n",
    "x_total = x_total[valid_index]\n",
    "\n",
    "i = np.arange(x_total.shape[0])\n",
    "np.random.shuffle(i)\n",
    "x_total = x_total[i]\n",
    "y_total = y_total[i]\n",
    "\n",
    "train_size = 100 * n_output\n",
    "x_train = x_total[:train_size]\n",
    "y_train = y_total[:train_size]\n",
    "x_test = x_total[train_size:]\n",
    "y_test = y_total[train_size:]\n",
    "\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "print(x_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "validation set을 나눕니다. \n",
    "- 실습시간에 배웠던 것처럼 Validation set 비율은 조정하셔도 됩니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "split = x_train.shape[0] // 5\n",
    "x_valid = x_train[:split]\n",
    "y_valid = y_train[:split]\n",
    "\n",
    "x_train = x_train[split:]\n",
    "y_train = y_train[split:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이미지를 greyscale로 변경합니다. \n",
    "1. RGB 값을 고려한 코드로 변경하셔도 됩니다. \n",
    "2. Augmentation을 고려해보세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(800, 32, 32, 3)\n",
      "(200, 32, 32, 3)\n",
      "(5000, 32, 32, 3)\n"
     ]
    }
   ],
   "source": [
    "# x_train = np.mean(x_train, axis=3, keepdims=True)\n",
    "# x_valid = np.mean(x_valid, axis=3, keepdims=True)\n",
    "# x_test = np.mean(x_test, axis=3, keepdims=True)\n",
    "# Use RGB and normalize pixel values by training data-channel\n",
    "mean = np.zeros(3)\n",
    "std = np.ones(3)\n",
    "for i in range(3):\n",
    "    mean[i] = np.mean(x_train[:, :, :, i])\n",
    "    std[i] = np.std(x_train[:, :, :, i])    \n",
    "x_train = (x_train - mean) / std\n",
    "x_valid = (x_valid - mean) / std\n",
    "x_test = (x_test - mean) / std\n",
    "\n",
    "epoch = 10000\n",
    "batch_size = 64\n",
    "\n",
    "print(x_train.shape)\n",
    "print(x_valid.shape)\n",
    "print(x_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make Data Generator\n",
    "for augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "datagen = tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "    width_shift_range=3,\n",
    "    height_shift_range=3,\n",
    "    fill_mode='constant',\n",
    "    horizontal_flip=True,\n",
    "    vertical_flip=False,\n",
    "    rotation_range=10,\n",
    "    zoom_range=[0.8, 1.2]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(200, 3072)\n",
      "(5000, 3072)\n"
     ]
    }
   ],
   "source": [
    "n_input = 32 * 32 * 3   # H * W * C\n",
    "\n",
    "# x_train = x_train.reshape([-1, n_input]) This will be done after augmentation.\n",
    "x_valid = x_valid.reshape([-1, n_input])\n",
    "x_test = x_test.reshape([-1, n_input])\n",
    "\n",
    "print(x_valid.shape)\n",
    "print(x_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 모델을 만듭니다.\n",
    "\n",
    "1. Optimizer를 다른 걸로 바꿔보세요\n",
    "2. Learning Rate를 바꿔보세요. Learning Rate Scheduling도 고려해보세요.\n",
    "3. Activation Function을 바꿔보세요. \n",
    "4. Dropout, DropConnect, Gaussian Dropout 을 고려해보세요.\n",
    "5. Augmentation을 고려해보세요. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0727 00:13:14.106511  4652 deprecation.py:323] From c:\\users\\ironm\\tf-nightly\\lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    }
   ],
   "source": [
    "x = tf.placeholder(tf.float32, [None, n_input])\n",
    "y = tf.placeholder(tf.int32, [None])\n",
    "training = tf.placeholder(tf.bool)\n",
    "\n",
    "n_units = [n_input, 512, 512, 512, n_output]\n",
    "\n",
    "weights, biases = [], []\n",
    "for i, (n_in, n_out) in enumerate(zip(n_units[:-1], n_units[1:])):\n",
    "    stddev = math.sqrt(2 / n_in) # Kaiming He Initialization\n",
    "    weight = tf.Variable(tf.random.truncated_normal([n_in, n_out], mean=0, stddev=stddev))\n",
    "    bias = tf.Variable(tf.zeros([n_out]))\n",
    "    weights.append(weight)\n",
    "    biases.append(bias)\n",
    "\n",
    "layer = x\n",
    "\n",
    "for i, (weight, bias) in enumerate(zip(weights, biases)):\n",
    "    if i < len(weights) - 1:\n",
    "        rate = tf.cond(training, lambda: 0.5, lambda: 0.0)\n",
    "        maxnorm = tf.keras.constraints.MaxNorm(2)\n",
    "        weight = maxnorm(weight)\n",
    "        # weight = tf.nn.dropout(weight, rate=rate) * (1 - rate) # DropConnect\n",
    "        layer = tf.matmul(layer, weight) + bias\n",
    "        layer = tf.nn.relu(layer)\n",
    "        layer = tf.keras.layers.GaussianDropout(0.5)(layer, training)\n",
    "        #layer = tf.nn.dropout(layer, rate=rate)\n",
    "    else:\n",
    "        layer = tf.matmul(layer, weight) + bias\n",
    "y_hat = layer\n",
    "\n",
    "y_hot = tf.one_hot(y, n_output)\n",
    "costs = tf.nn.softmax_cross_entropy_with_logits_v2(\n",
    "        labels=y_hot, logits=y_hat)\n",
    "cross_entropy_loss = tf.reduce_mean(costs)\n",
    "# L2-regularization\n",
    "l2_loss = 0.0\n",
    "for weight in weights:\n",
    "    l2_loss = l2_loss + tf.nn.l2_loss(weight)\n",
    "loss = cross_entropy_loss + 0.009 * l2_loss\n",
    "\n",
    "y_label = tf.argmax(y_hat, 1)\n",
    "accuracy = tf.count_nonzero(\n",
    "        tf.cast(tf.equal(tf.argmax(y_hot, 1), y_label),\n",
    "                tf.int64)) / tf.cast(tf.shape(y_hot)[0], tf.int64)\n",
    "\n",
    "extra_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "with tf.control_dependencies(extra_ops):\n",
    "    global_step = tf.Variable(1, trainable=False)\n",
    "    decay_steps = 20000\n",
    "    learning_rate = 0.0002\n",
    "    lr_decayed = tf.train.cosine_decay(learning_rate, global_step, decay_steps)\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=lr_decayed, beta1=0.85)\n",
    "    train_op = optimizer.minimize(loss, global_step=global_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpu_options = tf.GPUOptions()\n",
    "gpu_options.allow_growth = True\n",
    "session = tf.Session(config=tf.ConfigProto(gpu_options=gpu_options))\n",
    "session.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 11.8350 12.0090 11.9367 0.4425 0.3550 0.3572\n",
      "0.00020\n",
      "20 11.0438 11.2382 11.1845 0.4738 0.3750 0.3954\n",
      "0.00020\n",
      "30 10.2703 10.5014 10.4570 0.4775 0.4000 0.4058\n",
      "0.00020\n",
      "40 9.5445 9.7873 9.7458 0.5262 0.3750 0.4212\n",
      "0.00020\n",
      "50 8.8365 9.1099 9.0674 0.5350 0.4000 0.4274\n",
      "0.00020\n",
      "60 8.1561 8.4662 8.4269 0.5463 0.4100 0.4354\n",
      "0.00020\n",
      "70 7.5125 7.8560 7.8238 0.5575 0.4100 0.4360\n",
      "0.00020\n",
      "80 6.9075 7.2777 7.2448 0.5763 0.4450 0.4452\n",
      "0.00020\n",
      "90 6.3273 6.7353 6.7236 0.6050 0.4100 0.4466\n",
      "0.00020\n",
      "100 5.7958 6.2456 6.2157 0.6188 0.4550 0.4560\n",
      "0.00020\n",
      "110 5.3106 5.7818 5.7659 0.6250 0.4700 0.4528\n",
      "0.00020\n",
      "120 4.8512 5.3553 5.3582 0.6462 0.4600 0.4570\n",
      "0.00020\n",
      "130 4.4351 4.9843 4.9934 0.6562 0.4650 0.4568\n",
      "0.00020\n",
      "140 4.0800 4.6306 4.6422 0.6550 0.4650 0.4616\n",
      "0.00020\n",
      "150 3.7341 4.3514 4.3605 0.6713 0.4750 0.4690\n",
      "0.00020\n",
      "160 3.4367 4.0737 4.0946 0.6800 0.4650 0.4724\n",
      "0.00019\n",
      "170 3.1413 3.7668 3.8184 0.7137 0.4700 0.4760\n",
      "0.00019\n",
      "180 2.8775 3.5351 3.6079 0.7325 0.4850 0.4752\n",
      "0.00019\n",
      "190 2.6909 3.3406 3.4393 0.7288 0.5150 0.4722\n",
      "0.00019\n",
      "200 2.4836 3.2074 3.2616 0.7350 0.4900 0.4810\n",
      "0.00019\n",
      "210 2.3196 3.0482 3.1194 0.7362 0.5450 0.4860\n",
      "0.00019\n",
      "220 2.1557 2.9075 2.9740 0.7512 0.5000 0.4870\n",
      "0.00019\n",
      "230 2.0054 2.7271 2.8449 0.7863 0.5150 0.4942\n",
      "0.00019\n",
      "240 1.8895 2.6492 2.7669 0.7987 0.5500 0.4862\n",
      "0.00019\n",
      "250 1.8105 2.6383 2.7193 0.7825 0.5150 0.4766\n",
      "0.00019\n",
      "260 1.6918 2.4721 2.5985 0.8063 0.5350 0.5000\n",
      "0.00019\n",
      "270 1.5956 2.4294 2.5446 0.8113 0.5150 0.4934\n",
      "0.00019\n",
      "280 1.5214 2.3522 2.4679 0.8263 0.5300 0.4984\n",
      "0.00018\n",
      "290 1.4725 2.3586 2.4779 0.8225 0.5350 0.4900\n",
      "0.00018\n",
      "300 1.3975 2.2719 2.4159 0.8450 0.5600 0.4986\n",
      "0.00018\n",
      "310 1.3510 2.2354 2.3800 0.8375 0.5400 0.4990\n",
      "0.00018\n",
      "320 1.3043 2.2429 2.3309 0.8400 0.5350 0.5018\n",
      "0.00018\n",
      "330 1.2675 2.2276 2.3121 0.8488 0.5150 0.4912\n",
      "0.00018\n",
      "340 1.2158 2.1599 2.2844 0.8688 0.5550 0.4926\n",
      "0.00018\n",
      "350 1.1668 2.1483 2.2860 0.8675 0.5500 0.5030\n",
      "0.00018\n",
      "360 1.1522 2.1366 2.2396 0.8775 0.5650 0.5128\n",
      "0.00017\n",
      "370 1.1070 2.1118 2.2284 0.8838 0.5300 0.5110\n",
      "0.00017\n",
      "380 1.0900 2.1135 2.2256 0.8788 0.5100 0.5004\n",
      "0.00017\n",
      "390 1.0564 2.0800 2.2072 0.8850 0.5400 0.5112\n",
      "0.00017\n",
      "400 1.0318 2.0718 2.1714 0.9038 0.5250 0.5030\n",
      "0.00017\n",
      "410 1.0061 2.0998 2.2013 0.8925 0.5350 0.5060\n",
      "0.00017\n",
      "420 1.0035 2.0638 2.1887 0.8988 0.5450 0.5112\n",
      "0.00017\n",
      "430 0.9935 2.0770 2.1620 0.8938 0.5250 0.5108\n",
      "0.00016\n",
      "440 0.9658 2.0897 2.1799 0.9000 0.5300 0.5116\n",
      "0.00016\n",
      "450 0.9493 2.0607 2.1703 0.9075 0.5400 0.5094\n",
      "0.00016\n",
      "460 0.9181 2.0307 2.1488 0.9363 0.5400 0.5124\n",
      "0.00016\n",
      "470 0.9261 2.0764 2.1966 0.9225 0.5350 0.5040\n",
      "0.00016\n",
      "480 0.9069 2.1053 2.1615 0.9300 0.5350 0.5050\n",
      "0.00016\n",
      "490 0.9049 2.0429 2.1633 0.9287 0.5250 0.5066\n",
      "0.00015\n",
      "500 0.8821 2.0087 2.1284 0.9350 0.5500 0.5124\n",
      "0.00015\n",
      "510 0.8774 2.0626 2.1702 0.9287 0.5450 0.5082\n",
      "0.00015\n",
      "520 0.8665 2.0549 2.1480 0.9450 0.5400 0.5138\n",
      "0.00015\n",
      "530 0.8584 2.0406 2.1436 0.9387 0.5400 0.5140\n",
      "0.00015\n",
      "540 0.8689 2.0751 2.1889 0.9263 0.5200 0.5086\n",
      "0.00015\n",
      "550 0.8409 2.0677 2.1474 0.9437 0.5150 0.5160\n",
      "0.00014\n",
      "560 0.8336 2.0388 2.1362 0.9400 0.5550 0.5176\n",
      "0.00014\n",
      "570 0.8260 2.0538 2.1608 0.9363 0.5350 0.5142\n",
      "0.00014\n",
      "580 0.8140 2.0832 2.1874 0.9500 0.5400 0.5158\n",
      "0.00014\n",
      "590 0.8222 2.0800 2.1804 0.9350 0.4950 0.5124\n",
      "0.00014\n",
      "600 0.8138 2.0591 2.1613 0.9425 0.5550 0.5094\n",
      "0.00013\n",
      "610 0.7934 2.0326 2.1600 0.9550 0.5400 0.5070\n",
      "0.00013\n",
      "620 0.7865 2.0361 2.1621 0.9525 0.5600 0.5146\n",
      "0.00013\n",
      "630 0.7826 2.0376 2.1495 0.9663 0.5250 0.5152\n",
      "0.00013\n",
      "640 0.7834 2.0684 2.1449 0.9600 0.5350 0.5140\n",
      "0.00013\n",
      "650 0.7881 2.0886 2.1615 0.9613 0.5450 0.5160\n",
      "0.00012\n",
      "660 0.7903 2.0970 2.2108 0.9487 0.5400 0.5074\n",
      "0.00012\n",
      "670 0.7695 2.1320 2.1735 0.9650 0.5100 0.5132\n",
      "0.00012\n",
      "680 0.7571 2.0380 2.1543 0.9688 0.5650 0.5198\n",
      "0.00012\n",
      "690 0.7443 2.0561 2.1606 0.9725 0.5650 0.5220\n",
      "0.00012\n",
      "700 0.7430 2.0484 2.1491 0.9675 0.5500 0.5202\n",
      "0.00011\n",
      "710 0.7531 2.0687 2.1983 0.9587 0.5450 0.5108\n",
      "0.00011\n",
      "720 0.7388 2.0453 2.1602 0.9675 0.5450 0.5184\n",
      "0.00011\n",
      "730 0.7449 2.0314 2.1595 0.9675 0.5650 0.5170\n",
      "0.00011\n",
      "740 0.7397 2.0772 2.1783 0.9625 0.5600 0.5160\n",
      "0.00011\n",
      "750 0.7222 2.0218 2.1701 0.9675 0.5550 0.5140\n",
      "0.00010\n",
      "760 0.7212 2.0730 2.1852 0.9725 0.5500 0.5184\n",
      "0.00010\n",
      "770 0.7192 2.0547 2.1756 0.9700 0.5550 0.5166\n",
      "0.00010\n",
      "780 0.7075 2.0621 2.1604 0.9762 0.5650 0.5184\n",
      "0.00010\n",
      "790 0.7078 2.0506 2.1636 0.9750 0.5400 0.5188\n",
      "0.00010\n",
      "800 0.6985 2.0339 2.1750 0.9788 0.5550 0.5222\n",
      "0.00009\n",
      "810 0.7199 2.0706 2.2436 0.9613 0.5550 0.5124\n",
      "0.00009\n",
      "820 0.6971 2.0591 2.1798 0.9762 0.5300 0.5188\n",
      "0.00009\n",
      "830 0.6883 2.0838 2.2055 0.9812 0.5300 0.5194\n",
      "0.00009\n",
      "840 0.6821 2.0398 2.1814 0.9762 0.5650 0.5228\n",
      "0.00009\n",
      "850 0.6899 2.0701 2.1868 0.9738 0.5400 0.5278\n",
      "0.00008\n",
      "860 0.6825 2.0594 2.1695 0.9775 0.5400 0.5228\n",
      "0.00008\n",
      "0.5128\n"
     ]
    }
   ],
   "source": [
    "max_valid_epoch_idx = 0\n",
    "max_valid_accuracy = 0.0\n",
    "final_test_accuracy = 0.0\n",
    "for epoch_idx in range(1, 10000 + 1):\n",
    "    batches = 0\n",
    "    for x_batch, y_batch in datagen.flow(x_train, y_train, batch_size=batch_size):\n",
    "        x_batch = x_batch.reshape([-1, n_input])\n",
    "        session.run(\n",
    "                train_op,\n",
    "                feed_dict={\n",
    "                    x: x_batch,\n",
    "                    y: y_batch,\n",
    "                    training: True\n",
    "                })\n",
    "        batches += 1\n",
    "        if batches >= len(x_train) / batch_size:\n",
    "            break\n",
    "\n",
    "    if epoch_idx % 10 == 0:\n",
    "        train_loss_value, train_accuracy_value, lr = session.run(\n",
    "            [loss, accuracy, lr_decayed],\n",
    "            feed_dict={\n",
    "                x: x_train.reshape([-1, n_input]),\n",
    "                y: y_train,\n",
    "                training: False\n",
    "            })\n",
    "        \n",
    "        valid_loss_value, valid_accuracy_value = session.run(\n",
    "            [loss, accuracy],\n",
    "            feed_dict={\n",
    "                x: x_valid,\n",
    "                y: y_valid,\n",
    "                training: False\n",
    "            })\n",
    "            \n",
    "        test_loss_value, test_accuracy_value = session.run(\n",
    "            [loss, accuracy],\n",
    "            feed_dict={\n",
    "                x: x_test,\n",
    "                y: y_test,\n",
    "                training: False\n",
    "            })\n",
    "\n",
    "        print(epoch_idx, '%.4f' % train_loss_value, '%.4f' % valid_loss_value, '%.4f' % test_loss_value, '%.4f' % train_accuracy_value, '%.4f' % valid_accuracy_value, '%.4f' % test_accuracy_value)\n",
    "        print('%.5f' % lr)\n",
    "        if max_valid_accuracy < valid_accuracy_value:\n",
    "            max_valid_accuracy = valid_accuracy_value \n",
    "            max_valid_epoch_idx = epoch_idx\n",
    "            final_test_accuracy = test_accuracy_value\n",
    "            \n",
    "    # Early Stop\n",
    "    if max_valid_epoch_idx + 500 < epoch_idx: # Increase Early Stop bound\n",
    "        break\n",
    "        \n",
    "print(final_test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    32.88% 의 성능을 확인할 수 있습니다. 실습시간 배운 몇 가지 정규화와 최적화 과정을 동원하면 50% 정도의 성능까지는 쉽게 달성할 수 있음을 확인했습니다. 수업시간에 배운 내용들을 사용해 최대한 높은 성능을 나타내는 모델을 만들어보세요! \n",
    "주피터 노트북 파일을 제출해주시면 되며, 성능을 기준으로 점수를 매길 예정입니다. (상대평가)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
