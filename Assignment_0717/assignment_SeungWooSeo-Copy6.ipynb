{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment on Regularization and Optimization of Deep Learning\n",
    "\n",
    "이번 과제에서는 reagularization과 optimization에서 배운 내용들을 이용해 최대한 Deep Models의 성능을 높여보고자 합니다. Layer 4개짜리 MLP (각 hidden layer는 512개의 unit을 가짐) 상황에서 정규화와 최적화 방법론들을 총 동원해 성능을 높여주시면 됩니다.\n",
    "\n",
    "먼저, 아래 코드는 데이터 셋을 셋팅하는 부분입니다. 이 부분은 건드리시면 안됩니다. 이 부분을 건드리시면 0점 처리 됩니다. 외부 데이터 사용하셔도 안됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 32, 32, 3)\n",
      "(1000,)\n",
      "(5000, 32, 32, 3)\n",
      "(5000,)\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import math\n",
    "import random \n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "seed = 1\n",
    "random.seed(seed)\n",
    "np.random.seed(seed=seed)\n",
    "tf.random.set_random_seed(seed)\n",
    "\n",
    "(x_1, y_1), (x_2, y_2) = tf.keras.datasets.cifar100.load_data()\n",
    "x_total = np.concatenate([x_1, x_2], axis=0).astype(np.float64)\n",
    "y_total = np.concatenate([y_1, y_2], axis=0)\n",
    "\n",
    "n_output = 10\n",
    "\n",
    "valid_index, _ = np.where(y_total < n_output)\n",
    "y_total = y_total[valid_index].reshape([-1])\n",
    "x_total = x_total[valid_index]\n",
    "\n",
    "i = np.arange(x_total.shape[0])\n",
    "np.random.shuffle(i)\n",
    "x_total = x_total[i]\n",
    "y_total = y_total[i]\n",
    "\n",
    "train_size = 100 * n_output\n",
    "x_train = x_total[:train_size]\n",
    "y_train = y_total[:train_size]\n",
    "x_test = x_total[train_size:]\n",
    "y_test = y_total[train_size:]\n",
    "\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "print(x_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "validation set을 나눕니다. \n",
    "- 실습시간에 배웠던 것처럼 Validation set 비율은 조정하셔도 됩니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "split = x_train.shape[0] // 6\n",
    "x_valid = x_train[:split]\n",
    "y_valid = y_train[:split]\n",
    "\n",
    "x_train = x_train[split:]\n",
    "y_train = y_train[split:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이미지를 greyscale로 변경합니다. \n",
    "1. RGB 값을 고려한 코드로 변경하셔도 됩니다. \n",
    "2. Augmentation을 고려해보세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(834, 32, 32, 3)\n",
      "(166, 32, 32, 3)\n",
      "(5000, 32, 32, 3)\n"
     ]
    }
   ],
   "source": [
    "# x_train = np.mean(x_train, axis=3, keepdims=True)\n",
    "# x_valid = np.mean(x_valid, axis=3, keepdims=True)\n",
    "# x_test = np.mean(x_test, axis=3, keepdims=True)\n",
    "# Use RGB and normalize pixel values by training data-channel\n",
    "mean = np.zeros(3)\n",
    "std = np.ones(3)\n",
    "for i in range(3):\n",
    "    mean[i] = np.mean(x_train[:, :, :, i])\n",
    "    std[i] = np.std(x_train[:, :, :, i])    \n",
    "x_train = (x_train - mean) / std\n",
    "x_valid = (x_valid - mean) / std\n",
    "x_test = (x_test - mean) / std\n",
    "\n",
    "epoch = 10000\n",
    "batch_size = 64\n",
    "\n",
    "print(x_train.shape)\n",
    "print(x_valid.shape)\n",
    "print(x_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make Data Generator\n",
    "for augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "datagen = tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "    width_shift_range=5,\n",
    "    height_shift_range=5,\n",
    "    fill_mode='constant',\n",
    "    horizontal_flip=True,\n",
    "    vertical_flip=False,\n",
    "    rotation_range=10,\n",
    "    zoom_range=[0.8, 1.2]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(166, 3072)\n",
      "(5000, 3072)\n"
     ]
    }
   ],
   "source": [
    "n_input = 32 * 32 * 3   # H * W * C\n",
    "\n",
    "# x_train = x_train.reshape([-1, n_input]) This will be done after augmentation.\n",
    "x_valid = x_valid.reshape([-1, n_input])\n",
    "x_test = x_test.reshape([-1, n_input])\n",
    "\n",
    "print(x_valid.shape)\n",
    "print(x_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 모델을 만듭니다.\n",
    "\n",
    "1. Optimizer를 다른 걸로 바꿔보세요\n",
    "2. Learning Rate를 바꿔보세요. Learning Rate Scheduling도 고려해보세요.\n",
    "3. Activation Function을 바꿔보세요. \n",
    "4. Dropout, DropConnect, Gaussian Dropout 을 고려해보세요.\n",
    "5. Augmentation을 고려해보세요. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0728 07:01:30.473685  2244 deprecation.py:323] From c:\\users\\ironm\\tf-nightly\\lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    }
   ],
   "source": [
    "x = tf.placeholder(tf.float32, [None, n_input])\n",
    "y = tf.placeholder(tf.int32, [None])\n",
    "training = tf.placeholder(tf.bool)\n",
    "\n",
    "n_units = [n_input, 512, 512, 512, n_output]\n",
    "\n",
    "weights, biases = [], []\n",
    "for i, (n_in, n_out) in enumerate(zip(n_units[:-1], n_units[1:])):\n",
    "    stddev = math.sqrt(2 / n_in) # Kaiming He Initialization\n",
    "    weight = tf.Variable(tf.random.truncated_normal([n_in, n_out], mean=0, stddev=stddev))\n",
    "    bias = tf.Variable(tf.zeros([n_out]))\n",
    "    weights.append(weight)\n",
    "    biases.append(bias)\n",
    "    \n",
    "layer = x\n",
    "for i, (weight, bias) in enumerate(zip(weights, biases)):\n",
    "    if i < len(weights) - 1:\n",
    "        rate = tf.cond(training, lambda: 0.5, lambda: 0.0)\n",
    "        maxnorm = tf.keras.constraints.MaxNorm(2)\n",
    "        weight = maxnorm(weight)\n",
    "        # weight = tf.nn.dropout(weight, rate=rate) * (1 - rate) # DropConnect\n",
    "        layer = tf.matmul(layer, weight) + bias\n",
    "        layer = tf.nn.relu(layer)\n",
    "        layer = tf.keras.layers.GaussianDropout(0.5)(layer, training)\n",
    "        #layer = tf.nn.dropout(layer, rate=rate)\n",
    "    else:\n",
    "        layer = tf.matmul(layer, weight) + bias\n",
    "y_hat = layer\n",
    "\n",
    "y_hot = tf.one_hot(y, n_output)\n",
    "costs = tf.nn.softmax_cross_entropy_with_logits_v2(\n",
    "        labels=y_hot, logits=y_hat)\n",
    "cross_entropy_loss = tf.reduce_mean(costs)\n",
    "# L2-regularization\n",
    "l2_loss = 0.0\n",
    "for weight in weights:\n",
    "    l2_loss = l2_loss + tf.nn.l2_loss(weight)\n",
    "loss = cross_entropy_loss + 0.009 * l2_loss\n",
    "\n",
    "y_label = tf.argmax(y_hat, 1)\n",
    "accuracy = tf.count_nonzero(\n",
    "        tf.cast(tf.equal(tf.argmax(y_hot, 1), y_label),\n",
    "                tf.int64)) / tf.cast(tf.shape(y_hot)[0], tf.int64)\n",
    "\n",
    "extra_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "with tf.control_dependencies(extra_ops):\n",
    "    global_step = tf.Variable(1, trainable=False)\n",
    "    decay_steps = 20000\n",
    "    learning_rate = 0.00011\n",
    "    lr_decayed = tf.train.cosine_decay(learning_rate, global_step, decay_steps)\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=lr_decayed, beta1=0.85)\n",
    "    train_op = optimizer.minimize(loss, global_step=global_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpu_options = tf.GPUOptions()\n",
    "gpu_options.allow_growth = True\n",
    "session = tf.Session(config=tf.ConfigProto(gpu_options=gpu_options))\n",
    "session.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 12.2947 12.4052 12.3542 0.3405 0.2892 0.3104\n",
      "0.00011\n",
      "20 11.8864 12.0211 11.9620 0.3909 0.3614 0.3580\n",
      "0.00011\n",
      "30 11.5158 11.6776 11.6101 0.3993 0.3855 0.3584\n",
      "0.00011\n",
      "40 11.1815 11.3188 11.2715 0.4388 0.4277 0.3796\n",
      "0.00011\n",
      "50 10.8461 11.0050 10.9452 0.4197 0.3614 0.3740\n",
      "0.00011\n",
      "60 10.5164 10.7004 10.6270 0.4448 0.3976 0.3958\n",
      "0.00011\n",
      "70 10.2051 10.3966 10.3274 0.4508 0.3675 0.3968\n",
      "0.00011\n",
      "80 9.8795 10.0613 10.0049 0.4736 0.3916 0.4092\n",
      "0.00011\n",
      "90 9.5851 9.7945 9.7250 0.4652 0.3735 0.4038\n",
      "0.00011\n",
      "100 9.2810 9.4849 9.4253 0.4940 0.4096 0.4114\n",
      "0.00011\n",
      "110 8.9952 9.2144 9.1514 0.4736 0.3855 0.4030\n",
      "0.00011\n",
      "120 8.7099 8.9320 8.8749 0.4928 0.4096 0.4100\n",
      "0.00011\n",
      "130 8.4335 8.6834 8.6145 0.4808 0.4036 0.4046\n",
      "0.00011\n",
      "140 8.1734 8.4473 8.3602 0.4760 0.3855 0.4012\n",
      "0.00011\n",
      "150 7.8847 8.1611 8.0851 0.4904 0.4157 0.4094\n",
      "0.00011\n",
      "160 7.6465 7.8905 7.8394 0.4856 0.4458 0.4088\n",
      "0.00011\n",
      "170 7.3789 7.6397 7.5835 0.5060 0.4277 0.4264\n",
      "0.00011\n",
      "180 7.1295 7.4000 7.3460 0.5108 0.4398 0.4214\n",
      "0.00011\n",
      "190 6.8912 7.1734 7.1184 0.5096 0.4398 0.4300\n",
      "0.00011\n",
      "200 6.6647 6.9590 6.8967 0.5132 0.4217 0.4348\n",
      "0.00010\n",
      "210 6.4192 6.7368 6.6643 0.5228 0.4337 0.4254\n",
      "0.00010\n",
      "220 6.2096 6.5324 6.4692 0.5252 0.4398 0.4298\n",
      "0.00010\n",
      "230 5.9839 6.3294 6.2449 0.5372 0.4096 0.4358\n",
      "0.00010\n",
      "240 5.7821 6.1562 6.0659 0.5324 0.4157 0.4302\n",
      "0.00010\n",
      "250 5.5706 5.9179 5.8602 0.5372 0.4518 0.4376\n",
      "0.00010\n",
      "260 5.4041 5.7657 5.6920 0.5348 0.4277 0.4356\n",
      "0.00010\n",
      "270 5.1987 5.5767 5.5140 0.5420 0.3976 0.4324\n",
      "0.00010\n",
      "280 5.0011 5.3859 5.3249 0.5540 0.4398 0.4440\n",
      "0.00010\n",
      "290 4.8341 5.2203 5.1625 0.5564 0.4398 0.4418\n",
      "0.00010\n",
      "300 4.6860 5.0757 5.0092 0.5552 0.4337 0.4472\n",
      "0.00010\n",
      "310 4.5180 4.9027 4.8490 0.5695 0.4398 0.4388\n",
      "0.00010\n",
      "320 4.3743 4.7670 4.7271 0.5492 0.4096 0.4348\n",
      "0.00010\n",
      "330 4.2293 4.6528 4.5896 0.5779 0.4337 0.4470\n",
      "0.00010\n",
      "340 4.1008 4.4891 4.4536 0.5731 0.4578 0.4458\n",
      "0.00010\n",
      "350 3.9557 4.3775 4.3338 0.5851 0.4458 0.4412\n",
      "0.00009\n",
      "360 3.8134 4.2538 4.1994 0.5947 0.4518 0.4548\n",
      "0.00009\n",
      "370 3.6860 4.1470 4.0959 0.6055 0.4578 0.4562\n",
      "0.00009\n",
      "380 3.5808 4.0134 3.9978 0.5923 0.4819 0.4566\n",
      "0.00009\n",
      "390 3.4809 3.9015 3.8878 0.5971 0.4578 0.4600\n",
      "0.00009\n",
      "400 3.3817 3.7984 3.8107 0.6031 0.4699 0.4566\n",
      "0.00009\n",
      "410 3.2829 3.6881 3.7017 0.6103 0.4699 0.4550\n",
      "0.00009\n",
      "420 3.1977 3.6307 3.6307 0.6067 0.4759 0.4516\n",
      "0.00009\n",
      "430 3.1050 3.5456 3.5441 0.6115 0.4759 0.4578\n",
      "0.00009\n",
      "440 3.0226 3.5052 3.4758 0.6199 0.4518 0.4574\n",
      "0.00009\n",
      "450 2.9489 3.4161 3.3990 0.6223 0.4277 0.4512\n",
      "0.00009\n",
      "460 2.8821 3.3512 3.3242 0.6403 0.4819 0.4608\n",
      "0.00008\n",
      "470 2.8063 3.2864 3.2614 0.6127 0.4578 0.4618\n",
      "0.00008\n",
      "480 2.7417 3.2112 3.2086 0.6211 0.4458 0.4592\n",
      "0.00008\n",
      "490 2.6744 3.1429 3.1341 0.6103 0.4699 0.4694\n",
      "0.00008\n",
      "500 2.6008 3.0869 3.0691 0.6379 0.4639 0.4724\n",
      "0.00008\n",
      "510 2.5635 3.0404 3.0405 0.6307 0.4518 0.4666\n",
      "0.00008\n",
      "520 2.4838 2.9505 2.9677 0.6499 0.4940 0.4740\n",
      "0.00008\n",
      "530 2.4371 2.9245 2.9357 0.6331 0.5181 0.4702\n",
      "0.00008\n",
      "540 2.3700 2.8639 2.8812 0.6463 0.5000 0.4754\n",
      "0.00008\n",
      "550 2.3484 2.8133 2.8519 0.6559 0.4819 0.4668\n",
      "0.00007\n",
      "560 2.3004 2.7920 2.8158 0.6583 0.4940 0.4676\n",
      "0.00007\n",
      "570 2.2579 2.7744 2.7790 0.6475 0.4819 0.4684\n",
      "0.00007\n",
      "580 2.2007 2.7118 2.7173 0.6571 0.4880 0.4748\n",
      "0.00007\n",
      "590 2.1723 2.6583 2.7109 0.6631 0.4940 0.4724\n",
      "0.00007\n",
      "600 2.1661 2.6419 2.6796 0.6307 0.5241 0.4652\n",
      "0.00007\n",
      "610 2.1000 2.5798 2.6288 0.6643 0.5120 0.4756\n",
      "0.00007\n",
      "620 2.0707 2.5860 2.6210 0.6715 0.4940 0.4730\n",
      "0.00007\n",
      "630 2.0317 2.5359 2.5691 0.6595 0.4940 0.4822\n",
      "0.00007\n",
      "640 2.0131 2.5388 2.5776 0.6547 0.5361 0.4762\n",
      "0.00006\n",
      "650 1.9868 2.5113 2.5510 0.6583 0.5120 0.4752\n",
      "0.00006\n",
      "660 1.9623 2.4664 2.5249 0.6691 0.5000 0.4744\n",
      "0.00006\n",
      "670 1.9238 2.4546 2.4928 0.6859 0.5120 0.4794\n",
      "0.00006\n",
      "680 1.8822 2.4248 2.4691 0.6847 0.5060 0.4828\n",
      "0.00006\n",
      "690 1.8919 2.4122 2.4756 0.6607 0.5120 0.4740\n",
      "0.00006\n",
      "700 1.8518 2.4082 2.4347 0.6918 0.4880 0.4812\n",
      "0.00006\n",
      "710 1.8362 2.3878 2.4387 0.6835 0.5060 0.4796\n",
      "0.00006\n",
      "720 1.8044 2.3438 2.4130 0.6942 0.4940 0.4878\n",
      "0.00005\n",
      "730 1.7666 2.3413 2.3964 0.6966 0.4940 0.4880\n",
      "0.00005\n",
      "740 1.7580 2.3075 2.3761 0.6966 0.4819 0.4866\n",
      "0.00005\n",
      "750 1.7302 2.3152 2.3708 0.7086 0.5060 0.4856\n",
      "0.00005\n",
      "760 1.7195 2.2815 2.3371 0.6966 0.5060 0.4886\n",
      "0.00005\n",
      "770 1.7224 2.2995 2.3415 0.6835 0.4759 0.4826\n",
      "0.00005\n",
      "780 1.6927 2.2612 2.3204 0.6954 0.5000 0.4930\n",
      "0.00005\n",
      "790 1.7244 2.2710 2.3549 0.6763 0.4940 0.4770\n",
      "0.00005\n",
      "800 1.6573 2.2388 2.3023 0.7074 0.5120 0.4904\n",
      "0.00004\n",
      "810 1.6424 2.2357 2.2817 0.7002 0.4880 0.4918\n",
      "0.00004\n",
      "820 1.6665 2.2766 2.2981 0.6918 0.4940 0.4770\n",
      "0.00004\n",
      "830 1.6253 2.2108 2.2601 0.7098 0.5301 0.4908\n",
      "0.00004\n",
      "840 1.6080 2.2219 2.2671 0.7194 0.4819 0.4848\n",
      "0.00004\n",
      "850 1.6083 2.2137 2.2689 0.7218 0.4940 0.4934\n",
      "0.00004\n",
      "860 1.5797 2.1821 2.2434 0.7182 0.5120 0.4950\n",
      "0.00004\n",
      "870 1.5731 2.1986 2.2520 0.7122 0.4940 0.4906\n",
      "0.00004\n",
      "880 1.5567 2.2085 2.2445 0.7158 0.4759 0.4876\n",
      "0.00004\n",
      "890 1.5463 2.1612 2.2250 0.7122 0.5181 0.4970\n",
      "0.00003\n",
      "900 1.5316 2.1592 2.2181 0.7158 0.5301 0.4964\n",
      "0.00003\n",
      "910 1.5264 2.1406 2.2006 0.7266 0.5482 0.4980\n",
      "0.00003\n",
      "920 1.5124 2.1433 2.1951 0.7230 0.5301 0.4966\n",
      "0.00003\n",
      "930 1.5151 2.1401 2.1992 0.7146 0.5241 0.4932\n",
      "0.00003\n",
      "940 1.4933 2.1381 2.1900 0.7230 0.5120 0.4960\n",
      "0.00003\n",
      "950 1.4726 2.1168 2.1810 0.7470 0.4940 0.5030\n",
      "0.00003\n",
      "960 1.4704 2.1190 2.1849 0.7290 0.4880 0.4972\n",
      "0.00003\n",
      "970 1.4698 2.1295 2.1734 0.7266 0.5000 0.4932\n",
      "0.00003\n",
      "980 1.4552 2.1086 2.1568 0.7434 0.5181 0.4998\n",
      "0.00002\n",
      "990 1.4460 2.0978 2.1529 0.7458 0.5181 0.5004\n",
      "0.00002\n",
      "1000 1.4369 2.0875 2.1460 0.7422 0.5181 0.5038\n",
      "0.00002\n",
      "1010 1.4356 2.0832 2.1499 0.7410 0.5181 0.5018\n",
      "0.00002\n",
      "1020 1.4437 2.0948 2.1459 0.7398 0.5181 0.5004\n",
      "0.00002\n",
      "1030 1.4709 2.1213 2.1831 0.7206 0.4880 0.4906\n",
      "0.00002\n",
      "1040 1.4312 2.0890 2.1541 0.7398 0.5241 0.4974\n",
      "0.00002\n",
      "1050 1.4230 2.0692 2.1376 0.7494 0.5422 0.5020\n",
      "0.00002\n",
      "1060 1.4153 2.0723 2.1416 0.7530 0.5301 0.4988\n",
      "0.00002\n",
      "1070 1.4163 2.0777 2.1419 0.7422 0.5241 0.4984\n",
      "0.00002\n",
      "1080 1.4060 2.0655 2.1352 0.7470 0.5301 0.5024\n",
      "0.00002\n",
      "1090 1.4012 2.0656 2.1389 0.7482 0.5422 0.5018\n",
      "0.00001\n",
      "1100 1.4106 2.0722 2.1534 0.7314 0.5241 0.4994\n",
      "0.00001\n",
      "1110 1.3932 2.0695 2.1258 0.7518 0.5361 0.5046\n",
      "0.00001\n",
      "1120 1.3974 2.0722 2.1265 0.7446 0.5301 0.5014\n",
      "0.00001\n",
      "1130 1.3811 2.0498 2.1123 0.7542 0.5301 0.5070\n",
      "0.00001\n",
      "1140 1.3854 2.0550 2.1201 0.7494 0.5422 0.5040\n",
      "0.00001\n",
      "1150 1.3815 2.0650 2.1204 0.7434 0.5241 0.5024\n",
      "0.00001\n",
      "1160 1.3798 2.0564 2.1220 0.7470 0.5181 0.5060\n",
      "0.00001\n",
      "1170 1.3810 2.0457 2.1204 0.7482 0.5361 0.5014\n",
      "0.00001\n",
      "1180 1.3669 2.0340 2.1051 0.7566 0.5361 0.5038\n",
      "0.00001\n",
      "1190 1.3611 2.0315 2.0957 0.7626 0.5361 0.5052\n",
      "0.00001\n",
      "1200 1.3584 2.0308 2.0934 0.7554 0.5241 0.5076\n",
      "0.00001\n",
      "1210 1.3671 2.0410 2.1066 0.7470 0.5301 0.5034\n",
      "0.00001\n",
      "1220 1.3629 2.0308 2.0974 0.7482 0.5301 0.5074\n",
      "0.00001\n",
      "1230 1.3612 2.0364 2.0993 0.7494 0.5241 0.5082\n",
      "0.00001\n",
      "1240 1.3526 2.0286 2.0919 0.7506 0.5301 0.5088\n",
      "0.00000\n",
      "1250 1.3531 2.0295 2.0903 0.7566 0.5241 0.5078\n",
      "0.00000\n",
      "1260 1.3524 2.0289 2.0896 0.7530 0.5361 0.5096\n",
      "0.00000\n",
      "1270 1.3480 2.0275 2.0857 0.7542 0.5241 0.5078\n",
      "0.00000\n",
      "1280 1.3478 2.0293 2.0874 0.7554 0.5301 0.5072\n",
      "0.00000\n",
      "1290 1.3477 2.0323 2.0870 0.7518 0.5422 0.5094\n",
      "0.00000\n",
      "1300 1.3449 2.0290 2.0877 0.7542 0.5422 0.5072\n",
      "0.00000\n",
      "1310 1.3410 2.0256 2.0840 0.7578 0.5422 0.5080\n",
      "0.00000\n",
      "1320 1.3432 2.0276 2.0864 0.7566 0.5422 0.5062\n",
      "0.00000\n",
      "1330 1.3434 2.0297 2.0884 0.7542 0.5361 0.5062\n",
      "0.00000\n",
      "1340 1.3421 2.0273 2.0881 0.7554 0.5361 0.5056\n",
      "0.00000\n",
      "1350 1.3422 2.0277 2.0874 0.7566 0.5361 0.5060\n",
      "0.00000\n",
      "1360 1.3432 2.0295 2.0893 0.7554 0.5361 0.5060\n",
      "0.00000\n",
      "1370 1.3436 2.0310 2.0902 0.7554 0.5361 0.5052\n",
      "0.00000\n",
      "1380 1.3436 2.0306 2.0905 0.7542 0.5361 0.5044\n",
      "0.00000\n",
      "1390 1.3436 2.0305 2.0907 0.7554 0.5361 0.5054\n",
      "0.00000\n",
      "1400 1.3433 2.0301 2.0906 0.7554 0.5361 0.5052\n",
      "0.00000\n",
      "1410 1.3433 2.0302 2.0906 0.7554 0.5361 0.5054\n",
      "0.00000\n",
      "0.498\n"
     ]
    }
   ],
   "source": [
    "max_valid_epoch_idx = 0\n",
    "max_valid_accuracy = 0.0\n",
    "final_test_accuracy = 0.0\n",
    "for epoch_idx in range(1, 10000 + 1):\n",
    "    batches = 0\n",
    "    for x_batch, y_batch in datagen.flow(x_train, y_train, batch_size=batch_size):\n",
    "        x_batch = x_batch.reshape([-1, n_input])\n",
    "        session.run(\n",
    "                train_op,\n",
    "                feed_dict={\n",
    "                    x: x_batch,\n",
    "                    y: y_batch,\n",
    "                    training: True\n",
    "                })\n",
    "        batches += 1\n",
    "        if batches >= len(x_train) / batch_size:\n",
    "            break\n",
    "\n",
    "    if epoch_idx % 10 == 0:\n",
    "        train_loss_value, train_accuracy_value, lr = session.run(\n",
    "            [loss, accuracy, lr_decayed],\n",
    "            feed_dict={\n",
    "                x: x_train.reshape([-1, n_input]),\n",
    "                y: y_train,\n",
    "                training: False\n",
    "            })\n",
    "        \n",
    "        valid_loss_value, valid_accuracy_value = session.run(\n",
    "            [loss, accuracy],\n",
    "            feed_dict={\n",
    "                x: x_valid,\n",
    "                y: y_valid,\n",
    "                training: False\n",
    "            })\n",
    "            \n",
    "        test_loss_value, test_accuracy_value = session.run(\n",
    "            [loss, accuracy],\n",
    "            feed_dict={\n",
    "                x: x_test,\n",
    "                y: y_test,\n",
    "                training: False\n",
    "            })\n",
    "\n",
    "        print(epoch_idx, '%.4f' % train_loss_value, '%.4f' % valid_loss_value, '%.4f' % test_loss_value, '%.4f' % train_accuracy_value, '%.4f' % valid_accuracy_value, '%.4f' % test_accuracy_value)\n",
    "        print('%.5f' % lr)\n",
    "        if max_valid_accuracy < valid_accuracy_value:\n",
    "            max_valid_accuracy = valid_accuracy_value \n",
    "            max_valid_epoch_idx = epoch_idx\n",
    "            final_test_accuracy = test_accuracy_value\n",
    "            \n",
    "    # Early Stop\n",
    "    if max_valid_epoch_idx + 500 < epoch_idx: # Increase Early Stop bound\n",
    "        break\n",
    "        \n",
    "print(final_test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    32.88% 의 성능을 확인할 수 있습니다. 실습시간 배운 몇 가지 정규화와 최적화 과정을 동원하면 50% 정도의 성능까지는 쉽게 달성할 수 있음을 확인했습니다. 수업시간에 배운 내용들을 사용해 최대한 높은 성능을 나타내는 모델을 만들어보세요! \n",
    "주피터 노트북 파일을 제출해주시면 되며, 성능을 기준으로 점수를 매길 예정입니다. (상대평가)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
