{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment on Regularization and Optimization of Deep Learning\n",
    "\n",
    "이번 과제에서는 reagularization과 optimization에서 배운 내용들을 이용해 최대한 Deep Models의 성능을 높여보고자 합니다. Layer 4개짜리 MLP (각 hidden layer는 512개의 unit을 가짐) 상황에서 정규화와 최적화 방법론들을 총 동원해 성능을 높여주시면 됩니다.\n",
    "\n",
    "먼저, 아래 코드는 데이터 셋을 셋팅하는 부분입니다. 이 부분은 건드리시면 안됩니다. 이 부분을 건드리시면 0점 처리 됩니다. 외부 데이터 사용하셔도 안됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 32, 32, 3)\n",
      "(1000,)\n",
      "(5000, 32, 32, 3)\n",
      "(5000,)\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import math\n",
    "import random \n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "seed = 1\n",
    "random.seed(seed)\n",
    "np.random.seed(seed=seed)\n",
    "tf.random.set_random_seed(seed)\n",
    "\n",
    "(x_1, y_1), (x_2, y_2) = tf.keras.datasets.cifar100.load_data()\n",
    "x_total = np.concatenate([x_1, x_2], axis=0).astype(np.float64)\n",
    "y_total = np.concatenate([y_1, y_2], axis=0)\n",
    "\n",
    "n_output = 10\n",
    "\n",
    "valid_index, _ = np.where(y_total < n_output)\n",
    "y_total = y_total[valid_index].reshape([-1])\n",
    "x_total = x_total[valid_index]\n",
    "\n",
    "i = np.arange(x_total.shape[0])\n",
    "np.random.shuffle(i)\n",
    "x_total = x_total[i]\n",
    "y_total = y_total[i]\n",
    "\n",
    "train_size = 100 * n_output\n",
    "x_train = x_total[:train_size]\n",
    "y_train = y_total[:train_size]\n",
    "x_test = x_total[train_size:]\n",
    "y_test = y_total[train_size:]\n",
    "\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "print(x_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "validation set을 나눕니다. \n",
    "- 실습시간에 배웠던 것처럼 Validation set 비율은 조정하셔도 됩니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "split = x_train.shape[0] // 5\n",
    "x_valid = x_train[:split]\n",
    "y_valid = y_train[:split]\n",
    "\n",
    "x_train = x_train[split:]\n",
    "y_train = y_train[split:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이미지를 greyscale로 변경합니다. \n",
    "1. RGB 값을 고려한 코드로 변경하셔도 됩니다. \n",
    "2. Augmentation을 고려해보세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(800, 32, 32, 3)\n",
      "(200, 32, 32, 3)\n",
      "(5000, 32, 32, 3)\n"
     ]
    }
   ],
   "source": [
    "# x_train = np.mean(x_train, axis=3, keepdims=True)\n",
    "# x_valid = np.mean(x_valid, axis=3, keepdims=True)\n",
    "# x_test = np.mean(x_test, axis=3, keepdims=True)\n",
    "# Use RGB and normalize pixel values by training data-channel\n",
    "mean = np.zeros(3)\n",
    "std = np.ones(3)\n",
    "for i in range(3):\n",
    "    mean[i] = np.mean(x_train[:, :, :, i])\n",
    "    std[i] = np.std(x_train[:, :, :, i])    \n",
    "x_train = (x_train - mean) / std\n",
    "x_valid = (x_valid - mean) / std\n",
    "x_test = (x_test - mean) / std\n",
    "\n",
    "epoch = 10000\n",
    "batch_size = 64\n",
    "\n",
    "print(x_train.shape)\n",
    "print(x_valid.shape)\n",
    "print(x_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make Data Generator\n",
    "for augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "datagen = tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "    width_shift_range=5,\n",
    "    height_shift_range=5,\n",
    "    fill_mode='constant',\n",
    "    horizontal_flip=True,\n",
    "    vertical_flip=False,\n",
    "    rotation_range=10,\n",
    "    zoom_range=[0.8, 1.2]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(200, 3072)\n",
      "(5000, 3072)\n"
     ]
    }
   ],
   "source": [
    "n_input = 32 * 32 * 3   # H * W * C\n",
    "\n",
    "# x_train = x_train.reshape([-1, n_input]) This will be done after augmentation.\n",
    "x_valid = x_valid.reshape([-1, n_input])\n",
    "x_test = x_test.reshape([-1, n_input])\n",
    "\n",
    "print(x_valid.shape)\n",
    "print(x_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 모델을 만듭니다.\n",
    "\n",
    "1. Optimizer를 다른 걸로 바꿔보세요\n",
    "2. Learning Rate를 바꿔보세요. Learning Rate Scheduling도 고려해보세요.\n",
    "3. Activation Function을 바꿔보세요. \n",
    "4. Dropout, DropConnect, Gaussian Dropout 을 고려해보세요.\n",
    "5. Augmentation을 고려해보세요. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0723 22:58:49.667886  3268 deprecation.py:323] From c:\\users\\ironm\\tf-nightly\\lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    }
   ],
   "source": [
    "x = tf.placeholder(tf.float32, [None, n_input])\n",
    "y = tf.placeholder(tf.int32, [None])\n",
    "training = tf.placeholder(tf.bool)\n",
    "\n",
    "n_units = [n_input, 512, 512, 512, n_output]\n",
    "\n",
    "weights, biases = [], []\n",
    "for i, (n_in, n_out) in enumerate(zip(n_units[:-1], n_units[1:])):\n",
    "    stddev = math.sqrt(2 / n_in) # Kaiming He Initialization\n",
    "    weight = tf.Variable(tf.random.truncated_normal([n_in, n_out], mean=0, stddev=stddev))\n",
    "    bias = tf.Variable(tf.zeros([n_out]))\n",
    "    weights.append(weight)\n",
    "    biases.append(bias)\n",
    "    \n",
    "layer = x\n",
    "for i, (weight, bias) in enumerate(zip(weights, biases)):\n",
    "    if i < len(weights) - 1:\n",
    "        rate = tf.cond(training, lambda: 0.5, lambda: 0.0)\n",
    "        maxnorm = tf.keras.constraints.MaxNorm()\n",
    "        weight = maxnorm(weight)\n",
    "        # weight = tf.nn.dropout(weight, rate=rate) * (1 - rate) # DropConnect\n",
    "        layer = tf.matmul(layer, weight) + bias\n",
    "        layer = tf.nn.relu(layer)\n",
    "        layer = tf.nn.dropout(layer, rate=rate)\n",
    "    else:\n",
    "        layer = tf.matmul(layer, weight) + bias\n",
    "y_hat = layer\n",
    "\n",
    "y_hot = tf.one_hot(y, n_output)\n",
    "costs = tf.nn.softmax_cross_entropy_with_logits_v2(\n",
    "        labels=y_hot, logits=y_hat)\n",
    "cross_entropy_loss = tf.reduce_mean(costs)\n",
    "# L2-regularization\n",
    "l2_loss = 0.0\n",
    "for weight in weights:\n",
    "    l2_loss = l2_loss + tf.nn.l2_loss(weight)\n",
    "loss = cross_entropy_loss\n",
    "\n",
    "y_label = tf.argmax(y_hat, 1)\n",
    "accuracy = tf.count_nonzero(\n",
    "        tf.cast(tf.equal(tf.argmax(y_hot, 1), y_label),\n",
    "                tf.int64)) / tf.cast(tf.shape(y_hot)[0], tf.int64)\n",
    "\n",
    "extra_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "with tf.control_dependencies(extra_ops):\n",
    "    global_step = tf.Variable(1, trainable=False)\n",
    "    decay_steps = 20000\n",
    "    learning_rate = 0.0001\n",
    "    lr_decayed = tf.train.cosine_decay(learning_rate, global_step, decay_steps)\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=lr_decayed, beta1=0.85)\n",
    "    train_op = optimizer.minimize(loss, global_step=global_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpu_options = tf.GPUOptions()\n",
    "gpu_options.allow_growth = True\n",
    "session = tf.Session(config=tf.ConfigProto(gpu_options=gpu_options))\n",
    "session.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 1.8432 1.9514 1.9120 0.3675 0.3000 0.3364\n",
      "0.00010\n",
      "20 1.7627 1.8704 1.8385 0.4037 0.3600 0.3610\n",
      "0.00010\n",
      "30 1.6876 1.8193 1.7839 0.4325 0.3850 0.3828\n",
      "0.00010\n",
      "40 1.6653 1.8023 1.7817 0.4487 0.4150 0.3850\n",
      "0.00010\n",
      "50 1.6190 1.7903 1.7520 0.4612 0.3700 0.3994\n",
      "0.00010\n",
      "60 1.5936 1.7997 1.7447 0.4637 0.3650 0.3992\n",
      "0.00010\n",
      "70 1.5748 1.7780 1.7269 0.4738 0.3800 0.4014\n",
      "0.00010\n",
      "80 1.5499 1.7462 1.7096 0.4750 0.4200 0.4118\n",
      "0.00010\n",
      "90 1.5121 1.7397 1.6960 0.4988 0.4150 0.4112\n",
      "0.00010\n",
      "100 1.4740 1.7077 1.6774 0.5250 0.4450 0.4210\n",
      "0.00010\n",
      "110 1.4640 1.7192 1.6763 0.5162 0.4050 0.4166\n",
      "0.00010\n",
      "120 1.4545 1.7239 1.6697 0.5175 0.4000 0.4188\n",
      "0.00010\n",
      "130 1.4313 1.6965 1.6530 0.5350 0.4200 0.4264\n",
      "0.00010\n",
      "140 1.4113 1.6769 1.6487 0.5513 0.4100 0.4266\n",
      "0.00010\n",
      "150 1.4072 1.6690 1.6456 0.5375 0.4350 0.4308\n",
      "0.00010\n",
      "160 1.3969 1.6704 1.6393 0.5363 0.4450 0.4302\n",
      "0.00010\n",
      "170 1.3719 1.6571 1.6302 0.5463 0.4400 0.4346\n",
      "0.00010\n",
      "180 1.3479 1.6276 1.6154 0.5587 0.4700 0.4398\n",
      "0.00010\n",
      "190 1.3435 1.6492 1.6246 0.5463 0.4200 0.4376\n",
      "0.00010\n",
      "200 1.3256 1.6344 1.6052 0.5675 0.4200 0.4440\n",
      "0.00010\n",
      "210 1.3163 1.6389 1.6185 0.5613 0.4450 0.4360\n",
      "0.00010\n",
      "220 1.2924 1.6134 1.5950 0.5813 0.4700 0.4502\n",
      "0.00010\n",
      "230 1.2869 1.6338 1.6035 0.5675 0.4400 0.4438\n",
      "0.00009\n",
      "240 1.2687 1.6015 1.5896 0.5913 0.4550 0.4494\n",
      "0.00009\n",
      "250 1.2619 1.6140 1.5905 0.6138 0.4700 0.4484\n",
      "0.00009\n",
      "260 1.2451 1.5926 1.5838 0.5988 0.4850 0.4542\n",
      "0.00009\n",
      "270 1.2471 1.5950 1.5839 0.6075 0.4700 0.4464\n",
      "0.00009\n",
      "280 1.2199 1.5899 1.5741 0.6188 0.4500 0.4550\n",
      "0.00009\n",
      "290 1.2234 1.5656 1.5735 0.6138 0.4700 0.4550\n",
      "0.00009\n",
      "300 1.1891 1.5669 1.5660 0.6125 0.4900 0.4530\n",
      "0.00009\n",
      "310 1.1933 1.5675 1.5614 0.6300 0.5100 0.4586\n",
      "0.00009\n",
      "320 1.1850 1.5752 1.5712 0.6125 0.4550 0.4574\n",
      "0.00009\n",
      "330 1.1723 1.5591 1.5639 0.6362 0.5100 0.4634\n",
      "0.00009\n",
      "340 1.1659 1.5727 1.5654 0.6250 0.4800 0.4586\n",
      "0.00009\n",
      "350 1.1492 1.5764 1.5579 0.6438 0.5050 0.4634\n",
      "0.00009\n",
      "360 1.1346 1.5545 1.5528 0.6450 0.5000 0.4642\n",
      "0.00009\n",
      "370 1.1191 1.5584 1.5471 0.6637 0.5250 0.4690\n",
      "0.00009\n",
      "380 1.1203 1.5504 1.5499 0.6500 0.4900 0.4598\n",
      "0.00009\n",
      "390 1.0991 1.5329 1.5421 0.6550 0.5000 0.4644\n",
      "0.00008\n",
      "400 1.0885 1.5361 1.5403 0.6587 0.4950 0.4686\n",
      "0.00008\n",
      "410 1.0764 1.5283 1.5378 0.6587 0.4750 0.4726\n",
      "0.00008\n",
      "420 1.0684 1.5463 1.5321 0.6575 0.4800 0.4748\n",
      "0.00008\n",
      "430 1.0579 1.5212 1.5293 0.6775 0.4800 0.4786\n",
      "0.00008\n",
      "440 1.0612 1.5117 1.5346 0.6813 0.5100 0.4698\n",
      "0.00008\n",
      "450 1.0506 1.5104 1.5236 0.6850 0.5000 0.4774\n",
      "0.00008\n",
      "460 1.0363 1.4984 1.5259 0.6813 0.5050 0.4730\n",
      "0.00008\n",
      "470 1.0251 1.5019 1.5208 0.6775 0.4950 0.4786\n",
      "0.00008\n",
      "480 1.0243 1.5028 1.5211 0.6963 0.5100 0.4790\n",
      "0.00008\n",
      "490 1.0072 1.4981 1.5199 0.7075 0.5200 0.4816\n",
      "0.00008\n",
      "500 1.0034 1.5041 1.5281 0.6975 0.5200 0.4730\n",
      "0.00008\n",
      "510 0.9916 1.5196 1.5210 0.7175 0.5050 0.4758\n",
      "0.00008\n",
      "520 0.9846 1.4915 1.5167 0.7100 0.5300 0.4794\n",
      "0.00007\n",
      "530 0.9821 1.5113 1.5275 0.7025 0.5000 0.4794\n",
      "0.00007\n",
      "540 0.9746 1.5149 1.5218 0.7150 0.5250 0.4804\n",
      "0.00007\n",
      "550 0.9540 1.5024 1.5106 0.7212 0.5150 0.4806\n",
      "0.00007\n",
      "560 0.9516 1.4975 1.5083 0.7275 0.5500 0.4864\n",
      "0.00007\n",
      "570 0.9483 1.4997 1.5127 0.7262 0.5400 0.4824\n",
      "0.00007\n",
      "580 0.9320 1.4767 1.5186 0.7325 0.5350 0.4814\n",
      "0.00007\n",
      "590 0.9349 1.4979 1.5281 0.7300 0.5250 0.4770\n",
      "0.00007\n",
      "600 0.9228 1.4956 1.5207 0.7200 0.5350 0.4852\n",
      "0.00007\n",
      "610 0.9147 1.5013 1.5223 0.7375 0.5250 0.4812\n",
      "0.00007\n",
      "620 0.8984 1.4745 1.5052 0.7500 0.5600 0.4892\n",
      "0.00006\n",
      "630 0.8893 1.4647 1.5026 0.7450 0.5700 0.4892\n",
      "0.00006\n",
      "640 0.8872 1.4687 1.5057 0.7488 0.5550 0.4858\n",
      "0.00006\n",
      "650 0.8892 1.4910 1.5206 0.7488 0.5150 0.4798\n",
      "0.00006\n",
      "660 0.8703 1.4640 1.4990 0.7500 0.5550 0.4946\n",
      "0.00006\n",
      "670 0.8726 1.4838 1.5091 0.7425 0.5500 0.4896\n",
      "0.00006\n",
      "680 0.8633 1.4749 1.5142 0.7562 0.5550 0.4864\n",
      "0.00006\n",
      "690 0.8546 1.4614 1.5057 0.7638 0.5700 0.4908\n",
      "0.00006\n",
      "700 0.8451 1.4831 1.5098 0.7600 0.5550 0.4890\n",
      "0.00006\n",
      "710 0.8483 1.4721 1.5003 0.7775 0.5700 0.4964\n",
      "0.00006\n",
      "720 0.8405 1.4685 1.5097 0.7725 0.5750 0.4944\n",
      "0.00006\n",
      "730 0.8359 1.4487 1.5058 0.7662 0.5600 0.4928\n",
      "0.00005\n",
      "740 0.8281 1.4640 1.5195 0.7662 0.5650 0.4890\n",
      "0.00005\n",
      "750 0.8179 1.4481 1.5014 0.7638 0.5550 0.4946\n",
      "0.00005\n",
      "760 0.8107 1.4531 1.5054 0.7662 0.5600 0.4996\n",
      "0.00005\n",
      "770 0.8107 1.4602 1.5076 0.7812 0.5650 0.4952\n",
      "0.00005\n",
      "780 0.7998 1.4607 1.5135 0.7825 0.5550 0.4916\n",
      "0.00005\n",
      "790 0.8034 1.4697 1.5192 0.7738 0.5600 0.4908\n",
      "0.00005\n",
      "800 0.7926 1.4729 1.5149 0.7712 0.5450 0.4948\n",
      "0.00005\n",
      "810 0.7839 1.4701 1.5142 0.7762 0.5650 0.4994\n",
      "0.00005\n",
      "820 0.7832 1.4617 1.5101 0.7688 0.5600 0.5006\n",
      "0.00004\n",
      "830 0.7739 1.4643 1.5050 0.7875 0.5800 0.5006\n",
      "0.00004\n",
      "840 0.7766 1.4671 1.5059 0.7788 0.5700 0.5020\n",
      "0.00004\n",
      "850 0.7663 1.4565 1.4978 0.7887 0.5700 0.5074\n",
      "0.00004\n",
      "860 0.7594 1.4543 1.5061 0.7963 0.5450 0.5040\n",
      "0.00004\n",
      "870 0.7586 1.4602 1.5122 0.7987 0.5550 0.5022\n",
      "0.00004\n",
      "880 0.7500 1.4538 1.5061 0.8013 0.5550 0.5038\n",
      "0.00004\n",
      "890 0.7452 1.4542 1.5111 0.8000 0.5500 0.5002\n",
      "0.00004\n",
      "900 0.7451 1.4455 1.5068 0.7987 0.5550 0.5010\n",
      "0.00004\n",
      "910 0.7389 1.4482 1.5090 0.8025 0.5550 0.5010\n",
      "0.00004\n",
      "920 0.7396 1.4577 1.5122 0.8025 0.5500 0.4988\n",
      "0.00003\n",
      "930 0.7325 1.4383 1.5094 0.8063 0.5600 0.5006\n",
      "0.00003\n",
      "940 0.7286 1.4477 1.5072 0.8087 0.5650 0.5038\n",
      "0.00003\n",
      "950 0.7213 1.4462 1.5151 0.8075 0.5650 0.5054\n",
      "0.00003\n",
      "960 0.7207 1.4543 1.5191 0.8013 0.5600 0.5022\n",
      "0.00003\n",
      "970 0.7166 1.4546 1.5153 0.8013 0.5600 0.5032\n",
      "0.00003\n",
      "980 0.7130 1.4621 1.5195 0.8050 0.5550 0.4990\n",
      "0.00003\n",
      "0.5006\n"
     ]
    }
   ],
   "source": [
    "max_valid_epoch_idx = 0\n",
    "max_valid_accuracy = 0.0\n",
    "final_test_accuracy = 0.0\n",
    "for epoch_idx in range(1, 10000 + 1):\n",
    "    batches = 0\n",
    "    for x_batch, y_batch in datagen.flow(x_train, y_train, batch_size=batch_size):\n",
    "        x_batch = x_batch.reshape([-1, n_input])\n",
    "        session.run(\n",
    "                train_op,\n",
    "                feed_dict={\n",
    "                    x: x_batch,\n",
    "                    y: y_batch,\n",
    "                    training: True\n",
    "                })\n",
    "        batches += 1\n",
    "        if batches >= len(x_train) / batch_size:\n",
    "            break\n",
    "\n",
    "    if epoch_idx % 10 == 0:\n",
    "        train_loss_value, train_accuracy_value, lr = session.run(\n",
    "            [loss, accuracy, lr_decayed],\n",
    "            feed_dict={\n",
    "                x: x_train.reshape([-1, n_input]),\n",
    "                y: y_train,\n",
    "                training: False\n",
    "            })\n",
    "        \n",
    "        valid_loss_value, valid_accuracy_value = session.run(\n",
    "            [loss, accuracy],\n",
    "            feed_dict={\n",
    "                x: x_valid,\n",
    "                y: y_valid,\n",
    "                training: False\n",
    "            })\n",
    "            \n",
    "        test_loss_value, test_accuracy_value = session.run(\n",
    "            [loss, accuracy],\n",
    "            feed_dict={\n",
    "                x: x_test,\n",
    "                y: y_test,\n",
    "                training: False\n",
    "            })\n",
    "\n",
    "        print(epoch_idx, '%.4f' % train_loss_value, '%.4f' % valid_loss_value, '%.4f' % test_loss_value, '%.4f' % train_accuracy_value, '%.4f' % valid_accuracy_value, '%.4f' % test_accuracy_value)\n",
    "        print('%.5f' % lr)\n",
    "        if max_valid_accuracy < valid_accuracy_value:\n",
    "            max_valid_accuracy = valid_accuracy_value \n",
    "            max_valid_epoch_idx = epoch_idx\n",
    "            final_test_accuracy = test_accuracy_value\n",
    "            \n",
    "    # Early Stop\n",
    "    if max_valid_epoch_idx + 150 < epoch_idx: # Increase Early Stop bound\n",
    "        break\n",
    "        \n",
    "print(final_test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "32.88% 의 성능을 확인할 수 있습니다. 실습시간 배운 몇 가지 정규화와 최적화 과정을 동원하면 50% 정도의 성능까지는 쉽게 달성할 수 있음을 확인했습니다. 수업시간에 배운 내용들을 사용해 최대한 높은 성능을 나타내는 모델을 만들어보세요! \n",
    "주피터 노트북 파일을 제출해주시면 되며, 성능을 기준으로 점수를 매길 예정입니다. (상대평가)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
